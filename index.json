[{"content":"It is not a secret that Linux can juggle USB serial ports between restarts. This is something which doesn\u0026rsquo;t happen on linux, but if you ever looked at internet forums on how to fix issue, you for sure found advice to use custom udev rules.\nProblem statement Most of articles advice to use attributes found on USB device such as idVendor and idProduct. These are part of USB device descriptor and allow operating system to identify what was plugged. Eventually, people who run into issues with these can stick with serial. Problem is, I don\u0026rsquo;t want to create a dedicate rule for every single thing I plug into my device, after all it is universal serial bus.\nIf we are on attributes - some vendors keep using the same value for vendor, product and serial for all devices they manufacture. In such situations some advice to rely on udev KERNEL S value (which we will look closer soon). For sure some manufacturers violate the specification and denies whole point of us having device descriptor, but .. that\u0026rsquo;s life. Is there any other information we can follow?\nAccessing USB hub information The idVendor and idProduct attributes come from device side. On computer end we usually have USB hub or hubs which handle ports and do all the low level work. If we look at list of attributes available at this place we can find quite many unique elements:\nlooking at parent device \u0026#39;/devices/pci0000:00/0000:00:15.0/usb1/1-1\u0026#39;: KERNELS==\u0026#34;1-1\u0026#34; SUBSYSTEMS==\u0026#34;usb\u0026#34; DRIVERS==\u0026#34;usb\u0026#34; ATTRS{authorized}==\u0026#34;1\u0026#34; ATTRS{avoid_reset_quirk}==\u0026#34;0\u0026#34; ATTRS{bConfigurationValue}==\u0026#34;1\u0026#34; ATTRS{bDeviceClass}==\u0026#34;00\u0026#34; ATTRS{bDeviceProtocol}==\u0026#34;00\u0026#34; ATTRS{bDeviceSubClass}==\u0026#34;00\u0026#34; ... ATTRS{bmAttributes}==\u0026#34;80\u0026#34; ATTRS{busnum}==\u0026#34;1\u0026#34; ATTRS{configuration}==\u0026#34;\u0026#34; ATTRS{devnum}==\u0026#34;2\u0026#34; ATTRS{devpath}==\u0026#34;1\u0026#34; ATTRS{idProduct}==\u0026#34;ea60\u0026#34; ATTRS{idVendor}==\u0026#34;10c4\u0026#34; ATTRS{ltm_capable}==\u0026#34;no\u0026#34; ATTRS{manufacturer}==\u0026#34;Silicon Labs\u0026#34; .... ATTRS{rx_lanes}==\u0026#34;1\u0026#34; ATTRS{serial}==\u0026#34;12132\u0026#34; ATTRS{speed}==\u0026#34;12\u0026#34; ATTRS{tx_lanes}==\u0026#34;1\u0026#34; ATTRS{urbnum}==\u0026#34;341706\u0026#34; ATTRS{version}==\u0026#34; 1.10\u0026#34; ... (more to follow) Above is one of nodes which corresponds device plugged into hub 1 (busnum==1) at port 1 (devpath==1). While linux kernel might have different initialization sequence for some cases, the hardware part should never change, because one thing for sure will be consistent - it means usb hub port numbers. This is also where KERNELS value comes to play, in above example it is set to 1-1, which is bus + port number. Obviously there are more complex use cases such as hub under hub, under hub.\nThe ultimate udev rule to rule identifiers Because we successfully identified what linux kernel knows about port numbers we can try to bake udev rule for it. I won\u0026rsquo;t dive into details of how udev rules are matched or constructed as there are many materials describing that. Below I explain rationale behind this rule:\nKERNEL==\u0026#34;ttyUSB[0-9]*\u0026#34;, ATTRS{bDeviceClass}==\u0026#34;00\u0026#34;, ATTRS{bDeviceProtocol}==\u0026#34;00\u0026#34;, ATTRS{bDeviceSubClass}==\u0026#34;00\u0026#34;, ATTRS{busnum}==\u0026#34;*\u0026#34;, ATTRS{devnum}==\u0026#34;*\u0026#34;, ATTRS{devpath}==\u0026#34;*\u0026#34;, ATTRS{serial}==\u0026#34;*\u0026#34;, SYMLINK+=\u0026#34;ttyUSB$attr{busnum}.$attr{devnum}.$attr{devpath}.$attr{serial}\u0026#34; Looking at above - we can see there KERNEL==\u0026quot;ttyUSB[0-9]*\u0026quot;, which should match devices which got already identifier assigned by kernel. We wish to re-assign it, hence there are few more conditions.\nThe bDeviceClass, bDeviceProtocol and bDeviceSubClass are all set to 00, in order to match single edge. The hub infrastructure above will have different protocol. While it might seem redundant, it helps to keep uniqueness of results. It ensures that we will match udev node (I believe) which represents information where both descriptor and hub information is present and not usb device side.\nThe later part specifies that busnum, devnum and devpath as well as serial must be present (even if its empty). Finally we add symlink which uses above port identification.\nThis is really it - there are a lot of examples where you will find SYMLINK+=\u0026quot;myDevice\u0026quot;, which is fine, but since we look for generic solution of generic issue, we want to stick to information which is always available and do not change until device is not re-wired.\nLets have a look on results:\n$ ls -la /dev/ttyUSB* crw-rw---- 1 root dialout 188, 0 May 9 08:42 /dev/ttyUSB0 crw-rw---- 1 root dialout 188, 1 May 9 08:42 /dev/ttyUSB1 lrwxrwxrwx 1 root root 7 May 9 08:42 /dev/ttyUSB1.2.1.12132 -\u0026gt; ttyUSB1 lrwxrwxrwx 1 root root 7 May 9 08:42 /dev/ttyUSB1.3.4.2701CDF2 -\u0026gt; ttyUSB0 We can see multiple ttyUSB1 nodes, however for these longer 1 is actually bus identifier, followed by devnum, devpath and serial numbers. I\u0026rsquo;ve used all these for causes when device have multiple buses and multiple USB hubs. Lets see if it will last!\n","permalink":"https://dywicki.pl/2024/05/persistent-serial-identifiers-with-udev/","summary":"It is not a secret that Linux can juggle USB serial ports between restarts. This is something which doesn\u0026rsquo;t happen on linux, but if you ever looked at internet forums on how to fix issue, you for sure found advice to use custom udev rules.","title":"Persistent serial identifiers with udev"},{"content":"The Felix Fileinstall and Config Admin are two essential components of Apache Karaf runtime. I wrote about issues with array/list handling already twice. In 2015 and 2022. While earlier post was about syntax, later was about troubles with parsing of stored entries. Turns out, I\u0026rsquo;ve found a reason.\nWhen it comes to handling of configuration updates from filesystem the Fileinstall is first in the queue to carry these into runtime. However recently I was again hit by its behavior. While earlier issue was solved just by making configuration read only, this time I wanted to keep it open for modifications. And problem was clear - I could write using square brackets (array) syntax, but I could not use regular brackets (collection) syntax. Over the journey I\u0026rsquo;ve found following.\nTyped properties Fileinstall embeds felix-utils which ship class named TypedProperties. This little lovely thing can distinguish typed and non-typed properties. Later ones are treated as strings. Turns out, decision is made based on regex, which does recognize [ a, b, c] syntax but does not accept ( a, b, c ). Issue known for 5 years - see FELIX-6042.\nTo make things worse, fileinstall relies on outdated felix-utils release till today. I was first hit by issue with fileinstall 3.6.6 back in 2020. But problem is still present in most recent release - 3.7.4. Why? Because it does rely on felix-utils 1.11.2 which miss this patch: https://github.com/apache/felix-dev/commit/2a8b4eba2fb99bfac727818f315ea190542c92e7.\n","permalink":"https://dywicki.pl/2024/02/felix-config-admin-with-fileinstall-and-array-values/","summary":"The Felix Fileinstall and Config Admin are two essential components of Apache Karaf runtime. I wrote about issues with array/list handling already twice. In 2015 and 2022. While earlier post was about syntax, later was about troubles with parsing of stored entries. Turns out, I\u0026rsquo;ve found a reason.","title":"Felix Config Admin with Fileinstall and array values"},{"content":"The TwinCAT is automation software - both runtime and engineering environment from Beckhoff Automation. Since its early days it embedded \u0026ldquo;win\u0026rdquo;, which was a shortcut from windows. Today I\u0026rsquo;ve finally got a TwinCAT/BSD working within VM and had to enable old fashion ADS connection to integration developed within Apache PLC4X library and embedded in ConnectorIO Agent software.\nProblem statement The TwinCAT/BSD comes with secure ADS by default and firewall which blocks most of the ports. Because secure ADS uses TLS, currently not supported yet by Apache PLC4X, I had to find a way to enable \u0026ldquo;old\u0026rdquo; and insecure connection.\nSolution In order to get everything sorted we need first to elevate user permissions. Default user we log into via shell is Administrator, which is not permitted to amend system configuration.\nFor that reason we first have to do sudo sh.\nAfter getting root permission, we can edit firewall configuration by adding following line to /etc/pf.conf (information taken from official Beckhoff docs). The \u0026ldquo;pf\u0026rdquo; stands for packet filter which is firewall software for Free BSD. You can use vi to conduct editing.\npass in quick proto tcp to port 48898 synproxy state Once change is saved we need to reload firewall rules through pfctl -f /etc/pf.conf command.\nResults With above changes, it is finally possible to retrieve symbol list through ADS. :-) Please note fancy ASCII dump of exchanged payload. ;-)\n╔═AdsTableSizes═══════════════════════════════════════════════════════════════════════════════════╗ ║╔═symbolCount═╗╔═symbolLength══╗╔═dataTypeCount╗╔═dataTypeLength═╗╔═extraCount════╗╔═extraLength╗║ ║║0x00000013 19║║0x00000850 2128║║0x00000037 55 ║║0x00002c18 11288║║0x000007d0 2000║║0x00000000 0║║ ║╚═════════════╝╚═══════════════╝╚══════════════╝╚════════════════╝╚═══════════════╝╚════════════╝║ ╚═════════════════════════════════════════════════════════════════════════════════════════════════╝ | PLC4X Field | Hex | ##### | Query Syntax | Index | Offset | Type | Name | Size (B) | Type | Value (type) ------+-------------------------------+-----------+-----------+-----------+----------------------------------------+----------+------| 0 | 0x4040/0x5dffd:BOOL | 0x4040 | 0x5dffd | BOOL |Constants.bFPUSupport | 1 | 33 | true (java.lang.Boolean) 2 | 0x4040/0x5dfeb:BOOL | 0x4040 | 0x5dfeb | BOOL |Constants.bLittleEndian | 1 | 33 | true (java.lang.Boolean) 4 | 0x4040/0x5e002:BOOL | 0x4040 | 0x5e002 | BOOL |Constants.bMulticoreSupport | 1 | 33 | false (java.lang.Boolean) 6 | 0x4040/0x5dffc:BOOL | 0x4040 | 0x5dffc | BOOL |Constants.bSimulationMode | 1 | 33 | false (java.lang.Boolean) 8 | 0x4040/0x5dff4:VERSION | 0x4040 | 0x5dff4 | VERSION |Constants.CompilerVersion | 8 | 65 | {uiMajor=3, uiMinor=5, uiServicePack=13, uiPatch=40} (java.util.Map) 10 | 0x4040/0x5e008:DWORD | 0x4040 | 0x5e008 | DWORD |Constants.CompilerVersionNumeric | 4 | 19 | 50662696 (java.lang.Long) 12 | 0x4040/0x5e000:UINT | 0x4040 | 0x5e000 | UINT |Constants.nPackMode | 2 | 18 | 8 (java.lang.Integer) 14 | 0x4040/0x5dffe:WORD | 0x4040 | 0x5dffe | WORD |Constants.nRegisterSize | 2 | 18 | 64 (java.lang.Integer) 16 | 0x4040/0x5dfec:VERSION | 0x4040 | 0x5dfec | VERSION |Constants.RuntimeVersion | 8 | 65 | {uiMajor=3, uiMinor=5, uiServicePack=13, uiPatch=0} (java.util.Map) 18 | 0x4040/0x5e004:DWORD | 0x4040 | 0x5e004 | DWORD |Constants.RuntimeVersionNumeric | 4 | 19 | 50662656 (java.lang.Long) Clearly program on PLC is not doing anything, as there are no application specific symbols, just plain symbols available in (probably) every runtime.\nAs a remark - I am not a PLC programmer, what helped me a lot was tutorial on TwinCAT/BSD in a virtual machine by AllTwincat. Thanks to it, and above firewall update, I have finally a working TwinCAT3 runtime without a need to launch windows. :-)\n","permalink":"https://dywicki.pl/2023/10/twincat-bsd-with-insecure-ads-connection/","summary":"The TwinCAT is automation software - both runtime and engineering environment from Beckhoff Automation. Since its early days it embedded \u0026ldquo;win\u0026rdquo;, which was a shortcut from windows. Today I\u0026rsquo;ve finally got a TwinCAT/BSD working within VM and had to enable old fashion ADS connection to integration developed within \u003ca href=\"https://plc4x.apache.org\"\u003eApache PLC4X\u003c/a\u003e library and embedded in \u003ca href=\"https://connectorio.com/industrial-and-building-automation-monitoring-agent/\"\u003eConnectorIO Agent\u003c/a\u003e software.","title":"TwinCAT/BSD with insecure ADS connection"},{"content":"The gohugo is being called a content management system, because\u0026hellip; it allows to manage a content. As someone who used wordpress for far too long I found that gohugo was missed my expectations a bit. One of areas where I found it shortcomings are redirects and aliases.\nGohugo supports aliases which can be defined in frontmatter section (through \u0026ldquo;aliases\u0026rdquo; key), however their usage results in generation of extra html files with meta-refresh html header. Its clever way, which works for everyone.\nTo be fair, when I saw support for _redirects file I hoped that someone did heavy lifting and brought similar functionality for nginx deployments. I was wrong. :-) The _redirects file is specific to netlify. Additionally, nginx does not like configuration snippets like Apache2 do (with .htaccess files). So I was forced to make it myself.\nNginx config generator Gohugo supports alternative output formats of your webpage. It might be a sitemap, rss feed or something imaginary you need. Everything is generated from template files, so it can be pretty much anything, as long as it is a text. In order to bring new format there few steps needed. First of all, it must be declared in configuration, then it needs to be assigned to specific part (home, section, page), and then properly templated.\nBecause I wanted my format to result in \u0026ldquo;nginx.conf\u0026rdquo; file generation I had to declare not only output format but also media type.\nmediaTypes: \u0026#34;text/nginx-conf\u0026#34;: suffixes: \u0026#34;conf\u0026#34; outputFormats: nginx: isPlainText: true mediaType: text/nginx-conf baseName: nginx notAlternative: true noUgly: true This means that our template will be called \u0026lt;section\u0026gt;.nginx.conf. Since we need only one configuration snippet its necessary only at home level:\noutputs: home: - html - rss - nginx Final point is home.nginx.conf template, which turned to be quite simple:\n# Page aliases {{ range .Site.AllPages }} {{- if (isset .Params \u0026#34;aliases\u0026#34;) -}} {{- $page := . -}} {{- range .Params.aliases -}} rewrite ^{{ . }}$ {{ $page.RelPermalink }}; {{ end }} {{- end -}} {{ end }} Above template uses alias and redirects it to target page. The end result is obvious - the HTML file generated by hugo is not being used, which is ok.\nNginx configuration update In order to make use of generated snippet we still need to update nginx configuration itself. I decided to use a wildcard inclusion above public dir where HTML files are being stored. In below example site is hosted from /var/www/domains/test.gohugo/web directory and configuration is loaded from /var/www/domains/test.gohugo/*.conf files. Since we share there only redirects it should be still ok-ish from security point of view (after proper permissions setup!).\nAll in all nginx host configuration is as below:\nserver { server_name test.gohugo; root /var/www/domains/test.gohugo/web; access_log /var/log/nginx/test.gohugo.access.log; error_log /var/log/nginx/test.gohugo.org.error.log; index index.html index.htm; charset utf-8; location = /favicon.ico { access_log off; log_not_found off; } location = /robots.txt { access_log off; log_not_found off; } include /var/www/domains/test.gohugo/*.conf; location / { try_files $uri $uri/ =404; autoindex off; } } Proxied redirects One additional thing which I wished to test was redirects which do not expose target URI. This is one of common practices which can be used with full blown CMS systems where i.e. binary attachments are streamed or served through proxy endpoint. Nginx can do it, so why not making an extra data file for that?\nI started from basic data file called data/redirects.yaml:\nredirects: - title: \u0026#34;Connectorio Bindings Manual\u0026#34; uri: \u0026#34;/cn-bind-manual\u0026#34; target: \u0026#34;https://myimportantserver.com/wp-content/uploads/some-secret-resource.pdf\u0026#34; proxy: true The missing spot is just an extra section to our home.nginx.conf template file. :-)\n# Custom redirects {{ range .Site.Data.redirects.redirects }} # redirect for {{ .title }} {{- if (.proxy | default false) }} location ~ ^{{ .uri }}$ { {{- $url := urls.Parse .target }} rewrite ^{{ .uri }}$ {{ $url.RequestURI }} break; proxy_pass {{ $url.Scheme }}://{{ $url.Host }}; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_ssl_server_name on; proxy_ssl_protocols TLSv1 TLSv1.1 TLSv1.2; } {{ else }} rewrite ^{{ .uri }}$ {{ .target }}; {{- end }} {{ end }} The generated config fragment will ensure that 1) proxy directive is properly set up, 2) all necessary headers are set, 3) target file remain hidden. If redirect entry is not set as proxied, it will be simply rewritten by nginx. See:\nlocation ~ ^/cn-bind-manual$ { rewrite ^/cn-bind-manual$ /wp-content/uploads/some-secret-resource.pdf break; proxy_pass https://myimportantserver.com; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_ssl_server_name on; proxy_ssl_protocols TLSv1 TLSv1.1 TLSv1.2; } Summary It turns out that with a bit of struggle, it is possible to bring netlify\u0026rsquo;s features to basic nginx deployment of hugo. What gave me most of the pain was initial setup of media type and output formats. Once I found a working combination of output format, media type and template name whole thing began to work.\nI\u0026rsquo;m going to work a little bit more on this stuff and see if it will fly!\n","permalink":"https://dywicki.pl/2023/08/gohugo-and-nginx-config-generator/","summary":"The gohugo is being called a content management system, because\u0026hellip; it allows to manage a content. As someone who used wordpress for far too long I found that gohugo was missed my expectations a bit. One of areas where I found it shortcomings are redirects and aliases.","title":"Gohugo and nginx config generator"},{"content":"By default kubernetes tls secret must consist two fields a tls.key and tls.crt. It works perfectly fine in most of the cases, but not when we need mutual tls which is handled by ingress-nginx. This kind of ingress requires a secret with a \u0026ldquo;ca.crt\u0026rdquo; field to validate client certs. Obviously it tricks a kubectl which can\u0026rsquo;t append more than standard.\nOur help in this case will be kubectl patch. The patch command allows to replace or append new parts to resource definitions. In my case certificate and key file is generated with openssl, thus I wish to populate extra field in order to keep ingress-nginx happy. One note - if you miss ca.crt field in secret pointed by nginx.ingress.kubernetes.io/auth-tls-secret annotation, ingress will keep working. It will just not perform any mtls validation. It will emit a warning, but that\u0026rsquo;s all.\nIf you wonder how to make it happen, this is how I managed it:\n#!/bin/bash namespace=\u0026#34;foo\u0026#34; keyFile=\u0026#34;my-custom-ca.key\u0026#34; crtFile=\u0026#34;my-custom-ca.crt\u0026#34; secret=\u0026#34;client-certificate-ca\u0026#34; kubectl -n \u0026#34;$namespace\u0026#34; create secret tls \u0026#34;$secret\u0026#34; --key=$keyFile \\ --cert=$crtFile || (echo \u0026#34;Could not create $secret in namespace $namespace\u0026#34; \u0026amp;\u0026amp; exit 1) caContents=$(base64 -w0 $crtFile) kubectl -n \u0026#34;$namespace\u0026#34; patch secret \u0026#34;$secret\u0026#34; -p \u0026#34;{\\\u0026#34;data\\\u0026#34;:{\\\u0026#34;ca.crt\\\u0026#34;:\\\u0026#34;${caContents}\\\u0026#34;}}\u0026#34; This little thing keeps it as a single secret. Obviously you might not wish to do that in case of production setups, but that\u0026rsquo;s another thing. :-)\nAdditional point - if you are running macos with its flavor of shell, or an ancient linux distro, then base64 -w0 call will fail. In such case you can use kubectl get secret $secret with jsonpath output or with go-template trick I described in my previous post.\n","permalink":"https://dywicki.pl/2023/05/ingress-nginx-and-tls-secret-with-ca-crt-field/","summary":"By default kubernetes tls secret must consist two fields a \u003ccode\u003etls.key\u003c/code\u003e and \u003ccode\u003etls.crt\u003c/code\u003e. It works perfectly fine in most of the cases, but not when we need mutual tls which is handled by ingress-nginx. This kind of ingress requires a secret with a \u0026ldquo;ca.crt\u0026rdquo; field to validate client certs. Obviously it tricks a kubectl which can\u0026rsquo;t append more than standard.","title":"Kubernets TLS secret with ca.crt field for mtls"},{"content":"I believe that I am not only one who needed to get a secret value out of kubernetes. Usual advice focus on use of kubectl with json output and/or jsonpath. Problem is - the kubectl jsonpath is limited thus result needs to be scanned further by jq. It works, but jq is not part of everyone\u0026rsquo;s working environment.\nWhile looking for solution I found a blog post titled \u0026quot; Retrieve TLS certificates from Kubernetes\u0026quot;, which partially answered my question. Sadly it used both json and jq. It took me a bit to find out that kubectl output I have is not something which will be easy for jsonpath flavor supported by kubectl itself:\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;data\u0026#34;: { \u0026#34;ca.crt\u0026#34;: \u0026#34;LS0tLS1CRUdJTiBDRV...\u0026#34;, \u0026#34;tls.crt\u0026#34;: \u0026#34;LS0tLS1CRUdJTiBDRV...\u0026#34;, \u0026#34;tls.key\u0026#34;: \u0026#34;LS0tLS1CRUdJTiBSU0...\u0026#34; }, \u0026#34;kind\u0026#34;: \u0026#34;Secret\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;annotations\u0026#34;: { \u0026#34;kubectl.kubernetes.io/last-applied-configuration\u0026#34;: \u0026#34;....\u0026#34; }, \u0026#34;creationTimestamp\u0026#34;: \u0026#34;2023-05-02T14:42:00Z\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;app.kubernetes.io/managed-by\u0026#34;: \u0026#34;...\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34;client-root-ca-certificate\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;resourceVersion\u0026#34;: \u0026#34;16702\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;04540ad3-96c4-4989-bb71-cd9fc6cda3a1\u0026#34; }, \u0026#34;type\u0026#34;: \u0026#34;kubernetes.io/tls\u0026#34; } This is due to fact that data is an object and all filtering capabilities embedded in kubectl jsonpath implementation supports some conditions, but it assumes list, hence filtering list objects. A common example of filtering expressions such as -o jsonpath=\u0026quot;{.data[?(@. ...)]}\u0026quot; will not work, because @ element is an object inside of the list. There is no way to make a test expression against map key (or object property) itself. This seems to be a gap in initial json-path proposal which is addressed by jq.\nIn theory we could compare beginning of base64 representation, but it is not a reliable in situations where we have two elements in secret containing certificate. Beginning of base64 encoded certificate is always the same.\nLuckily a quick inspection of kubectl output format revealed \u0026ldquo;go-template\u0026rdquo; output which seemed more flexible in constructing conditions.\nkubectl get secret client-root-ca-certificate -ogo-template=\u0026#39;{{index .data \u0026#34;ca.crt\u0026#34; }}\u0026#39; Above example gets a \u0026ldquo;data\u0026rdquo; which is map and then reads a \u0026ldquo;ca.crt\u0026rdquo; key out of it. This pattern will work also for other secrets. Without jq and complicated jsonpath. ;)\n","permalink":"https://dywicki.pl/2023/05/one-shot-retrieval-of-tls-certificates-from-kubernetes/","summary":"I believe that I am not only one who needed to get a secret value out of kubernetes. Usual advice focus on use of \u003ccode\u003ekubectl\u003c/code\u003e with json output and/or jsonpath. Problem is - the \u003ccode\u003ekubectl\u003c/code\u003e jsonpath is limited thus result needs to be scanned further by jq. It works, but jq is not part of everyone\u0026rsquo;s working environment.","title":"One shot retrieval of TLS certificates from Kubernetes"},{"content":"A long time ago I wrote an post about Configuration Admin and Arrays. Recently I attempted to use this feature with openHAB and I found that it doesn\u0026rsquo;t work with its configuration framework.\nLocating issue was rather fast as I got HTTP 500 while retrieving configuration through openHAB rest API. After implementing a basic patch to properly map parameter values which come as an array I attempted modification through openHAB user interface. I found that reading works, however what I wrote initially as an array:\nperiods=[\u0026#34;DAY\u0026#34;, \u0026#34;MONTH\u0026#34;, \u0026#34;YEAR\u0026#34;] Was later stored as an list:\nperiods = ( \\ \u0026#34;YEAR\u0026#34;, \\ \u0026#34;MONTH\u0026#34;, \\ \u0026#34;DAY\u0026#34;, \\ \u0026#34;HOUR\u0026#34;, \\ ) Turns out that my code modifications are not needed. It is sufficient to keep configuration input properly formatted. Keep in mind that () allows to bring an collections. The prefix syntax described in earlier post, which determine type of contained elements, should still apply!\n","permalink":"https://dywicki.pl/2022/02/apache-felix-configuration-admin-and-collections/","summary":"A long time ago I wrote an post about \u003ca href=\"/2015/02/apache-felix-configuration-admin-with-array-values/\"\u003eConfiguration Admin and Arrays\u003c/a\u003e. Recently I attempted to use this feature with openHAB and I found that it doesn\u0026rsquo;t work with its configuration framework.","title":"Apache Felix Configuration Admin and collections"},{"content":"A while ago I managed to get Nginx and OpenID Connect working together. Obviously, there is a commercial plugin provided by Nginx authors, but for simpler cases we can use a a simpler way with auth_request which works nicely with OIDC token introspection served by for example Keycloak.\nBackground for work For some time I had a case where I had to cover legacy service which used HTTP basic auth and combine it with new one which used proper OAuth2. For that I combined nginx rewrite module with auth_request rule.\nConfig recipe location /rest/ { auth_request /_auth; proxy_set_header Authorization \u0026#34;Basic ${SERVER_AUTH}\u0026#34;; proxy_pass ${SERVER_URI}; } location /api/ { rewrite ^/api/(.*) $1 break; proxy_pass ${API_URI}$uri$is_args$args; } location = /_auth { internal; proxy_pass_request_body off; proxy_method POST; proxy_set_header Authorization \u0026#34;Basic ${OIDC_CLIENT_AUTH}\u0026#34;; proxy_set_header Content-Type \u0026#34;application/x-www-form-urlencoded\u0026#34;; proxy_set_body \u0026#34;token=$http_x_api_key\u0026amp;token_hint=access_token\u0026#34;; proxy_pass ${OIDC_INTROSPECT_URI}; } In above case there are 5 environment variables, out of which two are basic auth ceredentials:\nSERVER_AUTH - basic authentication to be injected for legacy service. SERVER_URI - legacy service location. API_URI - location of new service(s). OIDC_CLIENT_AUTH - basic authentication for nginx client in OAuth 2/OIDC server aka identity provider. OIDC_INTROSPECT_URI - token validation address In above situation the /rest endpoint is rewritten to legacy service. What is remarkable - this endpoint uses X-API-KEY header which is then forwarded to token introspection via $http_x_api_key request variable.\nI can\u0026rsquo;t remember right now why, but Authorization header didn\u0026rsquo;t work with auth_check for some reason.\nHow does it work Once configuration was set and started working it turns to be trivial. Getting there.. is usual hours long search for working snipped across various versions of Nginx.\nIn above example any call coming to /rest gets forwarded to internal /_auth location which is invisible over regular HTTP port. Answer from this endpoint different than 200 indicates that authentication have failed.\nThis means that standard HTTP error codes such 401/ 403/ 500 will instruct nginx to hold processing and return error to the caller.\nDockerfile The last point is making it working with Docker. While working on this issue I learned that it is possible to have environment inject with Nginx image.\nAll you have to do is to locate your config which uses environment variables in /etc/nginx/templates/default.conf.template. By default when spinning new instance Nginx will pre-process config and substitute ${PLACEHOLDERS} with their values pushed via docker env.\nI know you probably went over dozen of configuration fragments before coming here and you just got one more to test. Hope its gonna work for you!\n","permalink":"https://dywicki.pl/2020/11/nginx-verify-oidc-token-before-rewrite/","summary":"A while ago I managed to get Nginx and OpenID Connect working together. Obviously, there is a commercial plugin provided by Nginx authors, but for simpler cases we can use a a simpler way with \u003ccode\u003eauth_request\u003c/code\u003e which works nicely with OIDC token introspection served by for example Keycloak.","title":"Nginx: verify OIDC token before rewrite"},{"content":"Remote repositories are one of fundamental concepts promoted by Maven. Its use is quite common. It is handled by maven-deploy-plugin, so its migration is something you will probably notice sooner or later. This short post will explain changes in new version of plugin which you need to take care of when moving from 2.8 or earlier releases.\nMost of maven builds I interact with still rely on 2.5.x version of maven deploy plugin. New one brings few significant changes:\nNo support for uniqueVersion option, as of deploy plugin 3.0 all deployed snapshots must contain timestamp information. For example deployed snapshot is:\norg.openhab.core.reactor.bom-3.0.0-20201027.211058-7.pom and not\norg.openhab.core.reactor.bom-3.0.0-SNAPSHOT.pom. No support for legacy repositories, coming from changes in deployment scheme. In result of above parameter format for below parameters changed from id::layout::url to id::url: altDeploymentRepository altReleaseDeploymentRepository altSnapshotDeploymentRepository Especially last change is tricky one because it is easy to miss it. If you used plugin prior 3.0 release and just updated its version you need to amend eventual CI pipelines which relied on above, otherwise authorization information will not be used.\nIf you experience troubles with access and get all kinds of HTTP 401 errors then verify if your failed build contains similar lines:\n[INFO] --- maven-deploy-plugin:3.0.0-M1:deploy (default-cli) @ org.openhab.core.reactor --- [INFO] Using alternate deployment repository internal::default::https://repo.myorg.com/repository/ Downloading from internal::default: https://repo.myorg.com/repository/org/openhab/core/org.openhab.core.reactor/3.0.0-SNAPSHOT/maven-metadata.xml [WARNING] Could not transfer metadata org.openhab.core:org.openhab.core.reactor:3.0.0-SNAPSHOT/maven-metadata.xml from/to internal::default (https://repo.myorg.com/repository/): Authentication failed At first look everything looks fine - you have alt deployment uri, custom server with internal identifier. Yet whole process blows for no reason.\nActual reason is that maven deploy plugin looks for server called internal::default and not internal. This results in no authorization header sent to remote repository which obviously answers with 401.\nOne more thing to be aware - keep in mind taht there is difference between mvn deploy call which calls a phase and mvn deploy:deploy which calls a single plugin and org.apache.maven.plugins:maven-deploy-plugin:3.0.0-M1:deploy call. Last one requests specific version of plugin to be used. In first two cases version of maven deploy plugin used during execution of build is determined from actual project configuration. This means that your parents or maven super pom drives that for you.\n","permalink":"https://dywicki.pl/2020/10/maven-deploy-plugin-3-0-0-m1/","summary":"Remote repositories are one of fundamental concepts promoted by Maven. Its use is quite common. It is handled by \u003ca href=\"https://maven.apache.org/plugins/maven-deploy-plugin/\"\u003emaven-deploy-plugin\u003c/a\u003e, so its migration is something you will probably notice sooner or later. This short post will explain changes in new version of plugin which you need to take care of when moving from 2.8 or earlier releases.","title":"Maven deploy plugin 3.0.0-M1"},{"content":"One of most annoying things these days is managing CI/CD with multiple repositories. For quite long time github didn\u0026rsquo;t offer private repositories while gitlab did. This lead to situation where people been asking gitlab for additional features. One of these is support for mirroring git repositories from github to gitlab just to run pipelines. I happened to have the same requirement.\nI\u0026rsquo;ve run into github-gitlab scenario already several times and never really found a good-enough solution. We are lucky that github eventually started to fight for CI/CD market. Their workflows makes it possible to run actions of various kinds. It seems to be an awesome tool, yet I found it confusing. Learning another syntax after spending several years with competitors (circle, travis, gitlab) seems to be.. redundant. Without time to learn new tool I had to adopt existing ones.\nOne of things I realized is that Gitlab started to support scheduled pipelines. Making a cron job is doable, just like with good old Jenkins. More over, it seems to be possible to detect if job execution is triggered by push or schedule event.\nMy desired setup is like this:\n[github/openhab] [ gitlab ] [github/fork ] [ openhab-core ] ----\u0026gt; [ openhab-core ] ----\u0026gt; [ openhab-core ] ^ | `- CI/schedule is here As well as additional jobs Because openHAB project uses Travis CI and Jenkins adding gitlab pipeline didn\u0026rsquo;t make much sense for them. Obviously it is a common scenario that you must add .gitlab-ci.yml somewhere. Remember, we talk about free plans and limited options. We do not have access to source repository to stick with repository triggers. The only option is polling for new commits.\nEquipped in this knowledge (after several hours of fruitless attempts) I made my first scripted attempt which adds new remote, fetch changes from it, then push update to gitlab and github forks. An important note here, script uses rebase - so it tries to put my commits (in this case its only CI config) on top of remote ones.\n#!/bin/bash # dear executor, please help me debugging this... # set -xv # env # Below script uses two tokens - one to push changes to github (GITHUB_MIRROR_TOKEN) and other to push back rebased changes to gitlab (GITLAB_TOKEN). # Git complains on rebase as it must have some indication of author git config user.email \u0026#34;bot@myorg.com\u0026#34; git config user.name \u0026#34;MyOrg Bot\u0026#34; # get repository path without username/password and scheme repoUrl=$(git remote get-url origin | cut -d \u0026#39;@\u0026#39; -f 2) git fetch origin git remote set-url origin \u0026#34;https://gitlab-ci-token:${GITLAB_TOKEN}@${repoUrl}\u0026#34; git remote add --tags openhab https://github.com/openhab/openhab-core.git git remote add connectorio --tags https://${GITHUB_MIRROR_TOKEN}:x-oauth-basic@github.com/ConnectorIO/openhab-core.git git fetch --unshallow openhab git pull --rebase openhab master git push --tags -f origin HEAD:master # push all but last commit to github git push --tags -f connectorio HEAD~1:master This is already working version with all adjustments cause --unshallow was needed to push openhab tags to fork and gitlab mirror.\nWhole procedure is:\nSetup git author information Prepare \u0026ldquo;push\u0026rdquo; url Fetch changes from gitlab Override gitlab remote url, so writes are permitted Add source repository (openhab) Add github fork (connectorio) Fetch changes from source repo and rebase CI/CD config on top of it Push updated code to gitlab Push updated code (without CI/CD config) to github fork Pipeline I tested was quite simple:\nstages: - mirror - build - deploy mirror: stage: mirror script: ./.gitlab/sync.sh # build \u0026amp; deploy sections Duplicate job executions Shortly after I committed first revision I realized that such script can run forever just doing empty pulls and empty pushes. A naive approach I took would cause second build with same changes as mirror sync. I had to look for exit or entry condition which could drive execution across different paths. First one is - synchronization of repositories, second one is - maven build and deploy. I didn\u0026rsquo;t need to run both at the same time. For many reasons sync could be left alone to signal only synchronization issues and not build failures. This means that git pull --rebase openhab master is primary command to watch. Obviously above script doesn\u0026rsquo;t do any checks - you can build these yourself as my main goal here is cyclic deployment of fresh snapshots for further integration builds.\nOnce I started to dig into .gitlab-ci.yml syntax I found a mysterious element called rules. Normally I used only/refs and except/refs, but this one really matched my case. Rules support basic conditions such if, exists and changes. It turns out that only can be paired with variables giving similar result to branch filtering, so if any of my advice here doesn\u0026rsquo;t fly for any reason, try it. My complete pipeline definition is below:\nstages: - mirror - build - deploy mirror: stage: mirror script: ./.gitlab/sync.sh rules: - if: \u0026#39;$CI_PIPELINE_SOURCE == \u0026#34;schedule\u0026#34;\u0026#39; build: stage: build script: - \u0026#39;mvn -s .gitlab/settings.xml $MAVEN_CLI_OPTS $MAVEN_OPTS_EXTRA package -DnoOhSnapRepo\u0026#39; rules: - if: \u0026#39;$CI_PIPELINE_SOURCE != \u0026#34;schedule\u0026#34;\u0026#39; deploy: stage: deploy script: - if [ ! -f .gitlab/settings.xml ]; then echo \u0026#34;The .gitlab/settings.xml file is missing.\u0026#34;; fi - mvn $MAVEN_CLI_OPTS $MAVEN_OPTS_EXTRA -s .gitlab/settings.xml package org.apache.maven.plugins:maven-deploy-plugin:3.0.0-M1:deploy -DaltSnapshotDeploymentRepository=\u0026#34;${MAVEN_SERVER_ID}::${SNAPSHOT_REPOSITORY_URL}\u0026#34; -Dmaven.test.skip.exec=true -DskipChecks -DnoOhSnapRepo rules: - if: \u0026#39;$CI_PIPELINE_SOURCE != \u0026#34;schedule\u0026#34;\u0026#39; Conclusions \u0026amp; CI/CD rants Nowadays CI/CD landscape is different than what we had a decade ago. Some things are simpler, some are more complex than before. I still remember Hudson/Jenkins split in 2000\u0026rsquo;s, awful UI, never ending Jenkins build queue, fun with management of slave nodes and inventions around Jenkinsfile and job dsl. Sometimes I even miss these, because with Jenkins you could control everything. Reality is - no small company can sacrifice a man to continuously maintain CI/CD installation. Present difficulty is - how to achieve as much as Jenkins with smaller resource commitment?\nThe resource commitment is a critical point. I did and got paid for many strange things in past - ie. as part of one task I was automating release process (as I become a client release monkey). First releases were nightmare due to complex nature of pipeline. Once I even ended visiting Jenkins slave workspace through SSH to finish hours long product release. Whole thing blew up at rpm build during perform stage which did not trigger in my Maven build on mac. Beauty of situation was - I was able to fix situation in ad-hoc manner (read whole evening and half of the night), just by tweaking rpm spec.\nAll above changed with containerization which made it simpler to run an abstract executor locally in sandbox manner. We can have execution, but state is volatile. It is kept in other tools which simply store build artifacts. On one hand it is much easier to reproduce build, on other it is much harder to debug it when something goes wrong. Anyone who ever tried to find why gitlab executor does something odd running in Kubernetes knows what I mean.\nIn the end, lets enjoy our forks! :-)\n","permalink":"https://dywicki.pl/2020/10/gitlab-ci-for-github-fork/","summary":"One of most annoying things these days is managing CI/CD with multiple repositories. For quite long time github didn\u0026rsquo;t offer private repositories while gitlab did. This lead to situation where people been asking gitlab for additional features. One of these is support for mirroring git repositories from github to gitlab just to run pipelines. I happened to have the same requirement.","title":"Gitlab CI for github fork"},{"content":"In this pretty short introduction I am going to describe you a trick which I learned recently while doing new CI/CD configuration for one of open source projects I work with.\nWhole problem in forks is cost of their creation and maintenance. Obviously we do our own releases for various reasons. Starting point for what I am writing here is another remote GIT repository which contains several extra commits awaiting, let say for official release from origin maintainers.\n$ git remote -v github\tgit@github.com:splatch/openhab-core.git (fetch) github\tgit@github.com:splatch/openhab-core.git (push) origin\tgit@github.com:openhab/openhab-core.git (fetch) origin\tgit@github.com:openhab/openhab-core.git (push) You can\u0026rsquo;t hold your company product release due to 3rd party which takes too long to process your PR, so you fork and release internally. What you probably don\u0026rsquo;t want to do is handcrafting of Maven POMs just to get release done to your internal repository.\nInteresting part is how to perform fork-release without any pom updates. Some projects, which have mixed commercial/open source use, allow to configure distribution management (snapshots \u0026amp; releases) through properties, some do not.\nIn reality most of projects do not bother about that, being busy with bugs reported by users. Almost certainly it will not allow to specify project repository URL where tag should be created.\nEach Maven project which gets released through all sorts of plugins have SCM information. Something like this:\n\u0026lt;scm\u0026gt; \u0026lt;connection\u0026gt;scm:git:https://github.com/openhab/openhab-core.git\u0026lt;/connection\u0026gt; \u0026lt;developerConnection\u0026gt;scm:git:https://github.com/openhab/openhab-core.git\u0026lt;/developerConnection\u0026gt; \u0026lt;tag\u0026gt;HEAD\u0026lt;/tag\u0026gt; \u0026lt;url\u0026gt;https://github.com/openhab/openhab-core\u0026lt;/url\u0026gt; \u0026lt;/scm\u0026gt; \u0026lt;distributionManagement\u0026gt; \u0026lt;repository\u0026gt; \u0026lt;id\u0026gt;bintray\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;${oh.repo.distBaseUrl}/openhab-core/;publish=1\u0026lt;/url\u0026gt; \u0026lt;/repository\u0026gt; \u0026lt;snapshotRepository\u0026gt; \u0026lt;id\u0026gt;jfrog\u0026lt;/id\u0026gt; \u0026lt;url\u0026gt;${oh.repo.snapshotBaseUrl}/libs-snapshot-local\u0026lt;/url\u0026gt; \u0026lt;/snapshotRepository\u0026gt; \u0026lt;/distributionManagement\u0026gt; When you do internal release you usually change above sections to match your fork URI and repository manager.\nIt turns out that it is possible to avoid POM updates and rely on basic operations which will let maven use new locations. All you need are custom maven settings.xml file plus few switches and one trick.\nUse fork instead of origin You need to tag your fork, not an origin to which you have no access. To do such thing we can utilize resume option for prepare goal from maven-release-plugin. Resume lets you continue release process if it breaks. Whole thing rely on existence of release.properties file from prior attempt. This means that we can prepare contents of this file for our release and just continue.\nIf you will have a look into this file there are couple of entries related to scm repository, but most important is scm.url. If you set it to valid Maven SCM URI then scm set in project is ignored. :-)\nSo our release script should start from:\necho \u0026#34;scm.url=scm:git:$CI_REPOSITORY_URL\u0026#34; \u0026gt;\u0026gt; release.properties Use own maven repository Second step is really preparation of options for release:perform call. I skip preparation, cause it\u0026rsquo;s rather straight. The trick is how to override distribution management.\nWe know that deploy is done by deploy:deploy goal of maven-deploy-plugin. Starting from version 3.x line (btw 3.0.0.M1 is over two years old at moment of writing) it does support altReleaseDeploymentRepository option.\nKnowing this our perform call should look like this:\nmvn release:perform -Darguments=\u0026#34;-DaltReleaseDeploymentRepository=$SERVER_ID::default::$SERVER_URL\u0026#34; As far I remember the format of option is slightly different from altDeploymentRepository, so you need to test it.\nWith these two things your jenkins should be able to pick pretty much any repo and tag it at any point of time with no issues.\nFinal script Given that above snippets are not complete here you will find all-in-one. I skip .mvn/ci-settings.xml as it needs to define necessary server sections. I also omit .mvn/release.config file which should contain list of extra parameters for release:prepare and perform calls.\n#!/bin/bash if [ ! -f .mvn/ci-settings.xml ]; then echo \u0026#34;The .mvn/ci-settings.xml file is missing.\u0026#34;; exit 1 fi if [[ -z \u0026#34;${CI_REPOSITORY_URL}\u0026#34; ]]; then echo \u0026#34;You must set CI_REPOSITORY_URL pointing to tag repository location, ie. git@github.com:foo/bar.git\u0026#34; exit 1 fi if [[ -z \u0026#34;${RELEASE_VERSION}\u0026#34; || -z \u0026#34;${DEVELOPMENT_VERSION}\u0026#34; ]]; then echo \u0026#34;You must set RELEASE_VERSION pointing DEVELOPMENT_VERSION variables before calling script\u0026#34; exit 1 fi releaseOptions=$(\u0026lt;.mvn/release.config) releaseOptions=$(echo $releaseOptions|sed \u0026#34;s/\\n/ /g\u0026#34;) echo \u0026#34;scm.url=scm:git:$CI_REPOSITORY_URL\u0026#34; \u0026gt;\u0026gt; release.properties mvn -s .mvn/ci-settings.xml $MAVEN_CLI_OPTS $MAVEN_OPTS_EXTRA $releaseOptions release:prepare \\ -DautoVersionSubmodules=true \\ -DreleaseVersion=${RELEASE_VERSION} \\ -DdevelopmentVersion=${DEVELOPMENT_VERSION} \\ -Dtag=openhab-${RELEASE_VERSION} \\ -Darguments=\u0026#34;$releaseOptions\u0026#34; \\ -DpreparationGoals=\u0026#34;clean install\u0026#34; preparationStatus=$? if [[ $preparationStatus -eq 0 ]]; then echo \u0026#34;========================================\u0026#34; echo \u0026#34;Performing release of ${RELEASE_VERSION}\u0026#34; echo \u0026#34;========================================\u0026#34; mvn -s .mvn/ci-settings.xml $MAVEN_CLI_OPTS $MAVEN_OPTS_EXTRA release:perform \\ -Darguments=\u0026#34;-s $PWD/.mvn/ci-settings.xml ${releaseOptions}\u0026#34; \\ -Dgoals=\u0026#34;deploy\u0026#34; fi # call it with RELEASE_VERSION=x DEVELOPMENT_VERSION=y CI_REPOSITORY_URL=z ./ci-release.sh Enjoy yor forks! :-)\n","permalink":"https://dywicki.pl/2020/10/releasing-forked-maven-projects-with-ease/","summary":"In this pretty short introduction I am going to describe you a trick which I learned recently while doing new CI/CD configuration for one of open source projects I work with.","title":"Releasing forked Maven projects with ease"},{"content":"Wireshark is outstanding piece of software. I had few chances to make use of it, early in my IT days, then in one of bigger projects I worked on, and finally now - when I began working on industrial integration protocols.\nThis blog post will describe a journey I had with Wireshark over last couple of weeks and tell you why clicking couple of times everywhere you CAN is sometimes better.\nMost of us knows Wireshark as network traffic analysis tool. Over years it really evolved and permits to scan USB and even Bluetooth traffic. This makes it great candidate for other \u0026ldquo;serial\u0026rdquo; protocols, especially ones used in hardware and automation. I used this tool couple of months ago to implement Apache PLC4X and MSpec describing Link-Layer-Discovery-Protocol (LLDP) as well as for Profinet Discovery and Configuration Protocol (Profinet-DCP).\nRecently I\u0026rsquo;ve started working on CANopen implementation in Java and faced sad reality of automation market. Almost each and every tool is paid. Existing open-source toolkits offer quite poor functionality or function as a library with limited functionality.\nNaturally I did search for Wireshark + CAN multiple times over Google, DuckDuckGo. Each time I was getting the same answer: Enable CANopen protocol in Wireshark. It is simple, it must be simple, am I right?\nI looked at CAN preferences, but options mentioned in this answer were gone. Other suggestions on using next dissector lead me to window which did not give a CANopen anywhere.\nNow, the real answer you probably look for - you need to know which column to click. I kept clicking first column and getting everything but not CANopen. Below you can see Wireshark popup window you probably don\u0026rsquo;t use most of the time:\nDecode as .. Wireshark dissector selection.\nFinally, after being caught in this trap for two weeks, if not longer, I found answer. Exhausted by lack of success I finally went to Windows and started clicking over all columns. It turned out that LAST column is editable and that\u0026rsquo;s the place where you can actually pick higher layer protocol for CAN.\nYou just need to click into it, if it doesn\u0026rsquo;t work (as it didn\u0026rsquo;t for me) then click twice. Pay attention cause Wireshark GUI sometimes gets tricky with \u0026ldquo;clicks\u0026rdquo;, at least that was case for my linux distro. Just now I been trying to see if I was that dumb that I did not click this bloody column or my X manager played fool with me. Probably both, cause now by trying I managed to \u0026ldquo;freeze\u0026rdquo; row and block the dropdown.\nWhen you click last column in a right spot it will show a drop down list.\nChanging Wireshark dissector.\nHope that it will help others who ran into same trouble. I must admit that I did not expect myself being stuck on such basic issue for so long. Especially with program I knew for more than decade. It seems that the older I get, the easier I get stuck with such basic things. Remember, if something doesn\u0026rsquo;t work just keep clicking, eventually it will.\n","permalink":"https://dywicki.pl/2020/09/using-wireshark-with-can-and-canopen/","summary":"\u003ca href=\"http://wireshark.org/\"\u003eWireshark\u003c/a\u003e is outstanding piece of software. I had few chances to make use of it, early in my IT days, then in one of bigger projects I worked on, and finally now - when I began working on industrial integration protocols.{{ double-space-with-newline }}This blog post will describe a journey I had with Wireshark over last couple of weeks and tell you why clicking couple of times everywhere you \u003cem\u003eCAN\u003c/em\u003e is sometimes better.","title":"Using Wireshark with CAN and CANopen"},{"content":"Over past couple of months I\u0026rsquo;ve been playing with gitlab ci as it brings a bit of refreshing breeze after years of struggle with Jenkins. Don\u0026rsquo;t get me wrong, I value what Jenkins did for us over past decade. I just think that maintenance of it is a bit of nightmare. Yet, I\u0026rsquo;ve reached a place where I had to adjust some more stuff for gitlab in order to get my builds straight.\nIssues I\u0026rsquo;ve had are mainly related to customization of deploy phase and maven repositories. Today I\u0026rsquo;ve learned that recent version of Maven (it is 3.6.3 at the moment of writing) requires distinct mirror identifiers.\nSomewhat I missed that. I always thought it was used just to lookup credentials for authorization. I was wrong or never before used multiple mirrors in one maven settings.\nTo let you understand my motivation, I will briefly describe a case. I was trying to get side build for openHAB project which would be under my control. In order to do such I have to take care about all openHAB dependencies which are distributed over different places. Some of them hold same contents as Maven central, while other do hold project specific releases of its dependencies - namely various OSGi wrappers, repackaging of dependencies necessary by some legacy Eclipse related modules and so on. If I would hid behind one big proxy I would never spot changes in dependency chains. Having multiple mirrors which are aimed to proxy individual repositories allows to control where new dependencies come from during the build time. Once it is cached it is extremely hard to find which build pulled it over.\nThis means that even if you have same proxy server instance using same credentials, you need to declare mirrors with different ID. Take a look on below example:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34; ?\u0026gt; \u0026lt;settings xsi:schemaLocation=\u0026#34;http://maven.apache.org/SETTINGS/1.1.0 http://maven.apache.org/xsd/settings-1.1.0.xsd\u0026#34; xmlns=\u0026#34;http://maven.apache.org/SETTINGS/1.1.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;\u0026gt; \u0026lt;servers\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;${env.MAVEN_SERVER_ID}\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt;${env.MAVEN_REPO_USER}\u0026lt;/username\u0026gt; \u0026lt;password\u0026gt;${env.MAVEN_REPO_PASS}\u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; \u0026lt;/servers\u0026gt; \u0026lt;mirrors\u0026gt; \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;${env.MAVEN_SERVER_ID}\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;${env.CENTRAL_PROXY_MIRROR_OF}\u0026lt;/mirrorOf\u0026gt; \u0026lt;url\u0026gt;${env.CENTRAL_PROXY_URL}\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt; \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;${env.MAVEN_SERVER_ID}\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;${env.OPENHAB_PROXY_MIRROR_OF}\u0026lt;/mirrorOf\u0026gt; \u0026lt;url\u0026gt;${env.OPENHAB_PROXY_URL}\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt; \u0026lt;/mirrors\u0026gt; \u0026lt;/settings\u0026gt; As you can see, there are two mirrors using same id. In effect Maven will use just one of them. You can inspect this with Maven help plugin:\nMAVEN_SERVER_ID=proxy \\ MAVEN_REPO_USER=test \\ MAVEN_REPO_PASS=secret \\ CENTRAL_PROXY_MIRROR_OF=central \\ CENTRAL_PROXY_URL=http://proxy/central/ \\ OPENHAB_PROXY_MIRROR_OF=openhab-mirror \\ OPENHAB_PROXY_URL=http://proxy/openhab \\ mvn -s .gitlab/settings.xml help:effective-settings The result, at least till this morning for me, is quite surprising. The openhab mirror is lost, see below output!\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!-- ====================================================================== --\u0026gt; \u0026lt;!-- --\u0026gt; \u0026lt;!-- Generated by Maven Help Plugin on 2020-07-30T12:04:19+02:00 --\u0026gt; \u0026lt;!-- See: http://maven.apache.org/plugins/maven-help-plugin/ --\u0026gt; \u0026lt;!-- --\u0026gt; \u0026lt;!-- ====================================================================== --\u0026gt; \u0026lt;!-- ====================================================================== --\u0026gt; \u0026lt;!-- --\u0026gt; \u0026lt;!-- Effective Settings for \u0026#39;splatch\u0026#39; on \u0026#39;arch\u0026#39; --\u0026gt; \u0026lt;!-- --\u0026gt; \u0026lt;!-- ====================================================================== --\u0026gt; \u0026lt;settings xmlns=\u0026#34;http://maven.apache.org/SETTINGS/1.1.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34; http://maven.apache.org/SETTINGS/1.1.0 http://maven.apache.org/xsd/settings-1.1.0.xsd \u0026#34;\u0026gt; \u0026lt;localRepository\u0026gt;...\u0026lt;/localRepository\u0026gt; \u0026lt;servers\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;proxy\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt;test\u0026lt;/username\u0026gt; \u0026lt;password\u0026gt;secret\u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; \u0026lt;/servers\u0026gt; \u0026lt;mirrors\u0026gt; \u0026lt;mirror\u0026gt; \u0026lt;mirrorOf\u0026gt;central\u0026lt;/mirrorOf\u0026gt; \u0026lt;url\u0026gt;http://proxy/central\u0026lt;/url\u0026gt; \u0026lt;id\u0026gt;proxy\u0026lt;/id\u0026gt; \u0026lt;/mirror\u0026gt; \u0026lt;/mirrors\u0026gt; \u0026lt;!-- ommited --\u0026gt; \u0026lt;/settings\u0026gt; In order to get whole thing working you need to declare distinct server identifiers:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34; ?\u0026gt; \u0026lt;settings xsi:schemaLocation=\u0026#34;http://maven.apache.org/SETTINGS/1.1.0 http://maven.apache.org/xsd/settings-1.1.0.xsd\u0026#34; xmlns=\u0026#34;http://maven.apache.org/SETTINGS/1.1.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;\u0026gt; \u0026lt;servers\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;${env.MAVEN_SERVER_ID}\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt;${env.MAVEN_REPO_USER}\u0026lt;/username\u0026gt; \u0026lt;password\u0026gt;${env.MAVEN_REPO_PASS}\u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;${env.OPENHAB_PROXY_ID}\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt;${env.MAVEN_REPO_USER}\u0026lt;/username\u0026gt; \u0026lt;password\u0026gt;${env.MAVEN_REPO_PASS}\u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; \u0026lt;server\u0026gt; \u0026lt;id\u0026gt;${env.CENTRAL_PROXY_ID}\u0026lt;/id\u0026gt; \u0026lt;username\u0026gt;${env.MAVEN_REPO_USER}\u0026lt;/username\u0026gt; \u0026lt;password\u0026gt;${env.MAVEN_REPO_PASS}\u0026lt;/password\u0026gt; \u0026lt;/server\u0026gt; \u0026lt;/servers\u0026gt; \u0026lt;mirrors\u0026gt; \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;${env.CENTRAL_PROXY_ID}\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;${env.CENTRAL_PROXY_MIRROR_OF}\u0026lt;/mirrorOf\u0026gt; \u0026lt;url\u0026gt;${env.CENTRAL_PROXY_URL}\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt; \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;${env.OPENHAB_PROXY_ID}\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;${env.OPENHAB_PROXY_MIRROR_OF}\u0026lt;/mirrorOf\u0026gt; \u0026lt;url\u0026gt;${env.OPENHAB_PROXY_URL}\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt; \u0026lt;/mirrors\u0026gt; \u0026lt;/settings\u0026gt; Its not nicest, but it works. Also you can do an variant with suffixes added after ${env.MAVEN_SERVER_ID} . By this way you will limit number of environment variables which needs to be passed to build script.\nAlso keep in mind that Maven currently does not have support for \u0026ldquo;fallback\u0026rdquo; values you would expect. If environment variable is missing placeholder will be left intact. The ${env.var:-default} is not supported.\n","permalink":"https://dywicki.pl/2020/07/multiple-maven-mirros-gitlab-ci/","summary":"Over past couple of months I\u0026rsquo;ve been playing with gitlab ci as it brings a bit of refreshing breeze after years of struggle with Jenkins. Don\u0026rsquo;t get me wrong, I value what Jenkins did for us over past decade. I just think that maintenance of it is a bit of nightmare. Yet, I\u0026rsquo;ve reached a place where I had to adjust some more stuff for gitlab in order to get my builds straight.","title":"Maven mirros and gitlab ci"},{"content":"I\u0026rsquo;ve been looking for some ways to use Mockito to with builder pattern using fluent calls. There are couple of articles online, however none of these addressed my case, a builder which receives calls and returns fully working object after a final \u0026lsquo;\u0026lsquo;build\u0026rsquo;\u0026rsquo; call without the need for further setup.\nTL;DR You can combine multiple mocks using Mockito\u0026rsquo;s answer implementations which mocks getter calls on a second mock instance.\nProblem overview You might ask why I would care about that, well - in my case a Builder is an interface and built thing is also an interface resulting in situation that I have to mock two instances in order to make test pass. First - the builder to keep returning itself, second an object which needs to be created at the end.\nIn this short post I will explain how I did it. You might find this code useful especially if you do have a lot of mocking of builders and their return values and look for a way to unify that part.\nThat\u0026rsquo;s the order of actions we gonna do:\nCreate a instance of mock builder - in my case BridgeBuilder. Create instance of type to be created by builder - a Bridge in below code. For each call on BridgeBuilder we will mock a getter call on Bridge with return value set via builder. I come to following shape of code which covers the above flow. That\u0026rsquo;s how the mocking preparation looks a like:\nBridgeBuilder bridgeBuilder = mock(BridgeBuilder.class, \u0026#34;bridgeBuilderMock\u0026#34;); Bridge bridge = mock(Bridge.class, \u0026#34;bridgeMock\u0026#34;); builder( bridgeBuilder, () -\u0026gt; bridgeBuilder.withConfiguration(any()), bridge::getConfiguration, new MappedReturnValue\u0026lt;\u0026gt;() ); builder(bridgeBuilder, () -\u0026gt; bridgeBuilder.withBridge(any()), bridge::getBridgeUID, new MappedReturnValue\u0026lt;\u0026gt;()); builder(bridgeBuilder, () -\u0026gt; bridgeBuilder.withLabel(any()), bridge::getLabel, new MappedReturnValue\u0026lt;\u0026gt;()); builder(bridgeBuilder, () -\u0026gt; bridgeBuilder.withLocation(any()), bridge::getLocation, new MappedReturnValue\u0026lt;\u0026gt;()); builder(bridgeBuilder, () -\u0026gt; bridgeBuilder.withChannels(anyList()), bridge::getChannels, new MappedReturnValue\u0026lt;\u0026gt;()); when(bridgeBuilder.build()).thenReturn(bridge); As you can see it is relatively simple. It is a short notation to say that we return a builder (line 4). Additionally for each call on this mock (line 5) we will setup a getter call (line 6) using a MappedReturnValue function (line 7). Lets take a look on builder method.\nprivate \u0026lt;T, X\u0026gt; void builder(T mock, Supplier\u0026lt;T\u0026gt; builderCall, Supplier\u0026lt;X\u0026gt; getterCall, Function\u0026lt;InvocationOnMock, X\u0026gt; returnValue) { MapBuilderToMock\u0026lt;T, X\u0026gt; answer = new MapBuilderToMock\u0026lt;\u0026gt;(mock, getterCall, returnValue); when(builderCall.get()).thenAnswer(answer); } That\u0026rsquo;s a generic block which you would normally see - with one difference there is no \u0026ldquo;mock\u0026rdquo; returned but an answer. I\u0026rsquo;ve decided also to use a separate class instead of anonymous/lambda in order to increase readiness.\nIf you come to this article you probably know the syntax of when and thenAnswer, which is used above.\nI will just note here that answer implementation is superior to thenReturn cause it will have access to arguments passed to builder method call. With such powerful tool we can execute additional logic, and that\u0026rsquo;s what we really need. More importantly as you can see, you can record calls in lambda. It is possible because Supplier can be seen as portion of code which should be executed each time when its get() method is executed. For Mockito it doesn\u0026rsquo;t matter how we call code, as long as it is able to record method invocations. That\u0026rsquo;s why tool doesn\u0026rsquo;t complain with when(builderCall.get()).\nLets take a look on the MapBuilderToMock answer:\nstatic class MapBuilderToMock\u0026lt;T, X\u0026gt; implements Answer\u0026lt;T\u0026gt; { private final T builder; private final Supplier\u0026lt;X\u0026gt; getter; private final Function\u0026lt;InvocationOnMock, X\u0026gt; returnValue; public MapBuilderToMock(T builder, Supplier\u0026lt;X\u0026gt; getter, Function\u0026lt;InvocationOnMock, X\u0026gt; returnValue) { this.builder = builder; this.getter = getter; this.returnValue = returnValue; } @Override public T answer(InvocationOnMock invocation) throws Throwable { when(getter.get()).thenReturn(returnValue.apply(invocation)); return builder; } } What is really important is line 13, where we have another lambda call which transforms builder invocation to return value of a lambda. Very short, initial approach can be achieved by this:\nwhen(getter.get()).thenReturn(invocation.getArgument(0)); Working code After explanation - we can see whole code. Additional classes makes it longer than necessary, however my intention here is keeping these blocks separate in order to increase readability.\n@Test public void testDifferentHandlerFactoriesForBridgeAndThing() { BridgeBuilder bridgeBuilder = mock(BridgeBuilder.class, \u0026#34;bridgeBuilder\u0026#34;); Bridge bridge = mock(Bridge.class, \u0026#34;bridge\u0026#34;); builder(bridgeBuilder, () -\u0026gt; bridgeBuilder.withConfiguration(any()), bridge::getConfiguration, new MapArgument\u0026lt;\u0026gt;()); builder(bridgeBuilder, () -\u0026gt; bridgeBuilder.withBridge(any()), bridge::getBridgeUID, new MapArgument\u0026lt;\u0026gt;()); builder(bridgeBuilder, () -\u0026gt; bridgeBuilder.withLabel(any()), bridge::getLabel, new MapArgument\u0026lt;\u0026gt;()); builder(bridgeBuilder, () -\u0026gt; bridgeBuilder.withLocation(any()), bridge::getLocation, new MapArgument\u0026lt;\u0026gt;()); builder(bridgeBuilder, () -\u0026gt; bridgeBuilder.withChannels(anyList()), bridge::getChannels, new MapArgument\u0026lt;\u0026gt;()); when(bridgeBuilder.build()).thenReturn(bridge); Thing thing = mock(Thing.class, \u0026#34;thing\u0026#34;); ThingBuilder thingBuilder = mock(ThingBuilder.class, \u0026#34;thingBuilder\u0026#34;); builder(thingBuilder, () -\u0026gt; thingBuilder.withConfiguration(any()), thing::getConfiguration, new MapArgument\u0026lt;\u0026gt;()); builder(thingBuilder, () -\u0026gt; thingBuilder.withBridge(any()), thing::getBridgeUID, new MapArgument\u0026lt;\u0026gt;()); builder(thingBuilder, () -\u0026gt; thingBuilder.withLabel(any()), thing::getLabel, new MapArgument\u0026lt;\u0026gt;()); builder(thingBuilder, () -\u0026gt; thingBuilder.withLocation(any()), thing::getLocation, new MapArgument\u0026lt;\u0026gt;()); builder(thingBuilder, () -\u0026gt; thingBuilder.withChannels(anyList()), thing::getChannels, new MapArgument\u0026lt;\u0026gt;()); when(thingBuilder.build()).thenReturn(thing); // rest of the test } private \u0026lt;T, X\u0026gt; void builder(T mock, Supplier\u0026lt;T\u0026gt; builderCall, Supplier\u0026lt;X\u0026gt; getterCall, Function\u0026lt;InvocationOnMock, X\u0026gt; returnValue) { MapBuilderToMock\u0026lt;T, X\u0026gt; answer = new MapBuilderToMock\u0026lt;\u0026gt;(mock, getterCall, returnValue); when(builderCall.get()).thenAnswer(answer); } static class MappedReturnValue\u0026lt;X\u0026gt; implements Function\u0026lt;InvocationOnMock, X\u0026gt; { private final int index; public MappedReturnValue() { this(0); } public MappedReturnValue(int index) { this.index = index; } @Override public X apply(InvocationOnMock invocationOnMock) { return invocationOnMock.getArgument(index); } } static class MapBuilderToMock\u0026lt;T, X\u0026gt; implements Answer\u0026lt;T\u0026gt; { private final T builder; private final Supplier\u0026lt;X\u0026gt; getter; private final Function\u0026lt;InvocationOnMock, X\u0026gt; returnValue; public MapBuilderToMock(T builder, Supplier\u0026lt;X\u0026gt; getter, Function\u0026lt;InvocationOnMock, X\u0026gt; returnValue) { this.builder = builder; this.getter = getter; this.returnValue = returnValue; } @Override public T answer(InvocationOnMock invocation) throws Throwable { when(getter.get()).thenReturn(returnValue.apply(invocation)); return builder; } } Summary Above code allows you to build fully functional mocks for builders and their return values in a portable way. Thanks to possibility to record mock calls via lambda arguments we can make preparation process fast and easy to read. Use of additional logic is necessary only to map arguments received by builder to instance it is supposed to return at the end.\n","permalink":"https://dywicki.pl/2020/04/mocking-builders-with-mockito-answers/","summary":"I\u0026rsquo;ve been looking for some ways to use Mockito to with builder pattern using fluent calls. There are couple of articles online, however none of these addressed my case, a builder which receives calls and returns fully working object after a final \u0026lsquo;\u0026lsquo;build\u0026rsquo;\u0026rsquo; call without the need for further setup.","title":"Mocking builders with Mockito answers"},{"content":"It is already 12 months since I started using my (first since very long time) desktop PC. In my previous blog post I\u0026rsquo;ve described reasons why I decided to resign from Apple hardware. In this one I will tell you how I managed to drop OSX and its ecosystem with minor pains.\nI feel that I owe you also some insights - which applications I had to swap and which I was lucky to keep. Because I work with Java on daily basis I do not have any major troubles with portability of my software, however just few out many programs is actually written in Java.\nWhy the heck arch manjaro? From my personal observation it looks that most of people who have any contact with Linux ecosystem can distinguish major distributions such Debian, Ubuntu, CentOS and Red Hat. Of course there are many more. I\u0026rsquo;ve chosen arch based distribution mainly because at time I was installing it, it was one of very few which supported F2FS. I have NVME drive and I wanted to get as much performance as possible. Java is compiled language and compilation always involves reading sources and writing compiled code to disk. I build quite large open source projects, sometimes few times a hour. Other Linux distributions supported F2FS, however not as system partition. Keep in mind it was almost a year ago and Ubuntu/Debian likely improved its support for this filesystem.\nFrom biggest benefits of running arch-based distribution I really appreciate fact that I don\u0026rsquo;t need to reinstall system in order to migrate to new major version. Again, it is possible with other distributions, however its not recommended way for many of them and/or requires manual steps. As learnt from OSX experiences - update process can get tricky. Last thing I wanted to suffer after dropping Mac was waiting long hours for backup restoration. Manjaro/Arch, are only one distribution which I know, giving a simple promise - there is a rolling release. It means that system which you have installed will be moving forward. You have guarantee you stay on latest versions of packages or very close to them. This is pretty much how open source development looks a like - using latest releases if possible.\nPorting applications One of biggest changes I had to accept was package system. As almost every user who program under Apple OSX, I used Homebrew for several years. I installed with it many things, including vim, sublime as well as quite a lot others who I can not pull right now. Two years ago I managed to get brew (actually linuxbrew) working on linux in order to provision vagrant box. Sadly brew project for years didn\u0026rsquo;t want to support linux. I can understand that as variety of software versions used under Linux can easily overwhelm. I ended up with a fresh system installation and no \u0026ldquo;package\u0026rdquo; manager.\nWhat saved my life was Docker. If I would have Facebook account and I would need to describe my relation with Docker, then most likely I would summarize it as \u0026ldquo;it\u0026rsquo;s complicated\u0026rdquo;. I enjoy running some things in Docker on my NAS, but I don\u0026rsquo;t need to upgrade these containers to often. Each time when I have to do anything with them I have to remind myself command syntax. Till I was forced to use it I felt that I didn\u0026rsquo;t know docker well enough. I think I still miss a lot of knowledge about it. Docker solved for me issues with lack of good quality ActiveMQ and Cassandra packages under Linux. I was also able to use company specific containers to run parts of product responsible for user interface, report generation and so on which previously I ran using nginx and some fancy tools such phantomjs.\nDevelopment environment setup Development environment which includes IDE (IntelliJ) - I got via toolbox installed via arch packages. I use DiffMerge and I was able to get it installed via package system too. I had to tweak checksums to match with latest releases, but beside that - it went well. Ad hoc editor (sublime) - is also available via packets.\nBiggest change which I accepted while moving away from Apple was change of shell. I have used ZSH for years and I was happy with it. But as usual, there is always someone who will crush your confidence in a tool. That person for me was Zachary who I worked with at Rocana (acquired by Splunk). He said me that FISH is much better in handling completion and it is able to keep history of commands executed in given directory. This was something which got me interested but not that much to test it immediately. It turned out to be a very good recommendation and I started using fish as my primary shell after moving to linux. Beside that I also started using Terminator who makes a lot of sense on 4k screen where tabs in terminal emulator leave simply to much of empty space.\nI been also a long time user of Source Tree, a handy application allowing to manage git repositories.\nDon\u0026rsquo;t get me wrong, I am able to work with git from command line for most of the time, but sometimes when I have more changes or refactoring I prefer to be picky in checking in files or even their parts. I never got used to any IDE for that because there are too many distraction points in there - switching to application which is limited to version control allows to focus on commit. Source Tree was very good in that, simple and clean user interface allowing to select chunks of file to be staged and then committed. Sadly Atlassian didn\u0026rsquo;t decide to port Source Tree to Linux so I had to look for alternatives. I ended up buying Git Kraken. It offers subset of Atlassian\u0026rsquo;s product functionality but gives all things I been looking for, and it works. I was concerned by fact that it is yet-another-electron-app, meaning javascript running inside embedded browser pretending to be a desktop app, but as said before it works.\nFor quite long time I don\u0026rsquo;t use Adium nor Pidgin nor any other chat client. Under OSX I switched to Franz, yet due to its strange policy in version 5 which forces account registration I decided to move to Rambox. It does the same thing - wraps web page into tab inside of own window. There is one window to keep all instant messanger distractions in one place. Since most of communications nowadays goes over some form of cloud services, having a single window to aggregate them is useful to keep all distractions in one place.\nBrowser and surroundings If you would ask me what I miss mostly from OSX - I would definitely point Apple Mail and synchronization of contacts between phone and computer. This thing alone is something I regret most. I do use Thunderbird, but it\u0026rsquo;s not the same. It really feels far worse than osx default app. I been using Thunderbird before 2008, then around 2014 for corporate mailbox and come back to it last year. General feeling after launching Thunderbird after several year break is - it didn\u0026rsquo;t change at all. In fact it looks as ugly as before, with confusing settings panel as always and lack of good contact plugin. I could have missed one, if you can recommend something - just post a name in comments.\nWhile moving away from OSX I decided to drop Chrome. Why? Because chrome is the best tracking software ever made and I felt that I have to give it up. After my passwords, which I stored in the browser went to password.google.com where I can get them back in plaintext I got really concerned. It is not that I wasn\u0026rsquo;t aware that Google have them in the cloud, yet it was told everywhere it is not able to read encrypted copy of mine data.\nMy perception changed dramatically once something which is stored in browser was shown to me first time on web page. I remember old screen for looking passwords in Chrome. It wasn\u0026rsquo;t particularly great, but I would prefer it over having these on web page. More importantly I did not expect that change. Because of that I stopped using google as default browser and moved to Firefox with DuckDuckGo. It\u0026rsquo;s different, but not that much. I still turn google for some edge cases, however my development box - is owned by duck. ;-)\nDrawbacks I\u0026rsquo;m really happy with my PC, yet - to be fair with you - there are several things which are quite annoying:\n4k/hires/hidpi support X11 window manager used by default with Gnome 3 under Manjaro does not support partial scaling. This means that it supports 100, 200 or 300 percent scale, but not 125%. This is a real pain because for me 100 is to little and 200 is way to big. It needs to be manually adjusted with xrandr. Usage of xrandr have side effects. For example not all apps gets scaled properly with it. Firefox is one of them. Wayland, which is supposed to solve that have troubles with nvidia cards. For now, I use 100% scale and have adjusted all apps to work with this scale (Qt4, Qt5). However each time when I visit new webpage I have to adjust it by zooming in somewhere between 130 and 170% depending on its layout. I can stand differences between Qt5, GtK and other toolkits with no issues. I keep complaining on Thunderbird, but if I would be forced to stay with it, I still could manage it. There is one more, even more annoying thing - I can\u0026rsquo;t get 60 Hz on my display for whatever reason. This means that there is no point in watching any movies on this box. Even on youtube as you will experience flickering. It\u0026rsquo;s good for staying away from movies, but lets face it - its a issue.\nLinux drivers Nvidia drivers are available, but they cause some troubles with Wayland (a replacement for X Server). Open source driver does not solve it too. If you consider getting there it might be good idea to check with Intel cards which are used for mobile devices. Friend of mine managed to get Wayland up on his ubuntized Dell XPS 15 with no issues. I also have 7 years old Canon laser printer which was awful in terms of getting it work under macos. Debugging of issues was usualy solved via resetting CUPS system completely. I didn\u0026rsquo;t even try to get it running under linux. All printing and accounting I still do on OSX with dedicated software.\nPower management works fine - I can put computer into sleep and it will wake up. It happens that it doesn\u0026rsquo;t get up, mainly after update of system without restart, then I need to ssh into it and restart display manager. I do see from time to time core dumps in logs, but I do not track them unless they become a source for real troubles.\nOverall summary I am quite happy with my current setup. Despite of many minor issues and troubles with displays I experienced under Linux I feel myself more productive than before. It\u0026rsquo;s not only about tools, because they are available under OS X, but computer performance. I don\u0026rsquo;t use it for entertainment and it works as expected. All issues so far I was able to solve with answers found online.\nLinux as desktop is definitely not as nice nor as stable as OS X, there is still huge distance in terms of usability of both.\nYet, I\u0026rsquo;m happy to say that \u0026ldquo;This is Linux year\u0026rdquo;, at least for me.\nPS. It took me just 7 months to finish this blog post since first draft made on February 2018. ;-)\n","permalink":"https://dywicki.pl/2018/08/migration-from-osx-to-arch-linux/","summary":"\u003cp\u003eIt is already 12 months since I started using my (first since very long time) desktop PC. In my previous blog post I\u0026rsquo;ve described reasons why I decided to resign from Apple hardware. In this one I will tell you how I managed to drop OSX and its ecosystem with minor pains.\u003c/p\u003e\n\u003cp\u003eI feel that I owe you also some insights - which applications I had to swap and which I was lucky to keep. Because I work with Java on daily basis I do not have any major troubles with portability of my software, however just few out many programs is actually written in Java.\u003c/p\u003e","title":"Migration from OSX to Arch Linux"},{"content":"I must start from small confession. I am not an computer kido. My first computer was AMD K6 with 266 Hz clock I got for Christmas back in 1999. I\u0026rsquo;ve seen in my life Amiga, but I wasn\u0026rsquo;t part of long standing battle between platforms. I\u0026rsquo;ve seen Norton Commander on my friend PC who got his Pentium in 95, but I never had to run such tool on my own. Point of bringing whole history of myself coming to computers is to show you that I am relatively fresh to it.\nFirst Apple devices under the roof I got my first (and last at the same time) MacBook Pro in 2011. In fact this is the only one laptop I got from Apple. Primary reason for buying this expensive computer was - unix. Back at this time I was working on project which used Hadoop. From what I remember, it was not possible to run it under Windows. I had some small successes with git under Cygwin, remember it was almost 7 years ago, but it was pure pain. After seeing how much work and struggle git did cost me I decided to cut my sufferings and get unix based device. My former collaborators used Macs and they were very happy about it. They also engrafted such love into me. That\u0026rsquo;s how I become a Apple computer user. I started from 8,3 model with Mac OSX Lion (17\u0026quot; full hd screen). Despite of it\u0026rsquo;s size it was still lighter than my gaming laptop (Asus G50v) and quite powerful at moment of buying. Worth to note that it was my first device which supported SATA3.\nOver time I replaced traditional disk and installed OCZ Vertex 3 which had only 128 GB but gave a new breath to my laptop just after a year of using it. A year later I swapped memory and moved from 2x2 GB to 2x8 GB. Each change costed me about 380€, but thanks to these two spendings I had an amazing machine with big screen, fast disk and lots of memory.\nFirst Linux experiences To be fair with you I had my first linux experiences long time before I even knew about OSX. In past days, when I was going to technical school, I had no internet at home. Somewhere in 2003 I got recommendation from John, a system administrator managing network at internet cafe I was visiting. He proposed me to run Linux instead of Windows. This might seem silly these days, but I was between 16 and 17 years old and this man was the only one I knew who was paid for working with computers. For me this was some kind of holy grail. I can\u0026rsquo;t remember which distribution he gave to me, if it was Slackware or maybe Debian, I think it was slack. I managed to install it, but I didn\u0026rsquo;t pick it up as it was too difficult for me to manage it. Without internet connection at home finding solution for single configuration option was taking to long - walking for about 1,5 km to cafe and back. About two years later, my dreams come to true, despite of not having Slackware knowledge. I got my first job. It was sunny July 2005 and my workstation was powered by Mandriva Linux. While overall user experience was rather fine because I got user interface running with no troubles, I could not stand it. I simply hated huge fonts. They were ugly, ugly like hell. After years of windows experience first look at big fonts with blurry letters in editor was giving negative impression. I was using Eclipse as my primary IDE for programming and I couldn\u0026rsquo;t adjust it to look any closer to what I used to see under Windows. I had few attempts to run linux at home, but I never succeeded to stay longer than a week or two. In 2006 I had LG Laptop powered by Ubuntu, but due to poor performance caused by lack of drivers I went back to Windows. This is an interesting fact on the linux, even if it is extremely powerful for running server side, many programmers, including myself, couldn\u0026rsquo;t commit themselves to use it as development workstation for quite long time.\nApple device troubles My Apple experiences are, generally speaking, quite good. My hardware works, but at the same time it failed multiple times. But lets start from beginning. Once I got convinced to buy MacBook Pro I called authorised reseller, asked him about availability and after confirming hardware is available I went there and bought it. Here a small mystery occurs. I was completely new into Apple universe, so I showed up in cash desk and said - I would like to buy a Mac Book Pro. Apple Genius went behind storage doors and brought back a black box. He said - this is a special kind of MacBook Pro, so special it is packaged in black, not white one as usual. I did not ask anything, I\u0026rsquo;ve paid full price and went out without rising any questions. I am bringing this at the beginning as I was cheated by ispot. I haven\u0026rsquo;t had a clue that I was played by their worker in their biggest salon. A whole network claims it \u0026ldquo;fulfils restrictive requirements of Apple Premium Reseller\u0026rdquo;. I\u0026rsquo;ve forgotten about black box and was happy mac user until my computer broke in August 2013, few months after warranty. I called official service and after getting rough estimation of repair cost decided to go to unauthorised service. Guy there asked me if I ever removed battery from my laptop (which I did not), because there was a pencil note on cover under it. He said me this happens with refurbished devices. That\u0026rsquo;s the way I learnt that my computer was almost \u0026ldquo;a brand new\u0026rdquo; when I bought it. I think somewhere later same year or even a year later I\u0026rsquo;ve found information that issue I got was covered under extended warranty. Symptoms I had were screen/display flickering caused by graphic cards or thunderbolt controller issue. Of course when I called official service (where I bought computer) they did not even mention that such thing might be subject of repair program announced by Apple.\nAs I swapped disk and got Vertex SSD my configuration become unsupported. For example I\u0026rsquo;ve failed to update from Lion to Mountain Lion because earlier version of system didn\u0026rsquo;t have TRIM support. Installer simply failed to complete without giving any valuable information. High Sierra update also couldn\u0026rsquo;t move, for whatever reason, my devices to APFS.\nMy history of failures with Apple is much longer than that. I owe iMac 27\u0026quot; from late 2011 which was not heavily used and broke anyway. It ended up in service with broken graphic card. This time it happened again after warranty period, but unauthorised service I was using so far told me that such issue (black screen, spinning fans) is subject of extended repair program. When I brought computer (or rather mainframe if you think about it\u0026rsquo;s weight) to authorised service staff there instead of just picking up device started to count other reasons why computer may not work which were not under official repair program. They literally made impression that they were looking for a reason to avoid service program and charge few hundred euros from my pocket instead. I tested iMac with my Thunderbolt display and it did not send any output to second screen so I was sure that screen itself is fine and actual issue comes from graphics. Funny fact is that guys called me two days later saying that computer was full of dust, however issue was covered by official repair boundaries. I couldn\u0026rsquo;t comment on dust, as it seems to be collected there by design. ;-)\nSo far I enlisted two Apple devices who got broken so far and did mention third one. This got broken too - here I mean - Thunderbolt Display - luckily in this case issue was related to thunderbolt plug which broke over time. From not critical issue in this device I experienced overheated screen which lost colours and have some kind of blurry line going from top to bottom.\nWith all these things I found that Apple devices are not that rock solid as you would expect from so expensive hardware. In Poland where I live Apple is not cheap. It is more expensive than PC and there are no doubts about that. Sometimes I think it is kind of luxury brand here.\nQuestionable hardware strategy Having said that my laptop broke first time in late 2013 I had to make a choice - spend over 470€ on repair or buy a Retina model. I decided to stay with my old machine as Retina did not offer me anything dramatically better than what I already had (SSD, 16 GB of RAM) and had soldered memory. Higher weight of 17\u0026quot; laptop haven\u0026rsquo;t been an issue for me as I did not travel a lot. It was few weeks before October 2013 event and I couldn\u0026rsquo;t wait that long. I had to have working hardware. In the end a upgrade announced back then was PCIe drive, upgraded CPU models. Even if PCIe storage was faster still I haven\u0026rsquo;t had any reason to drop working machine and decided to hold on one more year with two and half year old laptop. With double price I paid for it (compared to Asus G50v) I wanted to use it as long as it was making sense. How it ended up that now I write this article on over 6 years old laptop? Well, Apple did invest into shrinking their laptops but not into things I considered necessary. Over time I found my laptop performance degrading and loosing distance to new releases, however I could not convince myself to spending a lot of money to soldered configuration regardless of how good it would be. I did hold my breath multiple times in past years - in 2013, 2014, 2015 and also 2016. In 2017 I did not hold any breath for Apple releases as I resigned from buying Apple computers for work. I been waiting for device with more than 16GB of memory because I know that sometimes I cross this limit. I was never looking for having slimmer device because I can sacrifice some mobility for performance (remember I used gaming laptop before). Few millimetres less does not make any difference to me. Even if I travelled a lot between 2013 and 2016 and had to carry on two laptops, I never had issues with my 17\u0026quot; model. It was slim enough. My issues were caused by second device which was Windows powered radio station I had to have with just to pick up calls within company. I called it that way because calling was the only one functionality I used on it. With two laptops and power supplies on my back I felt like world war two soldier. I was only missing an antenna coming out of my backpack.\nThese days hardware manufacturers keep working on making devices smaller and lighter. Even gaming notebooks with separate GPU are lighter than ultrabooks several years ago. This is great and Apple might have a fair part in making this movement and driving it with its hardware. However from an avarage Joe point of view such myself, I see only thin devices in their offering. Most of hardware suppliers have variety of laptop models - from kind of mobile workstation over notebooks down to ultrabooks. I think what is missing with Apple strategy is a workstation device which would sacriface some battery life and offered other advantages such extra ports. Retina displays are great, but most of people I know with hidipi screens have to scale text in order to be able to work. If you don\u0026rsquo;t have a eagle eye you will end up with nicely rendered fonts and similar amount of lines in your editor as you had before. Luckily amount of rows between 15\u0026quot; and 17\u0026quot; screens does not differ a lot. To be clear on this part - I do not miss 17\u0026quot; models that much, I miss a possibility to choose. Apple was quite successful with mobile devices having one or two models published every year. This changed slightly over time because they found that one size doesn\u0026rsquo;t match everyone. Yet, we need to remember that most of us use mobile devices in similar way, as its something which you connect with friends or family or improve overall connectivity with various online communities or systems available at work such mail, scheduler and so on. There are quite few people who work exclusively on mobile devices. In fact I don\u0026rsquo;t know currently anyone who do work only using mobile. [1] Computers are completely opposite to mobile devices in regard of their usage. There might be someone who uses it just for browsing web, someone can use it for watching movies (with or without web browser) and someone will use it to write software, doing vide or photo editing or technical drawing in CAD/CAM/CAE. Each of these use cases requires different software which is not yet available in cloud and for long time this will not change as years of investments in standalone software are difficult to transfer into cloud even if this business model scales better and might also pay better (this is something which might be clarified by company who did such move). For someone who works with video editing a card slot in device is something which is used on daily basis. Other groups of people will use it sporadically, some maybe will never need to use it. Now, if we will take a look, on computers available in Apple offer we will find that only one device which still have SD card slot is MacBook Air which is rather poor choice for photo editing. As far I know life with next release this socket will be gone and only one plug possible to use will be USB 3.1/Thunderbolt 3 meaning that all dongles you had bought till now will require one more dongle - a USB hub in middle. Not that I am missing this SD card socket, but lack of \u0026ldquo;traditional\u0026rdquo; usb after all years of this standard on the market is overwhelming for every hardware owner.\nInterestingly enough Apple announced a while ago that it works on modular Mac Pro, meaning a desktop computer in which you can swap parts. It sounds that it will get a bit closer to what people see in regular PCs. Maybe it is some kind of mark that people do like to swap parts in their desktops? With Apple\u0026rsquo;s ultimate weapon - which is OSX - they can control which devices gets better support and which doesn\u0026rsquo;t get support at all - for sure devices manufactured by Apple will be supported out of the box, while others will depend on supplier resources. It might be even easier with controlling that with physical barriers such mainboard sockets. Apple will design a new kind of plug which will require paying them royalties for using it, just like they did with headphone jack in iphone. I don\u0026rsquo;t know for sure what they gonna do. I think that even \u0026ldquo;analysts\u0026rdquo; specialised in Apple hardware don\u0026rsquo;t really know what they will do for sure.\nAll these rumors Since we are on questionable hardware strategy I can also bring a topic which is complete mystery to me. Some products of Apple are known and announced a long time before they are offered. Some other are receiving singular rumors which might happen or not. All this whispering, for me - a guy who do not follow Apple news feed, is completely ridiculus. Because company politics I don\u0026rsquo;t know if hardware which was upgraded a year ago and I bought this spring will not get updated over summer. In many cases people could hold a bit for two months and buy a fresh hardware instead. In fact, that what people did before 2016 update - they simply limited new orders because they were expecting something new. Now, if you think about this, who is a bigger beneficiary of unclear release policy? Think a bit and you will find that its defnitelly not you. As for consumer in your very best interest is to know an approximate release cycle. You can then see if next update will receive better/newer CPU and wait if you need it or decide to buy now as there is nothing fancy coming in next three months. With Apple - it\u0026rsquo;s a complete mystery. There are new CPU models available on the market? Screw them, we will skip them. Same story is reoccuring every year or two. Sometimes its nothing bad, sometimes its quite awful. This is literally what pushed me out of apple circle - I been waiting for some major upgrade for years hopping that within a half year there will be better device to buy. It happend to my friend that he bought an Mac Book which got updated within 3 months. With most (not to say every) Apple event I was finding that what they were offering was almost the same to what they have. In the end I\u0026rsquo;ve found that other manufacturers were able to deliver configurations which Apple couldn\u0026rsquo;t despite of its position.\nFinal remarks I don\u0026rsquo;t think I ever had \u0026ldquo;eye opening\u0026rdquo; moment with Apple, because I never closed them. I never considered myself a fan boy. Despite of having multiple devices from this company (I think) I kept some reasonable criticism to firm decisions and scepticism, backed by mentioned stories, to it\u0026rsquo;s quality. From following macrumors it\u0026rsquo;s clear that computer part of Apple\u0026rsquo;s business have better and worse moments. Some devices and markets segments are completely abandoned (for example mac mini), Mac Pro haven\u0026rsquo;t got any updates since 2013. Laptops on other end got some love with upgrades of hardware such CPUs and changed graphic cards, however there seems to be a a fair trouble with Apple waiting or skipping new components coming out from Intel production cycle. For example Apple offered 15\u0026quot; Retina with i7 from 4th generation from 2013 up to 2016 (Late-2013, Mid-2014, Mid-2015 models) upgrading processor within generation bounds. There were no device with 5th generation of Intel processors and their late 2016 release used a year old chips launched by Intel on Q3 2015. Apple finally grasped with Intel upstream with 2017 release by placing in their devices 7th generation of mobile processors.\nFrom a power user perspective, meaning someone who uses computer for work - announcing new model with a \u0026ldquo;touch bar\u0026rdquo;, \u0026ldquo;force touch trackpad\u0026rdquo; or slim keyboard - these are all nice things, however I would not place them first. These are nice to have. A real must have is a guarantee of having a hardware which is following latest releases by vendors, have predictable release cycle and reasonable maintainability.\nA story of migration from OSX to Linux will be part of separate post.\n[1] Once, while working at Red Hat, I meet colleague who was working for quality assurance department and he did work on ipad and didn\u0026rsquo;t have any laptop. It was possible for him to do such cause all systems he needed were available via ssh terminal or web page.\n","permalink":"https://dywicki.pl/2018/01/why-i-abandoned-apple-and-osx-for-linux/","summary":"I must start from small confession. I am not an computer kido. My first computer was AMD K6 with 266 Hz clock I got for Christmas back in 1999. I\u0026rsquo;ve seen in my life Amiga, but I wasn\u0026rsquo;t part of long standing battle between platforms. I\u0026rsquo;ve seen Norton Commander on my friend PC who got his Pentium in 95, but I never had to run such tool on my own. Point of bringing whole history of myself coming to computers is to show you that I am relatively fresh to it.","title":"Why I abandoned Apple and OSX for Linux"},{"content":"One of most important things, if not the most important in software, is release process. There are whole books written about \u0026ldquo;shipping software\u0026rdquo; and software release is one of key parts which needs to happen in order to deliver our programs to end users. In this very short post I will give you a short tip about how to do a test drive of a release which is not published yet. One of main principles of maven central is \u0026ldquo;what goes there, stays there\u0026rdquo;, meaning that anything which becomes public will stay public. For that reason we, as software developers, want to deploy things which are free of any major issues at release time. Staged release is one of things which are supported by maven-release-plugin. Overall idea behind this is to let people have a test drive before deploying artifacts to public repositories from which they can not be removed. Of course this might be seen as completely unnecessary step if you release a small library, but could be extremely useful for bigger projects, avoiding something I would call a quick fix hiccup.\nQuick fix hiccup Situation when project get released often is welcome. But situation when project gets released with bugs is not welcome at all. Again, this might depend on actual use case, some small failures discovered during release might be accepted, some other can be a show stopper. A quick fix hiccup happens when project gets released and then people who start using it keep discovering important issues which lead to another release. Lets assume we released a new major version called 3.0.0. With this release there is new issue discovered, so we release 3.0.1. After that we find another potentially dangerous bug generating new set of artifacts in version 3.0.2. As you can see - in both cases bug fixes have been made, however people who already started using project released as 3.0.0 needs to bump it twice in very short time window. Wouldn\u0026rsquo;t it be better to do one release but without major bugs instead?\nStaged release Someone who been thinking about this problem come to very simple conclusion - if problem is impossibility or inconvenience caused by unpublishing of artifacts, then it might be better to hold on for a moment and let people test binaries before they get published.\nDeployment phase in such release process is divided into two stages - first moving artifacts to test repository and second moving them from test repository to final one. A test repository can be anything - it can be an FTP server, maybe a filesystem. In general, it is just a place which is accessible to interested parties who would like to test our artifacts. Once testing phase is done and no major issues are found artifacts are deployed to public repository. For regular projects that would be maven central or any other location accessible for wider publicity.\nTesting of staged release Here we are going to a technical part of this article - which is - how to become a tester of staged artifacts. I will use maven as reference, but you can use any other build tool which is capable of downloading contents from configurable locations.\nOnce we know what is location of our binary artifacts which can be used for testing we need to modify maven settings.xml to include new remote repository:\n[sourcecode lang=\u0026ldquo;xml\u0026rdquo;] karaf-4.1.4\nfalse karaf-4.1.4 Karaf 4.1.4 Stage Repo https://repository.apache.org/content/repositories/orgapachekaraf-1102/ false true \\[/sourcecode\\] This is additional profile which can be quickly removed after release is finished or when testing is done. By this short piece of code you will force your maven installation to scan additional remote location for metadata and released artifacts. With such profile enabled you can build your software and verify that dependency which is about to get released works in your project.\nHow to build with staged dependency and rollback It is important to be aware what is difference between maven install and package phases. Here, just as recap I will mark that in most of cases we should use package phase because install changes state of local repository. Profile defined above will local repository too by downloading things not available in maven central. In case when release will be cancelled binaries which we downloaded from staging repository will be invalid and their checksum will differ. More importantly your local copy of dependency will be outdated and will not contain any fixes deployed after. For that reason usage of staged artifacts should be combined with temporary local repository to avoid above troubles in future. You can point alternate maven repository from command line via maven.repo.local property. You can also modify it in your maven settings, however it is less convenient. You can also create a temporary settings.xml which will be used only for testing and point it via --settings or -s option from command line.\nFinal thoughts We all know there is no software which is free of bugs. Staged releases does not guarantee software without bugs, but they help a lot when software runs in many various environments and authors can not test nor identify all possible use cases. Software quality depends on many factors. Quality assurance is very important, but all efforts put there are made to make sure that software works as expected for end users. Letting them run software, after all internal checks are made but before it gets published and announced ensures that software is free of any major bugs, at least the part of community which did test it.\nA small note at the end on \u0026ldquo;administrative\u0026rdquo; overhead caused by staged releases. Article on maven.apache.org describes manual procedure necessary to setup Apache Archiva. This area has been improved in Archiva 1.4.\n","permalink":"https://dywicki.pl/2017/12/testing-a-staged-release/","summary":"\u003cp\u003eOne of most important things, if not the most important in software, is release process. There are whole books written about \u0026ldquo;shipping software\u0026rdquo; and software release is one of key parts which needs to happen in order to deliver our programs to end users. In this very short post I will give you a short tip about how to do a test drive of a release which is not published yet. One of main principles of maven central is \u0026ldquo;what goes there, stays there\u0026rdquo;, meaning that anything which becomes public will stay public. For that reason we, as software developers, want to deploy things which are free of any major issues at release time.\n\u003ca href=\"https://maven.apache.org/guides/development/guide-testing-releases.html\"\u003eStaged release\u003c/a\u003e is one of things which are supported by maven-release-plugin. Overall idea behind this is to let people have a test drive before deploying artifacts to public repositories from which they can not be removed. Of course this might be seen as completely unnecessary step if you release a small library, but could be extremely useful for bigger projects, avoiding something I would call a quick fix hiccup.\u003c/p\u003e","title":"Testing a staged release of project dependency"},{"content":"Apache Felix Configuration Admin (CM) is widely used component which is responsible for provisioning of one of most common OSGi services. Main responsibility of it is to bring configurations stored in property files to services.\nWhile digging into Felix CM code I have found that it is able to create scalar values of certain type ie. Long, but also more complex structures such Array or Vector. The biggest issue was that I couldn’t find any way to force it to create array from string representation. Thanks to google ( FELIX-4431 found on 4th page of results) and debugger goodnes I finally managed to do it. Here is recipe how to proceed.\nConfiguration file Config file name which is source of properties must be named .config – otherwise array will not be created. Property must be written as follows: [source] property=[\u0026ldquo;value 1\u0026rdquo;, \u0026ldquo;value 2\u0026rdquo;, \u0026ldquo;value x\u0026rdquo;] [/source]\nInternally config admin is also storing information about value type. By default created values and collections will consist elements of type String. If you wish to change type of collection following prefixes are allowed:\nT = String I = Integer L = Long F = Float D = Double X = Byte S = Short C = Character B = Boolean Small prefix letters represents simple type. If you want to construct array of primitive ints then configuration syntax is following: [source] property=i[\u0026ldquo;1\u0026rdquo;, \u0026ldquo;2\u0026rdquo;, \u0026ldquo;3\u0026rdquo;] [/source]\nSmall note for Karaf users By default Karaf etc/ directory uses *.cfg suffix as fileinstall filter which means that this feature of Felix Configuration Admin will not work for you. You have two workarounds. Edit etc/config.properties and navigate to first line shown in listing and replace it with second: [source] felix.fileinstall.filter = .*\\.cfg felix.fileinstall.filter = .*\\.(cfg|config) [/source]\nCreate new file org.apache.felix.fileinstall-config.cfg with following contents: [source] felix.fileinstall.dir = ${karaf.base}/config felix.fileinstall.tmpdir = ${karaf.data}/generated-bundles felix.fileinstall.poll = 1000 felix.fileinstall.filter = .*\\.(cfg|config) [/source]\nQuick summary I am using Configuration Admin service since years and I didn’t realize this feature exists and it’s supported since very long time. Hope that this will let you to go over your more complex configurations! :-)\n","permalink":"https://dywicki.pl/2015/02/apache-felix-configuration-admin-with-array-values/","summary":"\u003cp\u003eApache Felix Configuration Admin (CM) is widely used component which is responsible for provisioning of one of most common OSGi services. Main responsibility of it is to bring configurations stored in property files to services.\u003c/p\u003e\n\u003cp\u003eWhile digging into Felix CM code I have found that it is able to create scalar values of certain type ie. Long, but also more complex structures such Array or Vector. The biggest issue was that I couldn’t find any way to force it to create array from string representation. Thanks to google ( \u003ca href=\"https://issues.apache.org/jira/browse/FELIX-4431\"\u003eFELIX-4431\u003c/a\u003e found on 4th page of results) and debugger goodnes I finally managed to do it. Here is recipe how to proceed.\u003c/p\u003e","title":"Apache Felix Configuration Admin with array values"},{"content":"I use Eclipse since years. Some of you may say that I’m a masochist. Well, people have different preferences. :-) I prefer Eclipse over other editors.\nWhat’s the pain? Eclipse had same look and feel since years. I used to have the same appearance under Windows/Linux/OSX. Everything was the same except fonts. I was very unhappy with default Juno look and feel which looks like few widgets deployed in browser. Even web-based IDEs looks better than Juno! There was some posts about that and some solutions. However nobody told how to get older look and feel in place.\nWhat’s the solution? It’s really simple. Go to Preferences \u0026gt; General \u0026gt; Apperance and change Theme to classic. Here hows Mac theme looks like: Here hows classic theme looks like: Thanks to this small change I may finally upgrade my environment to Juno. I just realised that my eclipse installation is almost 2 years old!\n","permalink":"https://dywicki.pl/2013/02/improve-eclipse-juno-look-and-feel/","summary":"\u003cp\u003eI use Eclipse since years. Some of you may say that I’m a masochist. Well, people have different preferences. :-) I prefer Eclipse over other editors.\u003c/p\u003e\n\u003ch3 id=\"whats-the-pain\"\u003eWhat’s the pain?\u003c/h3\u003e\n\u003cp\u003eEclipse had same look and feel since years. I used to have the same appearance under Windows/Linux/OSX. Everything was the same except fonts. I was very unhappy with default Juno look and feel which looks like few widgets deployed in browser. Even web-based IDEs looks better than Juno! There was \u003ca href=\"http://stackoverflow.com/a/11359663\"\u003esome posts\u003c/a\u003e about that and \u003ca href=\"http://www.vogella.com/blog/2012/07/11/eclipse-4-is-beautiful-create-your-own-eclipse-4-theme/\"\u003esome solutions\u003c/a\u003e. However nobody told how to get older look and feel in place.\u003c/p\u003e","title":"Improve eclipse juno look and feel"},{"content":"Piątego lutego miałem niekłamaną przyjemność podziwiać Jacka Laskowskiego prezentującego temat Praktyczne wprowadzenie do OSGi i Enterprise OSGi. Link do filmiku z prezentacją Jacka znajdziecie na Jego blogu. Tymczasem, poniżej wideo z Karafem. :-)\n","permalink":"https://dywicki.pl/2013/02/apache-karaf-pierwszy-projekt-z-osgi/","summary":"\u003cp\u003ePiątego lutego miałem niekłamaną przyjemność podziwiać \u003ca href=\"http://jlaskowski.blogspot.com\"\u003eJacka Laskowskiego\u003c/a\u003e prezentującego temat \u003ca href=\"http://warszawa.jug.pl/spotkania/20130205-osgi\"\u003ePraktyczne wprowadzenie do OSGi i Enterprise OSGi\u003c/a\u003e. Link do filmiku z prezentacją Jacka znajdziecie na \u003ca href=\"http://jlaskowski.blogspot.com/2013/02/nagranie-z-mojej-prezentacji-o.html\"\u003eJego blogu\u003c/a\u003e. Tymczasem, poniżej wideo z Karafem. :-)\u003c/p\u003e","title":"Apache Karaf - pierwszy projekt z OSGi"},{"content":"Apache Camel supports a mapped diagnostic context which can be used to improve log entries, but also there is a log component which makes it easier to create log entries. Together they can be used to create foundations of activity monitoring without need to deploy another tool or database.\nLogging anti patterns First of all let\u0026rsquo;s go throught logging anti patterns described years ago by Gojko Adzic. They are really good and shows how to avoid common problems:\nSingle log file for all people Lack of hierarchy in log categories (that\u0026rsquo;s mine idea) Incomplete data in entries (covers also one information split to multiple entries) Different separators in single line Inconsistent log entry format (covers also inconsistent log entry format) Multi-line entries Populating log after the action Let describe comon mistakes we can do with Camel and how these anti patterns can look in our case. My thoughts can be really close to Gojko, but I\u0026rsquo;ll try to show you where you can meet these anti patterns on Java ground.\nSingle log file As usual log file can be used by different people for diagnostic - eg. database administrators are not interested in all java stuff we do. Message processing, filtering and so on. They want to see query, their parameters and execution time. Nothing else. System administrators are another group which are forced to read dozen of lines to find exception with message \u0026lsquo;Too many open files\u0026rsquo;. IOExceptions are definitely category important for these people more than surrounding. We can leverage existing features from logging framework to create advanced filters to route logs.\nLack of hierarchy in log categories Many libraries use a single log category or only few to do logging. That\u0026rsquo;s completly wrong idea. Let take a look for Atomikos. By default all log are produced by \u0026lsquo;atomikos\u0026rsquo; category. Even if you are interested in connection pooling tracing you are forced to see almost all events happened from begining of transaction to it\u0026rsquo;s end. In case of Apache Camel and middleware - consider:\nUsage of own categories, different than a framework which will reflect your mediation process logic, not technical details. Even if you are genius programmer you can be confused by log entries not connected with any process stage. Be sure that all events produced by your processors, classes and context or route go to same log hierarchy - eg. org.code_house.esb.sap.finance, instead of \u0026ldquo;processor\u0026rdquo;, \u0026ldquo;org.myjavapackage\u0026rdquo; and so on. Be consistent on that. Do not trust a thread name and do not use it as the category. Name of it can be changed by camel core change or component update. Camel components have a proper packages and you can easily get rid of statements unnecessary for you.\nIncomplete data in entries That\u0026rsquo;s I think most common mistake. The best example is Hibernate and SQL parameters logging. To get all data related to SQL query exceution and parameter binding we all are forced to use two totally different categories which are not in same hierarchy. If you are happy with setting TRACE level for org.hibernate package you can skip this example. If you have all parameters and SQL query put it together to log stream. Even if line will be insane long you will be sure that you see parameters for this SQL query, not two different.\nDifferent separators in single line I like to have a log with brackets, pipes and dots. This is so geeky. You look like guy who can see matrix and knows what is happening. Actually you do, but text processing tools are not so smart as we. Parsing is as complex as log pattern is. So don\u0026rsquo;t use too complex patterns in production and try to use singular separator. Some commercial tools require a special log format. Cook it for this tool in separate file or use different appender. Even if it is redundant. Keep main log clear and not affected by specific format which cuts 50% of diagnostic information or make it harder to read.\nMulti-line entries Just imagine that your SQL query have 5 lines. Not too much, isn\u0026rsquo;t? But what\u0026rsquo;s when you have a 200KB (or more) payload? Your log will grow quicker than disk size. Basically multi line entries can make parsing harder, but also makes log file longer and harder to read. If you need to keep messages don\u0026rsquo;t do that in log file. That\u0026rsquo;s not place for that. Reuse auditing features from ServiceMix NMR or archive ActiveMQ store. You can also build your own mechanism as well if these are not sufficient using asynchronous Wire Tap pattern.\nPopulating log after the action Latest anti-pattern - remember that logs are produced by code should prelude a operation. Not other - otherwise log entry before exception will point to mediation which was fine or your will be not able to say what happened with given message. If you are against this point remember to notify people about your logging behavior. Most of them expect that log stream allows to see path before error occured (I received message with given ID), error (exception during message transofrmation) and recovery actions (message moved to dead letter).\nBest practices of logging with Apache Camel Apache Camel uses slf4j as logging library. It\u0026rsquo;s a generic facade to many different logging libraries. You don\u0026rsquo;t have to use it in your project directly, you still can use Log4j or commons-logging, but it\u0026rsquo;s always the better to have less dependencies and clean classpath. I will not complain about ideas behind slf4j here, just point to your nous and let you nod. :)\nName your elements If you going to live with your middleware you should know what happens when. Especially that you will have difficulty to access production environment with your lovely IDE and debugger. That\u0026rsquo;s why at least camel context should have a name. From runtime management point of view it should always have a given name. If you have a JMX console to do some administrative operations than 182-camel-10 is confusing. Just see image on right side and try to say what the following processors do. Sure, you can do that by walking whole JMX tree.\nUse a Mapper Diagnostic Context (MDC) MDC is an bridge from your application to logging framework. You can set some application specific variables and then re-use them to create rich log format or distinct log files by application id (yeah, it seems like missing feature in Tomcat for me too). This feature is disabled by default for Camel but it might be easily turned on. Let\u0026rsquo;s have a look on MDC variables set by Apache Camel:\nbreadcrumbId exchangeId messageId correlationId routeId camelContextId transactionKey Each variable from this list can be used in combination with log format or appender. For example if you have all variables set correctly you can use this format: [plain] log4j.appender.out.layout.ConversionPattern=%d{ABSOLUTE} | %-5.5p | %X{camelContextId} %X{bundle.version} | %X{routeId} %X{exchangeId} | %m%n [/plain] And you will get following line produced: [plain]23:56:12,158 | INFO | database-batch 2.8.0.fuse-01-06 | jms-inbound-hr | ID-Code-House-local-60526-1330335502920-25-33 | Received message[/plain]\nFor me it\u0026rsquo;s still do not look like something easy to read, that\u0026rsquo;s why I introduce appender: [plain] log4j.appender.integrationProcess=org.apache.log4j.sift.MDCSiftingAppender log4j.appender.integrationProcess.key=camelContextId log4j.appender.integrationProcess.default=unknown log4j.appender.integrationProcess.appender=org.apache.log4j.RollingFileAppender log4j.appender.integrationProcess.appender.layout=org.apache.log4j.PatternLayout log4j.appender.integrationProcess.appender.layout.ConversionPattern=%d{ABSOLUTE} | %-5.5p | %X{routeId} %X{bundle.version} | %X{exchangeId} | %m%n log4j.appender.integrationProcess.appender.file=${karaf.data}/log/mediation-$\\{camelContextId\\}.log log4j.appender.integrationProcess.appender.append=true log4j.appender.integrationProcess.appender.maxFileSize=1MB log4j.appender.integrationProcess.appender.maxBackupIndex=10\nlog4j.category.com.mycompnany.camel_toys.hr=INFO, integrationProcess [/plain] By default, when the camelContextId is not set log entries will be appended to file data/log/mediation-unknown.log. But for these which have a context ID it will look much better (file mediation-database-batch.log): [plain] 00:38:42,386 | INFO | jms-inbound-hr 2.8.0.fuse-01-06 | ID-Code-House-local-51055-1330383822002-4-67 | Received message with business key XYZ. 00:38:43,387 | INFO | jms-inbound-hr 2.8.0.fuse-01-06 | ID-Code-House-local-51055-1330383822002-4-67 | Saving message in database 00:38:44,388 | INFO | jms-inbound-hr 2.8.0.fuse-01-06 | ID-Code-House-local-51055-1330383822002-4-67 | Processing of message with business key XYZ complete. 00:38:44,389 | INFO | jms-inbound-hr 2.8.0.fuse-01-06 | ID-Code-House-local-51055-1330383822002-4-68 | Received message with business key ZYX. 00:38:45,390 | INFO | jms-inbound-hr 2.8.0.fuse-01-06 | ID-Code-House-local-51055-1330383822002-4-68 | Saving message in database 00:38:45,391 | ERROR | jms-inbound-hr 2.8.0.fuse-01-06 | ID-Code-House-local-51055-1330383822002-4-68 | Processing of message with business key ZYX failed. [/plain] As you see these log entries contains only a process specific informations and they are not influenced by technical details. A category is skipped because it is not relevant in this context (it still for developer logging but not here). Also you don\u0026rsquo;t have to extract a business key. You might use a breadcrumbId header or correlationId property. Before you will start using MDC be aware that the property names might be changed because issue CAMEL-5047 Clean up MDC property names. This configuration was tested with Camel up to 2.9 release. The bundle.version property is set by pax-logging, not Camel and I treat it as a mediation process version. Log4j sift appender is a wrapper so it will work also with syslog or an database appender.\nEmbedded processors Embedded processors are something common in java DSL. If you use them your category will contain a dollar sign (my.company.RouteBuilderExt$1). Then you should not implement too complex logic in it. If you need do something more than glue strings or set header and you wish to log that - then create a separate class and remember about log category hierarchy and do not use a RouteBuilder log category. Processor logic is deeper than the route builder\u0026rsquo;s and you do some specific task. Separate it. Keep same approach for AggreagatorStrategy and org.apache.camel.spi extensions.\n[java] package com.mycompnany.camel_toys.hr; public class Route extends RouteBuilder {\nLog log = LogFactory.getLog(Route.class);\n@Override public void configure() throws Exception { from(\u0026quot;\u0026quot;) .onException(Exception.class) .process(new Processor() { @Override public void process(Exchange exchange) throws Exception { org.apache.camel.Message out = exchange.getOut(); out.setHeader(org.apache.cxf.message.Message.RESPONSE_CODE, new Integer(500)); log.error(\u0026ldquo;Unexpected exception\u0026rdquo;, exchange.getException()); } } }) .end() .to(\u0026quot;\u0026quot;) } } [/java] It\u0026rsquo;s better to have: [java] package com.mycompnany.camel_toys.hr; // or .processor if we have few or logic is complex public class ExceptionProcessor implements Processor { // slf4j! private Log log = LoggerFactory.getLogger(ExceptionProcessor.class); @Override public void process(Exchange exchange) throws Exception { org.apache.camel.Message out = exchange.getOut(); out.setHeader(org.apache.cxf.message.Message.RESPONSE_CODE, new Integer(500)); log.error(\u0026ldquo;Unexpected exception\u0026rdquo;, exchange.getException()); } } [/java]\nUsage of camel-log component Most of use this component in more or less proper way. For example: [java] onException(Exception.class) .log(\u0026ldquo;Unexpected exception ${exception}\u0026rdquo;) .end() [/java] By default your logging category will be set to route id (eg route4, route5 etc) which will obviously not help in detecting the issues. Also log level is not present so it\u0026rsquo;s set by default to INFO. A proper usage is: [java] onException(Exception.class) // see, I have here a same category as route builder because it\u0026rsquo;s on same level .log(LoggingLevel.ERROR, \u0026ldquo;com.mycompnany.camel_toys.hr\u0026rdquo;, \u0026ldquo;Unexpected exception ${exception}\u0026rdquo;) .end() [/java]\nQuick summary It\u0026rsquo;s too late to continue this blog entry, but I hope it will be useful for all of you. Remember - be a reasonable. Use short log entries for business activity monitoring and keep them far from technical details. A statement \u0026ldquo;Rejecting message because validation error\u0026rdquo; is sufficient for 95% of people. The remaining 5% is involved in development but for these you have a separate log. :)\nI hope you like the idea of best practices and you would like to see more similar entries. If you feel yourself more experienced than I or you simply prefer a different approach and would like share your ideas - do that. It will be pleasure to discuss here or link to your reply on other blog.\n","permalink":"https://dywicki.pl/2012/02/advanced-apache-camel-logging-and-best-practices/","summary":"Apache Camel supports a \u003ca href=\"http://camel.apache.org/mdc-logging.html\"\u003emapped diagnostic context\u003c/a\u003e which can be used to improve log entries, but also there is a \u003ca href=\"http://camel.apache.org/log\"\u003elog component\u003c/a\u003e which makes it easier to create log entries. Together they can be used to create foundations of activity monitoring without need to deploy another tool or database.","title":"Advanced Apache Camel logging and best practices"},{"content":"Few months ago I\u0026rsquo;ve read an article written by my friend Jacek Laskowski - Enterprise OSGi runtime setup with Apache Aries Blueprint. In his article Jacek describes which bundles should be installed to get the blueprint working. As IBM employee Jacek can always promote IBM WebSphere in version X or Y which started (or will start) supporting blueprint as dependency injection mechanism. That\u0026rsquo;s not fine for these who do not run IBM producs and want something light. As you know, Aries and OSGi Blueprint is an alternative for old-fashion Spring approach.\nWhy I would use Aries Blueprint instead of Spring? That\u0026rsquo;s good question. Let me answer. First of all Aries runs fine with OSGi. With OSGi you have full modularity and extensive life cycle for your deployment units. Spring Framework is cool and offer you only life cycle for your beans. Just imagine that you don\u0026rsquo;t have to assembly whole webapp every time when your DAO implementation is changed. Also Spring have some issues with custom namespaces and supports only one way of hadling it - fail fast. If you are interested in details you can finds some answers on Guillaume Nodet blog in post about custom namespace handlers. Of course you can still run Spring-DM under OSGi or select some Spring Source product. For me, blueprint fits OSGi much better.\nHow to run OSGi under Tomcat As you (may) know OSGi is an module layer. Tomcat is an servlet container. If you wish run something under Tomcat that must be an WAR. So to start using OSGi under Tomcat I\u0026rsquo;ll simply embed it inside an war. Because I would expose a servlets registered under OSGi to be accessible throught tomcat connector I\u0026rsquo;ll also use additional proxy service. So there are two different solutions one is Equinox Servlet Bridge and Felix Http Bridge. Both works fine. For this post I\u0026rsquo;ve selected Felix artifacts.\nStarting Framework is really simple, you just need an instance of ServletContextListener which will start the framework. [java]public class StartupListener implements ServletContextListener {\nprivate Logger logger = LoggerFactory.getLogger(StartupListener.class);\nprivate FrameworkService service;\npublic void contextInitialized(ServletContextEvent event) { logger.info(\u0026ldquo;Starting felix framework\u0026rdquo;);\nthis.service = new FrameworkService(event.getServletContext()); this.service.start(); }\npublic void contextDestroyed(ServletContextEvent event) { logger.info(\u0026ldquo;Framework shutdown\u0026rdquo;);\nthis.service.stop(); } }[/java] I could finish my post here, but I\u0026rsquo;ve meet multiple problems. I will describe them here to not lost them in future. :)\nFrameworkService and ProvisioningActivator First of all, I would like to install some bundles after framework startup. Starting only Felix is not fun at all. We need modules! To get this part done we need to use an specific Felix configuration option called felix.systembundle.activators. It allows to pass BundleActivator instances using different classloader/different context than OSGi framework execution. By this way we can simply access resources embeded in WAR. I know it is not really nice solution, but I wanted to keep my demo simple. Just see the ProvisioningActivator code below. We use our servlet context during bundle activation!\n[java]public class ProvisionActivator implements BundleActivator {\nprivate final ServletContext servletContext;\nprivate static transient Logger logger = Logger.getLogger(ProvisionActivator.class.getName());\npublic ProvisionActivator(ServletContext servletContext) { this.servletContext = servletContext; }\npublic void start(BundleContext context) throws Exception { servletContext.setAttribute(BundleContext.class.getName(), context);\nArrayList installed = new ArrayList(); for (URL url : findBundles()) { Bundle bundle = context.installBundle(url.getFile(), url.openStream()); installed.add(bundle); }\nfor (Bundle bundle : installed) { try { bundle.start(); } catch (Exception e) { logger.warning(\u0026ldquo;Unable to start bundle \u0026quot; + bundle.getSymbolicName() + \u0026ldquo;. \u0026quot; + e); } } }\npublic void stop(BundleContext context) throws Exception { }\nprivate List findBundles() throws Exception { List list = new ArrayList();\nSet paths = this.servletContext.getResourcePaths(\u0026quot;/WEB-INF/bundles/\u0026rdquo;); logger.info(\u0026ldquo;List of entries in /WEB-INF/bundles/ \u0026quot; + paths); for (Object o : paths) { String name = (String)o; if (name.endsWith(\u0026quot;.jar\u0026rdquo;)) { URL url = this.servletContext.getResource(name); if (url != null) { list.add(url); } } }\nreturn list; } }[/java]\nNow we need proxy servlet and proxy startup listener. First receives calls from browser and forward its to OSGi servlets, second delivers some servlet events from war to OSGi. The setup is really simple: [xml]\u003c?xml version=\"1.0\" encoding=\"ISO-8859-1\"?\u003e Aries WebApp\norg.code\\_house.workshop.aries.war.StartupListener org.apache.felix.http.proxy.ProxyListener proxy org.apache.felix.http.proxy.ProxyServlet 1 proxy /\\* [/xml]\nOk, having the framework running is great, but now we would like to use Felix WebConsole to inspect environment we run. For that we need another important improvement. If you have done some webapps you know that servlet api should not be included in WARs. We need respect same rule in this case. We can not install servlet api as OSGi bundle because ProxyServlet uses WAR classloader, it is not part of any bundle. Because of that we need configure framework to load javax.servlet and javax.servlet.http packages from parent classloader. Just remember to add them in org.osgi.framework.system.packages configuration property. From OSGi point of view we need an bridge which registers HttpService. Bot OSGi part and proxy part are linked. To get everything running you need both of them.\nNow we are close to run everything. What we need to remember? Well under OSGi we have to handle dependencies just like under Tomcat, so I\u0026rsquo;ll install following artifacts:\norg.apache.aries.blueprint - blueprint-api, blueprint-core, to support declarative services org.apache.aries.proxy - proxy-api, proxy-impl, these two are blueprint dependencies org.apache.felix - http bridge, to link OSGi servlets with Proxy Servlet org.ops4j.pax.logging - pax-logging-api, pax-logging-service, just to see what happens org.apache.felix - webconsole, to track environment The basic setup can be really tiny. One thing I did not solve is proper logging. Because pax-logging-service uses log4j and some containers also do there is small conflict between those two. I did not investigate this realy deeply, but I suppose that there is need to put them into system packages list.\nSummary Running OSGi under Tomcat is not hard part. Actually many of providers do that, for example Atlassian have own extension system build on top of OSGi. If your product need an extreme extensibility, then this post is for you. If you will build it correctly then updates will work right after the update without need to restart whole web application.\nI hope you enjoyed this part. If you are looking for source code - visit github. The code contains few modules. After build you can drop a war/target/webapp-1.0.0.SNAPSHOT.war into tomcat webapps directory. You can run mvn jetty:run-war to see same effect. After that you will be able to access Felix WebConsole at http://localhost:8080/webapp-1.0.0.SNAPSHOT/system/console. The default user and password is admin. Have fun!\n","permalink":"https://dywicki.pl/2012/01/apache-aries-under-apache-tomcat/","summary":"Few months ago I\u0026rsquo;ve read an article written by my friend \u003ca href=\"http://jlaskowski.blogspot.com\"\u003eJacek Laskowski\u003c/a\u003e - \u003ca href=\"http://www.jaceklaskowski.pl/wiki/Enterprise_OSGi_runtime_setup_with_Apache_Aries_Blueprint\" title=\"Enterprise OSGi runtime setup with Apache Aries Blueprint\"\u003eEnterprise OSGi runtime setup with Apache Aries Blueprint\u003c/a\u003e. In his article Jacek describes which bundles should be installed to get the blueprint working. As IBM employee Jacek can always promote IBM WebSphere in version X or Y which started (or will start) supporting blueprint as dependency injection mechanism. That\u0026rsquo;s not fine for these who do not run IBM producs and want something light. As you know, Aries and OSGi Blueprint is an alternative for old-fashion Spring approach.","title":"Apache Aries under Apache Tomcat"},{"content":"One of bigest benefits of Java is byte code manipulation. You can change everything you want in your application without touching source code. That\u0026rsquo;s usefull for many cases, starting from legacy code, where we can\u0026rsquo;t simply modify and recompile library up to modern applications where aspects can be used to handle runtime exceptions. The most popular project is AspectJ which is part of Eclipse ecosystem, in this post I going to show you how to use AspectJ with Karaf.\nEnabling AspectJ Byte code manipulation requires access to class resource (.class file). It can be obtained from class loader which was used to load the class. Under Tomcat it\u0026rsquo;s easy. Anywhere you are, you can use MyClass.class.getClassLoader() and you will access web application class loader. But how to handle it under OSGi, where every bundle have own class loader? Equinox supports technic called hooks. It allows us, the developers, to extend framework in more declarative way. Hooks became very useful and since OSGi 4.3 release all certified frameworks will support few:\nResolver Hook Bundle Hook Service Hook and finally Weaving Hook The core difference between hooks in Equinox and in OSGi 4.3 is how they are registered. OSGi 4.3 allows to register hooks as typical services. In older Equinox they\u0026rsquo;re loaded from framework configuration or they are discovered by classpath scanning (they must be attached to Eqinox bundle as fragments). So whole trick is to start framework with hooks JARs. Also, to make aspects works, in older version, AspectJ must be loaded before any woven class, but that\u0026rsquo;s logical. You can\u0026rsquo;t weave classes when they already loaded and cached by JVM.\nEnabling in Karaf Because actual version of Karaf (2.2.4) still works with OSGi 4.2 we\u0026rsquo;ll use older way to enable aspects. First of all, before launch we\u0026rsquo;ll change default framework. By default Karaf distribution uses Felix as OSGi framework. Open the etc/config.properties and change the karaf.framework property to equinox. [java] karaf.framework=equinox [/java]\nIf you are interested in different versions of same framework, you can simply hack this file because the karaf.framework is used to look up another property. Few lines below in configuration file you can see: [plain] karaf.framework.equinox=${karaf.default.repository}/org/eclipse/osgi/3.6.2.R36x_v20110210/osgi-3.6.2.R36x_v20110210.jar karaf.framework.felix=${karaf.default.repository}/org/apache/felix/org.apache.felix.framework/3.0.9/org.apache.felix.framework-3.0.9.jar [/plain]\nFramework JAR is located by expression karaf.framework.${karaf.framework}. But that\u0026rsquo;s only a note for curious people. ;)\nOnce we switched to Equinox we have only few steps left. First of all we need to install aspectj and weaving hook. Last part is example aspects. I had problem to locate fresh version of Weaving hook because project was moved from Equinox Incubator to Equinox Bundles. We need download whole Equinox SDK to grab following artifacts from plugins directory:\norg.eclipse.equinox.weaving.aspectj_1.0.1.v20110502.jar org.eclipse.equinox.weaving.caching_1.0.100.v20110502.jar org.eclipse.equinox.weaving.hook_1.0.100.v20110502.jar I copied these artifacts to karaf home directory except org.eclipse.equinox.weaving.hook. It goes to lib directory, otherwise hook won\u0026rsquo;t be discovered. That\u0026rsquo;s because hook discovery is done by property file lookup. The lookup uses framework class loader and in Karaf case it is URLClassLoader with lib/ entries and framework archive.\nIn addition we\u0026rsquo;ll need also AspectJ runtime. We\u0026rsquo;ll install it from ServiceMix bundles repository but it will be installed directly from maven. No additional download steep need. After that we can launch bin/karaf and start installing artifacts:\n~/tools/apache-karaf-2.2.4-switch$ ./bin/karaf __ __ ____ / //_/____ __________ _/ __/ / ,\u0026lt; / __ `/ ___/ __ `/ /_ / /| |/ /_/ / / / /_/ / __/ /_/ |_|\\__,_/_/ \\__,_/_/ Apache Karaf (2.2.4) Hit \u0026#39;\u0026lt;tab\u0026gt;\u0026#39; for a list of available commands and \u0026#39;[cmd] --help\u0026#39; for help on a specific command. karaf@root\u0026gt; install file:lib/org.eclipse.equinox.weaving.hook_1.0.100.v20110502.jar Bundle ID: 49 karaf@root\u0026gt; install file:org.eclipse.equinox.weaving.aspectj_1.0.1.v20110502.jar Bundle ID: 50 karaf@root\u0026gt; install file:org.eclipse.equinox.weaving.caching_1.0.100.v20110502.jar Bundle ID: 51 karaf@root\u0026gt; install mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.aspectj/1.6.8._2 Bundle ID: 52 karaf@root\u0026gt; list [ 49] [Resolved ] [ ] [ 60] Aspect Weaving Hooks Plug-in (Incubation) (1.0.100.v20110502) Hosts: 0 [ 50] [Active ] [ ] [ 60] WeavingService Plug-in (Incubation) (1.0.1.v20110502) [ 51] [Active ] [ ] [ 60] Standard Caching Service for Equinox Aspects (Incubation) (1.0.100.v20110502) [ 52] [Active ] [ ] [ 60] Apache ServiceMix :: Bundles :: aspectj (1.6.8.2) To resolve correctly fragment bundle and connect it with equinox system bundle an restart may be needed. I recommend to restart, because it is the best way to connect everything together. Optionally you may set start level to be lower than default - 60. But for test purposes it may be leaved as is.\nRunning aspects After installing all stuff it would be nice to run some aspects, isn\u0026rsquo;t? :) If we have environment ready to handle byte code manipulation it would be nice to use it. As an example Aspect I will use char counter. After writing to PrintStream instance it will write information how many characters was writen and to which stream. I don\u0026rsquo;t know AspectJ syntax so it might not be optimal - feedback is welcome. [java] package org.code_house.workshop.karaf.aspect.aspectj;\nimport java.io.PrintStream; import org.osgi.framework.BundleActivator; import org.osgi.framework.BundleContext;;\npublic aspect HelloAspect {\nafter(String msg, PrintStream stream) : // after call call(void PrintStream.println(String)) // println \u0026amp;\u0026amp; !within(HelloAspect) // not from Aspect \u0026amp;\u0026amp; args(msg) // with String attribute \u0026amp;\u0026amp; target(stream) { // and PrintStream as a target stream.println(\u0026ldquo;You just wrote \u0026quot; + msg.length() + \u0026quot; chars to \u0026quot; + stream); // display message } } [/java] Because we going to weave classes during load time we need META-INF/aop.xml file to let discover our aspects. Contents of file are really simple and points to aspect class name. Also it worth to notify to be careful with AspectJ version. I\u0026rsquo;ve got an error: [plain]BCException: Unable to continue, this version of AspectJ supports classes built with weaver version 6.0 but the class org.code_house.workshop.karaf.aspect.aspectj.HelloAspect is version 7.0[/plain] The root cause is version mismatch between runtime and compile time. I used version 1.6.11 to compiler and 1.6.8 to run. After aligning versions to 1.6.8 it started working again. Second, important thing is a specific manifest entry Eclipse-SupplementBundle. It says which bundle is enhanced by aspects contained inside declaring bundle. I\u0026rsquo;ve put org.code-house.workshop.karaf.aspect.bundle because I going only to instrument one bundle. But value of this header might be an wildcard. Second option is to add Require-Bundle header pointing to org.aspectj.runtime.\nSo the last step is to install our artifacts and check if they\u0026rsquo;re runnign correctly. Let\u0026rsquo;s do that.\nkaraf@root\u0026gt; install mvn:org.code-house.workshop.karaf.aspect/aspectj/1.0.0.SNAPSHOT Bundle ID: 53 karaf@root\u0026gt; install mvn:org.code-house.workshop.karaf.aspect/bundle/1.0.0.SNAPSHOT Bundle ID: 54 karaf@root\u0026gt; list [ 53] [Active ] [ ] [ 60] Code-House :: Workshop :: Karaf :: Aspect :: AspectJ (1.0.0.SNAPSHOT) [ 54] [Active ] [ ] [ 60] Code-House :: Workshop :: Karaf :: Aspect :: Bundle (1.0.0.SNAPSHOT) karaf@root\u0026gt; start 53 54 Starting org.code-house.workshop.karaf.aspect.bundle You just wrote 52 chars to org.apache.felix.gogo.runtime.threadio.ThreadPrintStream@327800e9 Summary Whole code used to write this post is publically available on GitHub. Remember to use Aspects carefully, because they might change behaviour on whole environment, especially when you use centralized load time weaving. It\u0026rsquo;s worth to add excludes in your aop.xml file to don\u0026rsquo;t w enhance packages different than we really wish to instrument. If you are interested in AspectJ usage with OSGi 4.3, you can follow this topic.\n","permalink":"https://dywicki.pl/2011/11/running-aspects-under-osgi-4-2-with-karaf/","summary":"One of bigest benefits of Java is byte code manipulation. You can change everything you want in your application without touching source code. That\u0026rsquo;s usefull for many cases, starting from legacy code, where we can\u0026rsquo;t simply modify and recompile library up to modern applications where aspects can be used to handle runtime exceptions. The most popular project is \u003ca href=\"http://eclipse.org/aspectj/\"\u003eAspectJ\u003c/a\u003e which is part of \u003ca href=\"http://eclipse.org\"\u003eEclipse\u003c/a\u003e ecosystem, in this post I going to show you how to use AspectJ with Karaf.","title":"Running aspects under OSGi 4.2 with Karaf"},{"content":"Few hours ago I\u0026rsquo;ve found an usefull post about preserving message order with ActiveMQ written by Marcelo Jabali from FUSE Source.\nIn his example Marcelo used broker feature called Exclusive Consumers. It lets send messages only to one consumer and if it fails then second consumer gets all messages. I think it is not the best idea if we have many messages to process. Why we wouldn\u0026rsquo;t use few consumers with preserved message order? Well, I was sure it is not possible, but during last training I\u0026rsquo;ve found solution.\nBroker configuration So how to force ActiveMQ to preserve message order? It\u0026rsquo;s really simple, we just need to change dispatch policy for destination. We can do this for all queues or only for selected. [xml] \\[/xml\\] After this consumers should receive messages in same order like they were sent from producer. You can find example code on github: example-activemq-ordered. You can run all from maven: [bash] cd broker1; mvn activemq:run cd broker2; mvn activemq:run cd consumer; mvn camel:run cd consumer; mvn camel:run cd producer; mvn camel:run [/bash]\nUpgrade After posting update about this blog post to Twitter Dejan Bosanac send me few updates. He is co-author of ActiveMQ in Action so his knowledge is much more deeper than mine. :) First of all I mixed XML syntax. strictOrderDispatchPolicy is handled by topics, not queues. For second destination type strict order is turned on by strictOrderDispatch attribute set to true for policyEntry element. This preserves order but, as Dejan wrote, it will broke round robin and all messages will go to only one consumer, as in previous example given by Marcelo.\nAlso, Marcelo published second post about Message Groups which allows to preserve order and have multiple concurrent consumers on queue.\n","permalink":"https://dywicki.pl/2011/11/preserving-message-order/","summary":"\u003cp\u003eFew hours ago I\u0026rsquo;ve found an usefull \u003ca href=\"http://marcelojabali.blogspot.com/2011/11/preserving-message-order-with-apache.html\"\u003epost\u003c/a\u003e about preserving message order with \u003ca href=\"http://activemq.apache.org\"\u003eActiveMQ\u003c/a\u003e written by \u003ca href=\"http://marcelojabali.blogspot.com/\"\u003eMarcelo Jabali\u003c/a\u003e from \u003ca href=\"http://www.fusesource.com/\"\u003eFUSE Source\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIn his example Marcelo used broker feature called \u003ca href=\"http://activemq.apache.org/exclusive-consumer.html\"\u003eExclusive Consumers\u003c/a\u003e. It lets send messages only to one consumer and if it fails then second consumer gets all messages. I think it is not the best idea if we have many messages to process. Why we wouldn\u0026rsquo;t use few consumers with preserved message order? Well, I was sure it is not possible, but during last training I\u0026rsquo;ve found solution.\u003c/p\u003e","title":"Preserving message order in ActiveMQ"},{"content":"Management of OSGi - let\u0026rsquo;s face it - is not very hard. The OSGi environment is clearly defined and that gives programmers many mechanisms to create administrative tools. The problem begins when we would like to use only one tool to manage few projects or artifacts of different types. I know this from personal experience because when I run Camel, ActiveMQ and CXF every from them provides own administration console. Every of them requires own security configuration, looks differently, have own dependencies and so on.\nThis stands a little bit in contradiction with the OSGi specification idea, which tries to unify management of different things, not only dependencies (Core) but aspects like configuration (ConfigAdmin), deployment (DeploymentAdmin), meta data (MetaType), preferences (PreferencesService), users (UserAdmin) or permissions (PermissionAdmin). Naturally, creation of standard for management tools is too hard to be closed in any specification, even so good like OSGi.\nFelix WebConsole As response for problems with lack of common tool, available from browser a project Felix WebConsole was created. This is a subproject of Felix\u0026rsquo;a. In its assumptions Felix WebConsole should let easily extend itself through the use of mechanisms known from the core OSGi - that is, services. It should also let change look and feel and localize the tool. All these assumptions was covered, but number of extensions for Felix WebConsole do not grow like mushrooms after the rain. The question is, why? Now, in its simplicity Felix WebConsole make difficult creation of more complex extensions such as JMX. The problem is that our extension is only a servlet. From one hand it\u0026rsquo;s too much to put a link in list of bundles, on the other hand it is too little to make a few pages. If we\u0026rsquo;ll try we\u0026rsquo;ll begin to implement the second Struts framework based on servlets. Project team do not make things easier because it puts on the \u0026ldquo;lightness\u0026rdquo; console. It is expected to deploy console on mobile devices too.\nKaraf WebConsol as alternative Apache Karaf WebConsole is brand new project, which was made few months ago. After dynamic incubation phase, which maybe was too short, it was moved to sub-project of Karaf. Similar as precursor it points to lightness but also points to collaboration with other web frameworks, in this case it is another ASF project - the Apache Wicket. Through its use we obtained far-reaching component model. It means that you may add link to menu and style it with fragment CSS (without fragment bundle). You may add new tab with content or simply put another widget to dashboard. All these things you may see right after logging into console. All these extension won\u0026rsquo;t be possible without Wicket and Pax-Wicket. Thanks to huge amount of work made by Andreas Pieber on second project. Everything is stable and works as fast as Felix WebConsole.\nArchitecture of Karaf WebConsole As I meintoned before, WebConsole uses Pax-Wicket and Wicket as presentation layer (we don\u0026rsquo;t count Java Script libraries). Most of extensions uses also blueprint to register services. There is no problem, similary like in case of Felix WebConsole, to use Spring DM or declarative services. Everything works with Pax-Web, but it should be possible to run it with any HttpService. Whats\u0026rsquo;s more, an experimental branch of pax-wicket which I worked on previously named jsr330 let to use same components in a traditional container like is Tomcat. This means that the administrative tool went beyond the OSGi framework and will allow creation of multi-modular consoles, also in a traditional environment.\nExtensions New elements who are added to Karaf WebConsole are typically Wicket components or they are converted to them. Let see how to add new element to navigation: [java] package org.apache.karaf.webconsole.blueprint.internal.navigation;\nimport java.util.ArrayList; import java.util.List;\nimport org.apache.karaf.webconsole.blueprint.internal.BlueprintPage; import org.apache.karaf.webconsole.core.navigation.NavigationProvider; import org.apache.wicket.Page; import org.apache.wicket.markup.html.basic.Label; import org.apache.wicket.markup.html.link.BookmarkablePageLink; import org.apache.wicket.markup.html.link.Link;\npublic class BlueprintNavigationProvider implements NavigationProvider {\npublic List\u0026lt;Link\u0026gt; getItems(String componentId, String labelId) { List\u0026lt;Link\u0026gt; items = new ArrayList\u0026lt;Link\u0026gt;();\nLink link = new BookmarkablePageLink(componentId, BlueprintPage.class); link.add(new Label(labelId, \u0026ldquo;Blueprint\u0026rdquo;)); items.add(link);\nreturn items; }\n} [/java]\nInterface org.apache.karaf.webconsole.core.navigation.NavigationProvider is universal supplier of navigation elements. In web application it\u0026rsquo;s mostly about links. Now, when we have implementation we need to submit it into registry to let use it. In this particular example we going to use blueprint, but it might be a standard activator or any other declarative way as well. [xml]\u003c?xml version=\"1.0\" encoding=\"utf-8\" ?\u003e \\[/xml\\] Fragment above will cause addition of BlueprintPage as child of OSGi menu, because we set extends property. Result of execution you may see on attached picture.\nNavigation is only example, remember - you have much more possibilities:\norg.apache.karaf.webconsole.core.brand.BrandProvider - lets to change design (without fragment bundles) org.apache.karaf.webconsole.core.navigation.ConsoleTabProvider - causes addition of new tab in navigation org.apache.karaf.webconsole.core.navigation.SidebarProvider - adds new elements on left side org.apache.karaf.webconsole.core.widget.WidgetProvider - lets to publish new panels with content org.apache.karaf.webconsole.osgi.bundle.IActionProvider - adds specific link to bundle list org.apache.karaf.webconsole.osgi.bundle.IColumnProvider - adds column to bundle list org.apache.karaf.webconsole.osgi.bundle.IDecorationProvider - adds icon bundle list ","permalink":"https://dywicki.pl/2011/11/introduction-to-karaf-webconsole/","summary":"\u003cp\u003eManagement of OSGi - let\u0026rsquo;s face it - is not very hard. The OSGi environment is clearly defined and that gives programmers many mechanisms to create administrative tools. The problem begins when we would like to use only one tool to manage few projects or artifacts of different types. I know this from personal experience because when I run Camel, ActiveMQ and CXF every from them provides own administration console. Every of them requires own security configuration, looks differently, have own dependencies and so on.\u003c/p\u003e","title":"Introduction to Karaf WebConsole"},{"content":"Last two days I\u0026rsquo;ve spent hacking Swing code. I decided to run standalone producer application to show real interaction with broker. You may treat this Swing app like entry point for people to our middleware system. Users simply do \u0026ldquo;transfers\u0026rdquo; from this application and don\u0026rsquo;t know anything about technical details. I added text area to main window to show structure of message sent to broker.\nProducing messages with JMS Most of communication systems, whanever you will go have two different kinds of values, first - main and mainly used is body, second is typical metadata named headers or properties or parameters. JMS is not different, you can create different kinds of messages and set headers to them (in JMS world they\u0026rsquo;re named property, but I preffer header). Let check messaging code we have: [java highlight=\u0026ldquo;20,22,24,28-31\u0026rdquo;] // import declarations \u0026hellip; public class JmsMessageSender implements MessageSender, InitializingBean {\nprivate ConnectionFactory connectionFactory; private Session session; private final String destination; private MessageProducer producer;\npublic JmsMessageSender(String destination) { this.destination = destination; }\npublic void sendMessage(String message, Map\u0026lt;String, Object\u0026gt; headers) throws Exception {\nif (session == null || producer == null) { afterPropertiesSet(); }\nTextMessage jmsMsg = session.createTextMessage(message); for (String header : headers.keySet()) { jmsMsg.setObjectProperty(header, headers.get(header)); } producer.send(jmsMsg); }\npublic void afterPropertiesSet() throws Exception { Connection connection = connectionFactory.createConnection(); session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); Queue queue = session.createQueue(destination); producer = session.createProducer(queue); }\n// not important } [/java] First of all we inject connectionFactory using Spring configuration file. In line 20 we create text message, because we sending messages with String as content. Line 22 sets headers for message and finally sends it in line 24 using message producer created in lines 28-31. If we\u0026rsquo;ll look closer these lines we\u0026rsquo;ll see standard initalization code automatically called during bean creation by Spring. In line 29 we create session without transaction support and create producer to send messages. Connection factory is configured in XML with properties file to extract informations like broker url username and password.\n[source lang=\u0026ldquo;xml\u0026rdquo;]\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \\[/source\\] I decided to use JSON over XML. Regarding last post about data structures it is not the best choice but it is very simple. So don't treat it as reference implementation. I just wish to don't use JAXB annotations. :-) Headers Headers are common thing for routing. This is main reason why I allow to provide headers as parameter to sendMessage method. In many cases it is better to use headers or even extract headers from body of message in one endpoint to reduce number of content reads. Example how to use headers for routing we\u0026rsquo;ll see in next part about Apache Camel.\nJust as a note types of headers supported by JMS spec: Boolean Byte Short Int Long Float Double String Object We can get or set headers using get Type Property(String name) or set Type Property(String name, Type value). Remember that headers of type \u0026ldquo;object\u0026rdquo; set by setObjectProperty must be serializable, otherwise they will be dropped before sending.\nProducer application As I said at begining of the post I spent two days on Swing hacks. :) I injected two additional depentencies to this module - mig layout and better beans binding. First is responsible for frame layout, second for interactions between model and Swing controls. Whole application is simple form. All informations are displayed in status bar at bottom of the window, rest is taken by message area and combo boxes.\nI have say, that binding framework is cool and reduced number of code lines I had to write without it. If you are interested in exact code structure, please go to producer module in mom-sample github repository. Because it is not in area of this post I will not write more about desktop implementation details.\nFor these who wish run sample (I belive you would do that) - after executing [source lang=\u0026ldquo;bash\u0026rdquo;]mvn clean install[/source] You can simply execute target/producer.jar by doulbe click. This is fat-jar with all dependencies needed by producer application.\n","permalink":"https://dywicki.pl/2011/06/sh-html/","summary":"\u003cp\u003eLast two days I\u0026rsquo;ve spent hacking Swing code. I decided to run standalone producer application to show real interaction with broker. You may treat this Swing app like entry point for people to our middleware system. Users simply do \u0026ldquo;transfers\u0026rdquo; from this application and don\u0026rsquo;t know anything about technical details. I added text area to main window to show structure of message sent to broker.\u003c/p\u003e\n\u003ch2 id=\"producing-messages-with-jms\"\u003eProducing messages with JMS\u003c/h2\u003e\n\u003cp\u003eMost of communication systems, whanever you will go have two different kinds of values, first - main and mainly used is body, second is typical metadata named headers or properties or parameters. JMS is not different, you can create different kinds of messages and set headers to them (in JMS world they\u0026rsquo;re named property, but I preffer header).\nLet check messaging code we have:\n[java highlight=\u0026ldquo;20,22,24,28-31\u0026rdquo;]\n// import declarations \u0026hellip;\npublic class JmsMessageSender implements MessageSender, InitializingBean {\u003c/p\u003e","title":"Building sample middleware - producer"},{"content":"XML Schema jest bodajże najlepszym sposobem walidacji dokumentów XML. Model zastosowany w przypadku tego meta-języka pozwala na tworzenie złożonych konstrukcji. W oparciu o niego można budować własne rozszerzenia czy też zagnieżdżać w sekcjach xsd:appinfo dodatkowe metadane. Dzisiaj jednak nie o tym, a o wzorcach projektowych. Sam się zdziwiłem gdy trafiłem na artykuł Introducing Design Patterns in XML Schemas. W życiu się nie zastanawiałem czy to co piszę w XSD ma coś wspólnego z wzorcami czy nie. Sun wyprzedził w tym momencie chyba wszystkich. :-)\nWzorce, które zostały wymienione we wspomnianym artykule odnoszą się do powiązania definiowanych typów z definiowanymi elementami. Ciężko mi się zgodzić z tym, że wybór wzorca jest krytyczny przy projektowaniu schematu, ponieważ schemat zazwyczaj ma przeznaczenie już w chwili pisania i zazwyczaj nie możemy powiedzieć, zrobimy to wzorcem X, ponieważ sam nasuwa się wzorzec Y. Ale to tak tylko moim zdaniem.\nTabelka poniżej prezentuje zawartość przeniesioną ze strony Suna. Zawiera ona 4 najpopularniejsze wzorce. Dwa najczęściej spotykane w internecie to Venetian Blind oraz Garden of Eden ze względu na to, że są bardzo podatne na ponowne użycie. WzorzecCharakterystykaRussian Doll, przykładZawiera jeden element globalny, pozostałe są lokalne.\nJest tylko jeden poprawny element.\nMoże uprościć przestrzeń nazw poprzez zastosowanie atrybutu elementFormDefault dla elementu xsd:schema.\nNadaje się tylko dla pojedynczych schematów.\nPozwala na ponowne użycie tylko całej gałęzi, a nie każdego typu z osobna.\nSalami Slice, przykładWszystkie elementy są globalne, stąd każdy może być użyty w charakterze root node\u0026rsquo;a.\nWszystkie elementy można ponownie użyć.\nŁatwe wiązanie schematów pomiędzy plikami.\nPowoduje większą złożoność w przestrzeni nazw.\nTrudny do określenia root.\nVenetian Blind, przykładPochodna Russian Doll, zawiera jeden element globalny, pozostałe są lokalne\nZawiera tylko jeden element nadrzędny.\nPozwala na ponowne użycie wszystkich typów oraz elementu nadrzędnego.\nŁatwa praca z wieloma plikami.\nOgraniczona enkapsulacja poprzez ekspozycję wszystkich typów.\nGarden of Eden, przykładPołączenie Venetian Blind oraz Salami Slice. Wiele elementów globalnych, wiele typów publicznych. Wiele kandydatów na root node.\nPozwala ponownie użyć elementy oraz typy.\nŁatwa praca z wieloma plikami.\nZawiera wiele potencjalnych elementów nadrzędnych.\nOgraniczona enkapsulacja.\nTrudna do czytania i zrozumienia.\nŹródło: Sun Developers Network\nPrzykłady Russian Doll [sourcecode lang=\u0026ldquo;xml\u0026rdquo;]\u0026lt;xsd:schema xmlns:xsd=\u0026ldquo;http://www.w3.org/2001/XMLSchema\u0026quot; targetNamespace=\u0026ldquo;http://schemas.sun.com/point/russiandoll\u0026quot; xmlns:tns=\u0026ldquo;http://schemas.sun.com/point/russiandoll\u0026quot; elementFormDefault=\u0026ldquo;qualified\u0026rdquo;\u0026gt;\n\u0026lt;xsd:element name=\u0026ldquo;Line\u0026rdquo;\u0026gt; xsd:complexType xsd:sequence \u0026lt;xsd:element name=\u0026ldquo;PointA\u0026rdquo;\u0026gt; xsd:complexType \u0026lt;xsd:attribute name=\u0026ldquo;x\u0026rdquo; type=\u0026ldquo;xsd:integer\u0026rdquo;/\u0026gt; \u0026lt;xsd:attribute name=\u0026ldquo;y\u0026rdquo; type=\u0026ldquo;xsd:integer\u0026rdquo;/\u0026gt; \u0026lt;/xsd:complexType\u0026gt; \u0026lt;/xsd:element\u0026gt; \u0026lt;xsd:element name=\u0026ldquo;PointB\u0026rdquo;\u0026gt; xsd:complexType \u0026lt;xsd:attribute name=\u0026ldquo;x\u0026rdquo; type=\u0026ldquo;xsd:integer\u0026rdquo;/\u0026gt; \u0026lt;xsd:attribute name=\u0026ldquo;y\u0026rdquo; type=\u0026ldquo;xsd:integer\u0026rdquo;/\u0026gt; \u0026lt;/xsd:complexType\u0026gt; \u0026lt;/xsd:element\u0026gt; \u0026lt;/xsd:sequence\u0026gt; \u0026lt;/xsd:complexType\u0026gt; \u0026lt;/xsd:element\u0026gt; \u0026lt;/xsd:schema\u0026gt;[/sourcecode]\nSalami Slice [sourcecode lang=\u0026ldquo;xml\u0026rdquo;]\u0026lt;xsd:schema xmlns:xsd=\u0026ldquo;http://www.w3.org/2001/XMLSchema\u0026quot; targetNamespace=\u0026ldquo;http://schemas.sun.com/point/salami\u0026quot; xmlns:tns=\u0026ldquo;http://schemas.sun.com/point/salami\u0026quot; xmlns=\u0026ldquo;http://schemas.sun.com/point/salami\u0026quot; elementFormDefault=\u0026ldquo;qualified\u0026rdquo;\u0026gt;\n\u0026lt;xsd:element name=\u0026ldquo;PointA\u0026rdquo;\u0026gt; xsd:complexType \u0026lt;xsd:attribute name=\u0026ldquo;x\u0026rdquo; type=\u0026ldquo;xsd:integer\u0026rdquo;/\u0026gt; \u0026lt;xsd:attribute name=\u0026ldquo;y\u0026rdquo; type=\u0026ldquo;xsd:integer\u0026rdquo;/\u0026gt; \u0026lt;/xsd:complexType\u0026gt; \u0026lt;/xsd:element\u0026gt;\n\u0026lt;xsd:element name=\u0026ldquo;PointB\u0026rdquo;\u0026gt; xsd:complexType \u0026lt;xsd:attribute name=\u0026ldquo;x\u0026rdquo; type=\u0026ldquo;xsd:integer\u0026rdquo;/\u0026gt; \u0026lt;xsd:attribute name=\u0026ldquo;y\u0026rdquo; type=\u0026ldquo;xsd:integer\u0026rdquo;/\u0026gt; \u0026lt;/xsd:complexType\u0026gt; \u0026lt;/xsd:element\u0026gt;\n\u0026lt;xsd:element name=\u0026ldquo;Line\u0026rdquo;\u0026gt; xsd:complexType xsd:sequence \u0026lt;xsd:element ref=\u0026ldquo;PointA\u0026rdquo;/\u0026gt; \u0026lt;xsd:element ref=\u0026ldquo;PointB\u0026rdquo;/\u0026gt; \u0026lt;/xsd:sequence\u0026gt; \u0026lt;/xsd:complexType\u0026gt; \u0026lt;/xsd:element\u0026gt; \u0026lt;/xsd:schema\u0026gt;[/sourcecode]\nVenetian Blind [sourcecode lang=\u0026ldquo;xml\u0026rdquo;]\u0026lt;xsd:schema xmlns:xsd=\u0026ldquo;http://www.w3.org/2001/XMLSchema\u0026quot; targetNamespace=\u0026ldquo;http://schemas.sun.com/point/venetianblind\u0026quot; xmlns:tns=\u0026ldquo;http://schemas.sun.com/point/venetianblind\u0026quot; xmlns=\u0026ldquo;http://schemas.sun.com/point/venetianblind\u0026quot; elementFormDefault=\u0026ldquo;qualified\u0026rdquo;\u0026gt;\n\u0026lt;xsd:complexType name=\u0026ldquo;PointType\u0026rdquo;\u0026gt; \u0026lt;xsd:attribute name=\u0026ldquo;x\u0026rdquo; type=\u0026ldquo;xsd:integer\u0026rdquo;/\u0026gt; \u0026lt;xsd:attribute name=\u0026ldquo;y\u0026rdquo; type=\u0026ldquo;xsd:integer\u0026rdquo;/\u0026gt; \u0026lt;/xsd:complexType\u0026gt;\n\u0026lt;xsd:element name=\u0026ldquo;Line\u0026rdquo;\u0026gt; xsd:complexType xsd:sequence \u0026lt;xsd:element name=\u0026ldquo;PointA\u0026rdquo; type=\u0026ldquo;PointType\u0026rdquo;/\u0026gt; \u0026lt;xsd:element name=\u0026ldquo;PointB\u0026rdquo; type=\u0026ldquo;PointType\u0026rdquo;/\u0026gt; \u0026lt;/xsd:sequence\u0026gt; \u0026lt;/xsd:complexType\u0026gt; \u0026lt;/xsd:element\u0026gt; \u0026lt;/xsd:schema\u0026gt; [/sourcecode]\nGarden of Eden [sourcecode lang=\u0026ldquo;xml\u0026rdquo;]\u0026lt;xsd:schema xmlns:xsd=\u0026ldquo;http://www.w3.org/2001/XMLSchema\u0026quot; targetNamespace=\u0026ldquo;http://schemas.sun.com/point/gardenofeden\u0026quot; xmlns=\u0026ldquo;http://schemas.sun.com/point/gardenofeden\u0026quot; elementFormDefault=\u0026ldquo;qualified\u0026rdquo;\u0026gt;\n\u0026lt;xsd:complexType name=\u0026ldquo;PointType\u0026rdquo;\u0026gt; \u0026lt;xsd:attribute name=\u0026ldquo;x\u0026rdquo; type=\u0026ldquo;xsd:integer\u0026rdquo;/\u0026gt; \u0026lt;xsd:attribute name=\u0026ldquo;y\u0026rdquo; type=\u0026ldquo;xsd:integer\u0026rdquo;/\u0026gt; \u0026lt;/xsd:complexType\u0026gt;\n\u0026lt;xsd:complexType name=\u0026ldquo;LineType\u0026rdquo;\u0026gt; xsd:sequence \u0026lt;xsd:element ref=\u0026ldquo;PointA\u0026rdquo;/\u0026gt; \u0026lt;xsd:element ref=\u0026ldquo;PointB\u0026rdquo;/\u0026gt; \u0026lt;/xsd:sequence\u0026gt; \u0026lt;/xsd:complexType\u0026gt;\n\u0026lt;xsd:element name=\u0026ldquo;PointA\u0026rdquo; type=\u0026ldquo;PointType\u0026rdquo;/\u0026gt;\n\u0026lt;xsd:element name=\u0026ldquo;PointB\u0026rdquo; type=\u0026ldquo;PointType\u0026rdquo;/\u0026gt;\n\u0026lt;xsd:element name=\u0026ldquo;Line\u0026rdquo; type=\u0026ldquo;LineType\u0026rdquo;/\u0026gt; \u0026lt;/xsd:schema\u0026gt; [/sourcecode]\n","permalink":"https://dywicki.pl/2011/06/xml-schema-design-patterns/","summary":"\u003cp\u003eXML Schema jest bodajże najlepszym sposobem walidacji dokumentów XML. Model zastosowany w przypadku tego meta-języka pozwala na tworzenie złożonych konstrukcji. W oparciu o niego można budować własne rozszerzenia czy też zagnieżdżać w sekcjach xsd:appinfo dodatkowe metadane. Dzisiaj jednak nie o tym, a o wzorcach projektowych. Sam się zdziwiłem gdy trafiłem na artykuł \u003ca href=\"http://developers.sun.com/jsenterprise/archive/nb_enterprise_pack/reference/techart/design_patterns.html\"\u003eIntroducing Design Patterns in XML Schemas\u003c/a\u003e. W życiu się nie zastanawiałem czy to co piszę w XSD ma coś wspólnego z wzorcami czy nie. Sun wyprzedził w tym momencie chyba wszystkich. :-)\u003c/p\u003e","title":"XML Schema Design Patterns"},{"content":"Data with middleware The data structures are very important in middleware world. Because we often connect multiple systems we need to define an \u0026ldquo;domain model\u0026rdquo; for integration. The domain model means that objects we share between all systems are well known, well defined, well understand in multiple teams often provided by multiple vendors. Let see what does it means in practice.\nWell known If you talk with business guys you have to use same terms with programmers. You might be mediator but not the translator. As integration team member mediate correct shape of solution but don\u0026rsquo;t became a translator between business divisions and developers (unless you are business analitic). All people should use same terms to minimalise problems with number of definitions. That\u0026rsquo;s first step of multiple projects, not only integration. If you look for deeper knowledge of domain definitions check a \u0026quot; Domain Driven Design\u0026quot; book written by Eric Evans and published in 2003. Ok, maybe business is not always part of integration project, but you communicate systems used by people, isn\u0026rsquo;t?\nWell defined You have names and scopes for your classes and objects. Probably you have 90% of fields that will be defined in them. Remember to don\u0026rsquo;t put too much informations from one system to domain model. Domain model is not list of entities from one system. Make sure that you don\u0026rsquo;t share database identifier from one system to second. Identifier should be business, for example most of clients might be identified by personal document, most of companies can be identified by tax id, not by database sequence. For future reasons try to extract atomic values, like for databases. With atomic values you will have less text processing to extract specific data and less code to maintain.\nWell understand That\u0026rsquo;s case for distributed teams. Usually every team have own parts of software to do and every team plays different game. Someone else produces messages, someone consumes them, this is place when you\u0026rsquo;ll have small wars. Make sure that data structures you all have to share match common needs, that defined identifiers are fine for different systems too. I met multiple situations when system analytics from two companies spent lots of hours on phone defining field constraints. Don\u0026rsquo;t follow this path.\nWhole integration project is about to fail in 95% cases - if you will not have data structures. Ok, we know that proper structure is key for integration. Second important point in data is representation. I don\u0026rsquo;t going to talk about REST at all, but rather about format of serialization. We have number of options here, just look table below Serialization FormatProsConsJava Object Serialization\nVery easy to maintain\nEasy to use with JMS / ActiveMQ - ObjectMessages\nBuilt in Java\nAvailable only for systems written in Java\nBoth sides must use same classes and versions (if we use serialization id)\nAs every binary format - it is hard to process\nXML (with XML Schema)\nEasy to read and write, both for computer and people (I don\u0026rsquo;t belive YAML)\nEasy to use with JMS / ActiveMQ - TextMessages\nVery easy validation, with XML Schema\nWell supported in many languages (including validation with XML Schema), DOM api provides complex operations\nRequires additional libraries, some languages don\u0026rsquo;t have any binding from XML to Objects and vice versa\nSynchronisation between Java and XML Schema (code generation)\nWithout proper XML Schema might be too lax\nNot the best for big portions of data because overflow\nJSON\nEasy to read and write, both for computer and people, even easier than XML\nEasy to use with JMS / ActiveMQ - TextMessages\nSmaller than XML, easier to parse, even without any API\nSupported in number of languages\nLack of well supported official structure control tool (like XML Schema)\nLack of official API for handling JSON\nRequires additional libraries, some languages don\u0026rsquo;t have any binding from JSON to Objects and vice versa\nThere is number of different wire formats not listed here - for example Protobuf, EDI which match this scenario too. I don\u0026rsquo;t count them (even if I should) because first is not well supported in many languages, second is rather legacy format not well for new systems.\nStructures used in MOM Like in most cases, also with middleware we have some data structures. Because we going to process simple cash transactions we\u0026rsquo;ll have following classes:\norg.code_house.mom.domain.Transaction org.code_house.mom.domain.Client org.code_house.mom.domain.Money org.code_house.mom.domain.Money.Currency Transaction Every transaction will have unique identifier generated by UUID. Transaction will point to some Client and have assigned Money object.\nClient In our case client is very simple POJO which contains fields ID and name. For middleware purposes we going to use only ID. Name might be usefull for user interface operations.\nMoney Because we going to transfer some money to client account from producer we have to use proper data structure for a \u0026ldquo;cash\u0026rdquo;. Few years ago Martin Fowler in his book \u0026quot; Patterns of Enterprise Application Architecture\u0026quot; defined a Money pattern which is suitable to handle money values. We have to remember that client account might be handled in different currency and we cannot send only amount information. We need a currency too.\nThat\u0026rsquo;s all for data structures. It was more about philosophy than about code, but don\u0026rsquo;t worry - in next part we going to write more. All needed classes you\u0026rsquo;ll find in git repository. You may download zip archive | tar archive if you don\u0026rsquo;t have git client.\n","permalink":"https://dywicki.pl/2011/06/building-sample-middleware-data-structures/","summary":"\u003ch2 id=\"data-with-middleware\"\u003eData with middleware\u003c/h2\u003e\n\u003cp\u003eThe data structures are very important in middleware world. Because we often connect multiple systems we need to define an \u0026ldquo;domain model\u0026rdquo; for integration. The domain model means that objects we share between all systems are \u003cstrong\u003ewell known\u003c/strong\u003e, \u003cstrong\u003ewell defined\u003c/strong\u003e, \u003cstrong\u003ewell understand\u003c/strong\u003e in multiple teams often provided by multiple vendors. Let see what does it means in practice.\u003c/p\u003e\n\u003ch4 id=\"well-known\"\u003eWell known\u003c/h4\u003e\n\u003cp\u003eIf you talk with business guys you have to use same terms with programmers. You might be mediator but not the translator. As integration team member mediate correct shape of solution but don\u0026rsquo;t became a translator between business divisions and developers (unless you are business analitic). All people should use same terms to minimalise problems with number of definitions. That\u0026rsquo;s first step of multiple projects, not only integration. If you look for deeper knowledge of domain definitions check a \u0026quot; \u003ca href=\"http://domaindrivendesign.org/books/evans_2003\"\u003eDomain Driven Design\u003c/a\u003e\u0026quot; book written by \u003ca href=\"http://domaindrivendesign.org/about#eric\"\u003eEric Evans\u003c/a\u003e and published in 2003. Ok, maybe business is not always part of integration project, but you communicate systems used by people, isn\u0026rsquo;t?\u003c/p\u003e","title":"Building sample middleware – data structures"},{"content":"At end of the May I had great time in United Kingdom providing consulting. After this I started thinking about sharing an idea of middleware application based on ActiveMQ and Camel features. I\u0026rsquo;ve spent few days to create sufficient example.\nWhat middleware is? Middleware is a general term, about some software which is some kind of proxy between other systems. What for? - you could ask. Generally because communication from point to point is not the best to build bigger applications, and some components in middle of communication allows us to inject new logic without changing source systems. Could you imagine situation where you have a number of system producing messages and an number of consumers of these messages written in different languages? That\u0026rsquo;s typical case where middleware is going to be usefull. There is number of middleware tools from various categories, in this post we\u0026rsquo;ll learn how to use ActiveMQ to build message oriented middleware. What does the MOM means? Generally that we have asynchronous communication without direct method invocations. Producer don\u0026rsquo;t know anything about consumer and vice versa. If you are interested in MOM - pick up \u0026quot; ActiveMQ in Action\u0026quot; book (written by commiters of ActiveMQ) from Manning Publications Co. where this term is described in greater detail.\nThe scenario In our scenario we we\u0026rsquo;ll use number of queues, every client will have own queue and will consume messages only from this queue. The producer will send messages with header which will be used to determine client queue. In the middle we have Apache Camel which will process incoming messages and dispatch them to concrete client queues. Camel will also copy messages to log queue for audit processor (not listed on image). As alternative I going to show you how to extract client ID from message body instead of forcing producer to use headers.\nSetting up ActiveMQ If you are Linux or MacOS user you could use brew package manager to install activemq using following command: [source lang=\u0026ldquo;bash\u0026rdquo;]brew install activemq[/source] Other users can download ActiveMQ package from project site.\nOk, we have ActiveMQ installed, in next steep we going to create an test instance of activemq where we\u0026rsquo;ll hack configuration. To do this execute following command: [source lang=\u0026ldquo;bash\u0026rdquo;]activemq-admin create mom[/source] After execution of command you should get following directory structure We don\u0026rsquo;t going to use all of these files, so in my set up I going to remove next configuration files:\nBecause we don\u0026rsquo;t going to use SSL yet: broker.ks broker.ts Because we don\u0026rsquo;t use security yet: credentials.properties Because we don\u0026rsquo;t going to use ActiveMQ webconsole: jetty-realm.properties jetty.xml These files are unecessary at this moment in our setup.\nConfiguring ActiveMQ Becase activemq-admin script creates dummy broker configuration we have to modify it a bit. Remember to put elements in XML file with alphabetic order. Since ActiveMQ 5.4 release all elements are ordered.. First of all we going to modify destinationPolicy element: [source lang=\u0026ldquo;xml\u0026rdquo; highlight=\u0026ldquo;4,5\u0026rdquo;] [/source] First policyEntry element disables producerFlowControl. In other words ActiveMQ will accept all messages sent by producer, even when the memory limit was exceeded. Messages that are not able to be put in memory will be persisted in store. By default ActiveMQ have producerFlowControl set to true, and broker will notify producer to wait untill there will be place in memory, eg. messages from queue will be consumed. Second entry turn on advisory messages for message delivery and consumption for MOM.Client.* queues. These advisory messages are sent when message was received from producer and was succesfully processed by consumer. ActiveMQ offers number of advisory topics and messages. Some of them are disabled, some other are enabled by default.\nSecond change in configuration file is predefined destinations. [source lang=\u0026ldquo;xml\u0026rdquo;] [/source]\nThese destinations will be created when broker starts, even if there is no consumers or producers. After these changes we can start our broker with following command: [source lang=\u0026ldquo;bash\u0026rdquo;]$ACTIVEMQ_HOME/bin/mom console[/source] With VisualVM we can check which destinations are created by default. When you start monitoring tool simply select local process with org.apache.activemq.console.Main which should be displayer after execution of command listed above. That\u0026rsquo;s all for ActiveMQ set up. If you have troubes with configuration changes, you can pick up all changes directly from git repository or download zip archive | tar archive if you don\u0026rsquo;t have git client.\n","permalink":"https://dywicki.pl/2011/06/building-sample-middleware-broker/","summary":"\u003cp\u003eAt end of the May I had great time in United Kingdom providing consulting. After this I started thinking about sharing an idea of middleware application based on ActiveMQ and Camel features. I\u0026rsquo;ve spent few days to create sufficient example.\u003c/p\u003e\n\u003ch1 id=\"what-middleware-is\"\u003eWhat middleware is?\u003c/h1\u003e\n\u003cp\u003eMiddleware is a general term, about some software which is some kind of proxy between other systems. What for? - you could ask. Generally because communication from point to point is not the best to build bigger applications, and some components in middle of communication allows us to inject new logic without changing source systems. Could you imagine situation where you have a number of system producing messages and an number of consumers of these messages written in different languages? That\u0026rsquo;s typical case where middleware is going to be usefull.\nThere is number of middleware tools from various categories, in this post we\u0026rsquo;ll learn how to use ActiveMQ to build message oriented middleware. What does the MOM means? Generally that we have asynchronous communication without direct method invocations. Producer don\u0026rsquo;t know anything about consumer and vice versa. If you are interested in MOM - pick up \u0026quot; \u003ca href=\"http://manning.com/snyder/\"\u003eActiveMQ in Action\u003c/a\u003e\u0026quot; book (written by commiters of ActiveMQ) from \u003ca href=\"http://manning.com\"\u003eManning Publications Co.\u003c/a\u003e where this term is described in greater detail.\u003c/p\u003e\n","title":"Building sample middleware - broker"},{"content":"Pod koniec maja świetnie spędziłem czas w Wielkiej Brytanii dostarczając consulting. Tuż po nim zacząłem myśleć o podzieleniu się ideą aplikacji middleware zbudowanej na ActiveMQ z funkcjami Camela. Spędziłem kilka dni tworząc wystarczający przykład.\nCzym jest middleware? Middleware jest ogólnym terminem traktującym o oprogramowaniu które jest pewnego rodzaju pośrednikiem między systemami. Po cóż? - możesz zapytać. Generalnie ponieważ komunikacja pomiędzy punktami nie jest najlepsza do budowania większych aplikacji, co więcej niektóre komponenty pomiędzy komunikującymi pozwalają wstrzykiwać nową logikę bez zmieniania systemów komunikujących się. Czy możesz wyobrazić sobie sytuację gdzie wiele systemów tworzy komunikaty i wielu konsumentów tych komunikatów stworzonych w różnych językach? To jest typowy scenariusz gdzie middleware będzie użyteczne. Istnieje wiele narzędzi middleware i kilka ich kategorii, w tym wpisie dowiemy się jak używać ActiveMQ do budowania message oriented middleware. Co oznacza termin MOM? To, że mamy asynchroniczną komunikację bez bezpośrednich wywołań metod. Producent nic nie wie o konsumencie i na odwrót. Jeśli jesteś zainteresowany MOM-ami - zajrzyj do książki \u0026ldquo;ActiveMQ in Action\u0026rdquo; (napisanej przez commiterów ActiveMQ) z wydawnictwa Manning, w której termin ten jest opisany szerzej.\nScenariusz W naszym scenariuszu będziemy mieli kilka kolejek, każdy klient będzie posiadał własną kolejkę i konsumował komunikaty tylko z niej. Producent będzie wysyłał komunikaty z nagłówkiem, który będzie użyty to stworzenia nazwy kolejki klienta. Wewnątrz będziemy używać Apache Camel, który będzie przetwarzał wiadomości przychodzące i rozsyłał je do kolejek konkretnych klientów. Camel będzie również kopiował komunikaty do kolejki z logami dla procesu audytującego (niewymieniony na diagramie). Jako alternatywę zamierzam wam pokazać jak pobrać identyfikator klienta z treści komunikaty zamiast zmuszać producenta do używania nagłówków.\nStawianie ActiveMQ Jeśli jesteś użytkownikiem Linuxa bądź MacOSa możesz użyć menadżera paczek brew by zainstlować activemq następującym poleceniem: [source lang=\u0026ldquo;bash\u0026rdquo;]brew install activemq[/source] Pozostali użytkownicy mogą pobrać dystrybucję ActiveMQ ze strony projektu.\nMamy zainstalowane ActiveMQ, w następnym kroku stworzymy testową instancję brokera, którą wykorzystamy do modyfikacji konfiguracji. Aby stworzyć instancję należy wywołać polecenie: [source lang=\u0026ldquo;bash\u0026rdquo;]activemq-admin create mom[/source] Po wykonaniu tego polecenia powinniście otrzymać następującą strukturę katalogów Nie będziemy używać wszystkich plików, zatem w moim setupie zostaną usunięte następujące pliki:\nPonieważ jeszcze nie używamy SSL: broker.ks broker.ts Ponieważ nie zabezpieczamy brokera: credentials.properties Ponieważ nie będziemy używać konsoli webowej: jetty-realm.properties jetty.xml Te pliki są niepotrzebne w tej chwili w naszej instalacji.\nKonfiguracja ActiveMQ Ponieważ skrypt activemq-admin tworzy domyślną konfigurację brokera będziemy musieli ją nieco zmodyfikować. Pamiętaj aby wstawiać elementy w XML w porządku alfabetycznym. Od wydania 5.4 elementy w konfiguracji ActiveMQ są uporządkowane.. Na początku zmienimy element destinationPolicy: [source lang=\u0026ldquo;xml\u0026rdquo; highlight=\u0026ldquo;4,5\u0026rdquo;] [/source] Pierwszy element policyEntry wyłącza producerFlowControl. Innymi słowy ActiveMQ będzie akceptować wszystkie wiadomości wysłane przez producenta nawet po przekroczeniu limitu pamięci. Komunikaty które nie będą mieściły się w pamięci będą zapisywane (na dysku). Domyślnie dyrektywa producerFlowControl w ActiveMQ jest ustawiona na true i broker będzie informował producenta by poczekać na zwolnienie pamięci, np. na to by komunikaty z kolejki zostały skonsumowane. Drugi element włącza komunikaty pomocnicze dla dostarczenia i konsumpcji komunikató∑ z kolejek MOM.Client.*. Komunikaty te są wysyłane przez brokera gdy komunikat od producenta został przyjęty i pomyślnie przetworzony przz konsumenta. ActiveMQ oferuje kila komunikatów i topiców pomocniczych. Niektóre z nich są domyślnie wyłączone, inne włączone.\nDruga zmiana to dodanie w konfiguracji pre definiowanych destynacji. [source lang=\u0026ldquo;xml\u0026rdquo;] [/source]\nOwe destynacje zostaną stworzone podczas uruchomienia brokera, nawet jeśli nie będzie konsumentów i producentów. Po tych zmianach możemy wystartować naszego brokera następującym poleceniem: [source lang=\u0026ldquo;bash\u0026rdquo;]$ACTIVEMQ_HOME/bin/mom console[/source] Z poziomu VisualVM możemy sprawdzić które destynacje zostały stworzone. Po uruchomieniu narzędzia monitorującego po prostu wybieramy lokalny proces org.apache.activemq.console.Main, który powinien być wyświetlony po wykonaniu powyższego polecenia. To wszystko jeśli idzie o ustawienia ActiveMQ. Jeśli trafiłeś na problemy ze zmianami w konfiguracji możesz je pobrać bezpośrednio z repozytorium git bądź pobrać archiwum zip | archiwum tar w przypadku gdy nie masz klienta git.\nCzęść 1: Building sample middleware - broker Część 2: Building sample middleware - struktury danych Część 3: Building sample middleware - producent Część 4: Building sample middleware - camel Część 5: Building sample middleware - konsument Część 6: Building sample middleware - audyt ","permalink":"https://dywicki.pl/2011/06/budowanie-przykladowego-middleware-broker/","summary":"\u003cp\u003ePod koniec maja świetnie spędziłem czas w Wielkiej Brytanii dostarczając consulting. Tuż po nim zacząłem myśleć o podzieleniu się ideą aplikacji middleware zbudowanej na ActiveMQ z funkcjami Camela. Spędziłem kilka dni tworząc wystarczający przykład.\u003c/p\u003e\n\u003ch1 id=\"czym-jest-middleware\"\u003eCzym jest middleware?\u003c/h1\u003e\n\u003cp\u003eMiddleware jest ogólnym terminem traktującym o oprogramowaniu które jest pewnego rodzaju pośrednikiem między systemami. Po cóż? - możesz zapytać. Generalnie ponieważ komunikacja pomiędzy punktami nie jest najlepsza do budowania większych aplikacji, co więcej niektóre komponenty pomiędzy komunikującymi pozwalają wstrzykiwać nową logikę bez zmieniania systemów komunikujących się. Czy możesz wyobrazić sobie sytuację gdzie wiele systemów tworzy komunikaty i wielu konsumentów tych komunikatów stworzonych w różnych językach? To jest typowy scenariusz gdzie middleware będzie użyteczne.\nIstnieje wiele narzędzi middleware i kilka ich kategorii, w tym wpisie dowiemy się jak używać ActiveMQ do budowania message oriented middleware. Co oznacza termin MOM? To, że mamy asynchroniczną komunikację bez bezpośrednich wywołań metod. Producent nic nie wie o konsumencie i na odwrót. Jeśli jesteś zainteresowany MOM-ami - zajrzyj do książki \u003ca href=\"http://manning.com/snyder/\"\u003e\u0026ldquo;ActiveMQ in Action\u0026rdquo;\u003c/a\u003e (napisanej przez commiterów ActiveMQ) z \u003ca href=\"http://manning.com\"\u003ewydawnictwa Manning\u003c/a\u003e, w której termin ten jest opisany szerzej.\u003c/p\u003e\n","title":"Budowanie przykładowego middleware - broker"},{"content":"Hej, Nazywam się Łukasz a ten blog jest o tworzeniu middleware bez licencji. Zazwyczaj, jeśli nie kupujesz licencji znajdujesz crack, ale w naszym przypadku - używamy open source.\nZacząłem używać open source dawno temu, kiedy byłem programistą PHP. Od tamtego czasu spotkałem wiele projektów i zrobiłem w nich liczne poprawki. Generalnie, nie wszystkie projekty są gotowe do użycia produkcyjnego i nie będę zmuszał do produktyzacji tych kiepskich.\nJeśli jesteś zainteresowany moim profilem zawodowym, proszę odwiedź Linked In. Jestem otwarty na oferty pracy i konsultingu na całym świecie. ","permalink":"https://dywicki.pl/o-mnie/","summary":"\u003cp\u003eHej,\nNazywam się Łukasz a ten blog jest o tworzeniu middleware bez licencji. Zazwyczaj, jeśli nie kupujesz licencji znajdujesz crack, ale w naszym przypadku - używamy open source.\u003c/p\u003e\n\u003cp\u003eZacząłem używać open source dawno temu, kiedy byłem programistą PHP. Od tamtego czasu spotkałem wiele projektów i zrobiłem w nich liczne poprawki. Generalnie, nie wszystkie projekty są gotowe do użycia produkcyjnego i nie będę zmuszał do produktyzacji tych kiepskich.\u003c/p\u003e\n\u003cp\u003eJeśli jesteś zainteresowany moim profilem zawodowym, proszę odwiedź Linked In. Jestem otwarty na oferty pracy i konsultingu na całym świecie.\n\u003ca href=\"http://pl.linkedin.com/in/splatch\"\u003e\u003cimg alt=\"View Łukasz Dywicki\u0026rsquo;s profile on LinkedIn\" loading=\"lazy\" src=\"http://www.linkedin.com/img/webpromo/btn_myprofile_160x33.png\"\u003e\u003c/a\u003e\u003c/p\u003e","title":"O mnie"},{"content":"Hey, My name is Lukasz and this blog is about making middleware without licenses. Normally if you don\u0026rsquo;t buy license you crack something, but in our case - we use open source stuff.\nI started using open source software long time ago, when I was a PHP programmer. From that time I meet various projects and I did number of hacks in them. Generally not all projects are ready to production use and I won\u0026rsquo;t force you to do production with some weak project.\nIf you would check my professional profile please visit Linked In. I am open for job opportunities, consulting offers across the world. ","permalink":"https://dywicki.pl/about-me/","summary":"\u003cp\u003eHey,\nMy name is Lukasz and this blog is about making middleware without licenses. Normally if you don\u0026rsquo;t buy license you crack something, but in our case - we use open source stuff.\u003c/p\u003e\n\u003cp\u003eI started using open source software long time ago, when I was a PHP programmer. From that time I meet various projects and I did number of hacks in them. Generally not all projects are ready to production use and I won\u0026rsquo;t force you to do production with some weak project.\u003c/p\u003e","title":"About me"},{"content":"I work with Apache Karaf almost every day. There is a lot of commands provided by default and most of them are a bit anonymous. In this post I would like introduce these commands.\nList bundles Common command executed in Karaf shell is list. There is few switches which makes this command more usable. First of them is -l which shows bundle locations, second is -t. Second switch is available from Karaf 2.1.\nBelow example output of these commands:\n__ __ ____ / //_/____ __________ _/ __/ / ,\u0026lt; / __ `/ ___/ __ `/ /_ / /| |/ /_/ / / / /_/ / __/ /_/ |_|__,_/_/ __,_/_/ Apache Karaf (2.1.2) Hit \u0026#39;\u0026lt;tab\u0026gt;\u0026#39; for a list of available commands and \u0026#39;[cmd] --help\u0026#39; for help on a specific command. Hit \u0026#39;\u0026lt;ctrl-d\u0026gt;\u0026#39; or \u0026#39;osgi:shutdown\u0026#39; to shutdown Karaf. karaf@root\u0026gt; list -l START LEVEL 100 , List Threshold: 50 ID State Blueprint Spring Level Location [ 34] [Resolved ] [ ] [ ] [ 60] mvn:org.apache.geronimo.specs/geronimo-servlet_2.5_spec/1.1.2 [ 39] [Active ] [ ] [ ] [ 60] mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.jetty/6.1.25_1 karaf@root\u0026gt; list -l -t 0 START LEVEL 100 , List Threshold: 0 ID State Blueprint Spring Level Location [ 0] [Active ] [ ] [ ] [ 0] System Bundle [ 1] [Active ] [ ] [ ] [ 5] mvn:org.ops4j.pax.url/pax-url-mvn/1.2.1 [ 2] [Active ] [ ] [ ] [ 5] mvn:org.ops4j.pax.url/pax-url-wrap/1.2.1 [ 3] [Active ] [ ] [ ] [ 8] mvn:org.ops4j.pax.logging/pax-logging-api/1.5.3 [ 4] [Active ] [ ] [ ] [ 8] mvn:org.ops4j.pax.logging/pax-logging-service/1.5.3 [ 5] [Active ] [ ] [ ] [ 10] mvn:org.apache.felix/org.apache.felix.configadmin/1.2.4 [ 6] [Active ] [ ] [ ] [ 11] mvn:org.apache.felix/org.apache.felix.fileinstall/3.0.2 [ 7] [Active ] [Created ] [ ] [ 20] mvn:org.apache.aries.blueprint/org.apache.aries.blueprint/0.2-incubating [ 8] [Active ] [Created ] [ ] [ 30] mvn:org.apache.karaf.jaas/org.apache.karaf.jaas.config/2.1.2 [ 9] [Active ] [Created ] [ ] [ 30] mvn:org.apache.karaf.admin/org.apache.karaf.admin.command/2.1.2 [ 10] [Active ] [Created ] [ ] [ 30] mvn:org.apache.karaf/org.apache.karaf.management/2.1.2 [ 11] [Active ] [Created ] [ ] [ 30] mvn:org.apache.karaf.deployer/org.apache.karaf.deployer.spring/2.1.2 [ 12] [Active ] [Created ] [ ] [ 30] mvn:org.apache.karaf.features/org.apache.karaf.features.core/2.1.2 [ 13] [Active ] [Created ] [ ] [ 30] mvn:org.apache.karaf.shell/org.apache.karaf.shell.packages/2.1.2 [ 14] [Active ] [ ] [ ] [ 30] mvn:org.apache.aries.jmx/org.apache.aries.jmx.blueprint/0.2-incubating [ 15] [Active ] [Created ] [ ] [ 30] mvn:org.apache.karaf.jaas/org.apache.karaf.jaas.modules/2.1.2 [ 16] [Active ] [Created ] [ ] [ 30] mvn:org.apache.karaf.shell/org.apache.karaf.shell.ssh/2.1.2 [ 17] [Active ] [Created ] [ ] [ 30] mvn:org.apache.karaf.features/org.apache.karaf.features.management/2.1.2 [ 18] [Active ] [Created ] [ ] [ 30] mvn:org.apache.karaf.features/org.apache.karaf.features.command/2.1.2 [ 19] [Active ] [Created ] [ ] [ 30] mvn:org.apache.karaf.shell/org.apache.karaf.shell.log/2.1.2 [ 20] [Active ] [Created ] [ ] [ 30] mvn:org.apache.karaf.admin/org.apache.karaf.admin.core/2.1.2 [ 21] [Active ] [ ] [ ] [ 30] mvn:org.apache.aries.jmx/org.apache.aries.jmx/0.2-incubating [ 22] [Active ] [Created ] [ ] [ 30] mvn:org.apache.karaf.deployer/org.apache.karaf.deployer.blueprint/2.1.2 [ 23] [Active ] [ ] [ ] [ 30] mvn:org.apache.mina/mina-core/2.0.0-RC1 [ 24] [Active ] [Created ] [ ] [ 30] mvn:org.apache.karaf.shell/org.apache.karaf.shell.dev/2.1.2 [ 25] [Active ] [Created ] [ ] [ 30] mvn:org.apache.karaf.shell/org.apache.karaf.shell.osgi/2.1.2 [ 26] [Active ] [ ] [ ] [ 30] mvn:org.apache.sshd/sshd-core/0.4.0 [ 27] [Active ] [Created ] [ ] [ 30] mvn:org.apache.karaf.shell/org.apache.karaf.shell.commands/2.1.2 [ 28] [Active ] [Created ] [ ] [ 30] mvn:org.apache.karaf.deployer/org.apache.karaf.deployer.features/2.1.2 [ 29] [Active ] [Created ] [ ] [ 30] mvn:org.apache.karaf.shell/org.apache.karaf.shell.console/2.1.2 [ 30] [Active ] [Created ] [ ] [ 30] mvn:org.apache.karaf.admin/org.apache.karaf.admin.management/2.1.2 [ 32] [Active ] [Created ] [ ] [ 30] mvn:org.apache.karaf.shell/org.apache.karaf.shell.config/2.1.2 [ 34] [Resolved ] [ ] [ ] [ 60] mvn:org.apache.geronimo.specs/geronimo-servlet_2.5_spec/1.1.2 [ 39] [Active ] [ ] [ ] [ 60] mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.jetty/6.1.25_1 As you see first comman returns short list which contains bundles installed by me (servlet api and jetty). Second list contains bundles default installed also by Karaf. Another usefull switch is -s which shows symbolic names:\nkaraf@root\u0026gt; list -s START LEVEL 100 , List Threshold: 50 ID State Blueprint Spring Level Symbolic name [ 34] [Resolved ] [ ] [ ] [ 60] org.apache.geronimo.specs.geronimo-servlet_2.5_spec (1.1.2) [ 39] [Active ] [ ] [ ] [ 60] org.apache.servicemix.bundles.jetty (6.1.25.1) Switch -t may be mixed with both -s and -l.\nList services After first impression with OSGi and bundles as modules most of us moving to using OSGi services. That\u0026rsquo;s really cool stuff and gives a lot of fun, but without helper commands we may stuck. Karaf provides command named ls which shows services exported by given bundle.\nkaraf@root\u0026gt; ls 132 Apache Karaf :: Web Console :: Admin Plugin (132) provides: ----------------------------------------------------------- osgi.service.blueprint.compname = adminPlugin felix.webconsole.label = admin objectClass = javax.servlet.Servlet service.id = 176 ---- osgi.blueprint.container.version = 2.1.2 osgi.blueprint.container.symbolicname = org.apache.karaf.webconsole.admin objectClass = org.osgi.service.blueprint.container.BlueprintContainer service.id = 178 If we would check which services are in use by our bundle we have very usefull switch -u.\nkaraf@root\u0026gt; ls -u 21 Apache Aries JMX Bundle (21) uses: ---------------------------------- service.vendor = Apache Software Foundation service.pid = org.apache.felix.cm.ConfigurationAdmin service.description = Configuration Admin Service Specification 1.2 Implementation objectClass = org.osgi.service.cm.ConfigurationAdmin service.id = 33 List packages When you work under OSGi it\u0026rsquo;s important which packages you import and export. The two commands packages:imports and packages:exports will simply show what\u0026rsquo;s comes and goes from your bundle. I\u0026rsquo;ll not show how these commands work but I have little trick for you. When you\u0026rsquo;re unable to resolve bundle because you have missing import packages and you have ClassNotFoundException type dev:dynamic-import bundleid command. This command add DynamicImport-Package: * entry to bundle manifest. After that, when you\u0026rsquo;ll resolve bundle type packages:imports and check complete list of import you missed in your headers.\nFeatures All commands shown above are strictly related to OSGi. But Karaf is a little bigger and allow you do more than OSGi execution environment. One of tools which Karaf build on top of OSGi framework is features mechanism. You may define list of things to install and add dependencies between them instead typing install command line after line. But sometimes you would like to check what given feature contains. To do that type features:info command. This command requires feature name as argument.\nkaraf@root\u0026gt; features:info webconsole Description of webconsole 2.1.2 feature ---------------------------------------------------------------- Feature has no configuration Feature depends on: webconsole-base 2.1.2 Feature contains followed bundles: mvn:org.apache.karaf.webconsole/org.apache.karaf.webconsole.admin/2.1.2 mvn:org.apache.karaf.webconsole/org.apache.karaf.webconsole.features/2.1.2 mvn:org.apache.karaf.webconsole/org.apache.karaf.webconsole.gogo/2.1.2 You may use few additional switches: -b, -d, -c. First shows bundles in feature, second bundle dependencies and last feature configuration. We have also another switch -t which shows all these informations plus tree of features and it\u0026rsquo;s bundles.\nkaraf@root\u0026gt; features:info -t webconsole Description of webconsole 2.1.99-SNAPSHOT feature ---------------------------------------------------------------- Feature has no configuration Feature depends on: webconsole-base 2.1.99-SNAPSHOT Feature contains followed bundles: mvn:org.apache.karaf.webconsole/org.apache.karaf.webconsole.admin/2.1.99-SNAPSHOT mvn:org.apache.karaf.webconsole/org.apache.karaf.webconsole.features/2.1.99-SNAPSHOT mvn:org.apache.karaf.webconsole/org.apache.karaf.webconsole.gogo/2.1.99-SNAPSHOT Feature tree webconsole 2.1.99-SNAPSHOT + mvn:org.apache.karaf.webconsole/org.apache.karaf.webconsole.admin/2.1.99-SNAPSHOT + mvn:org.apache.karaf.webconsole/org.apache.karaf.webconsole.features/2.1.99-SNAPSHOT mvn:org.apache.karaf.webconsole/org.apache.karaf.webconsole.gogo/2.1.99-SNAPSHOT webconsole-base 2.1.99-SNAPSHOT + mvn:org.apache.felix/org.apache.felix.metatype/1.0.4 + mvn:org.apache.karaf.webconsole/org.apache.karaf.webconsole.branding/2.1.99-SNAPSHOT mvn:org.apache.felix/org.apache.felix.webconsole/3.1.6 http 2.1.99-SNAPSHOT + mvn:org.ops4j.pax.web/pax-web-api/0.8.1 + mvn:org.ops4j.pax.web/pax-web-spi/0.8.1 + mvn:org.ops4j.pax.web/pax-web-runtime/0.8.1 mvn:org.ops4j.pax.web/pax-web-jetty/0.8.1 jetty [7.0,8.0) * Tree contains 1 unresolved dependencies * means that node declares dependency but the dependant feature is not available. Development commands I mentioned dev:dynamic-import command before. But Karaf have few more commands which makes development easier. First of all is dev:show-tree which shows bundles tree, for example:\nkaraf@root\u0026gt; dev:show-tree 39 Bundle dump [39] is currently ACTIVE dump [39] +- org.apache.karaf.diagnostic.core [16] +- org.apache.aries.blueprint [7] +- org.apache.felix.configadmin [5] | +- org.ops4j.pax.logging.pax-logging-api [3] +- org.ops4j.pax.logging.pax-logging-api [3] Another command you may use is dev:framework which allows you change OSGi framework used by Karaf. I don\u0026rsquo;t use this command to often.\nLast command I would introduce is dev:create-dump commited to Karaf trunk by me. This command creates zip archive which contains diagnostic stuff you may attach to JIRA or send to developers in your company to check what was wrong. By default dumps contains log entries from $KARAF_BASE/data/log, list of installed bundles and features. You may also create new diagnostic providers. Sample code is available in SVN: demo, provider class, blueprint config.\nComplete Remember that every Karaf command can be executed with \u0026ndash;help switch which shows all arguments and switches. In this post you was introduced to following commands:\nlist and switches -t -l -s ls and -u switch packages:imports, packages:exports, dev:dynamic-import features:info and switches -b -d -c and -t dev:show-tree, dev:framework, dev:create-dump ","permalink":"https://dywicki.pl/2010/12/apache-karaf-commands-rediscovered/","summary":"I work with Apache Karaf almost every day. There is a lot of commands provided by default and most of them are a bit anonymous. In this post I would like introduce these commands.","title":"Apache Karaf commands rediscovered"},{"content":"W tym wpisie zostanie omówiony proces OSGi-fikacji artefaktów, który przechodziłem gdy uruchamiałem prostą usługę na ServiceMix, która miała śledzić zewnętrzny RSS i pobierać z niego wpisy. Postanowiłem skorzystać z camel-rss. Przykłady które były do niego załączone są wystarczające by stworzyć odpowiedniego konsumenta\u0026hellip;\nProblem zaczął się gdy usiłowałem uruchomić endpoint camela w OSGi. Mimo poprawnej konfiguracji, rozwiązanych zależności otrzymywałem wyjątek: [code]java.lang.NoClassDefFoundError: Could not initialize class com.sun.syndication.feed.synd.SyndFeedImpl at com.sun.syndication.io.SyndFeedInput.build(SyndFeedInput.java:123) at org.apache.camel.component.rss.RssUtils.createFeed(RssUtils.java:34) at org.apache.camel.component.rss.RssEntryPollingConsumer.createFeed(RssEntryPollingConsumer.java:54) at org.apache.camel.component.feed.FeedEntryPollingConsumer.poll(FeedEntryPollingConsumer.java:42) at org.apache.camel.impl.ScheduledPollConsumer.run(ScheduledPollConsumer.java:106) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441) at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317) at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:619) [/code] Naturalnie, strasznie zirytowany, wziąłem się za dochodzenie - początkowo byłem przekonany że brakuje importów w camel-rss jednakże krótkie googlowanie wskazało rozwiązanie. Winne było kilka linii w klasie PluginManager: [code language=\u0026ldquo;java\u0026rdquo;] private Class[] getClasses() throws ClassNotFoundException { // Ten ClassLoader wskazuje na bundle w którym jest zdefiniowany endpoint! ClassLoader classLoader = Thread.currentThread().getContextClassLoader(); List classes = new ArrayList(); boolean useLoadClass = Boolean.valueOf(System.getProperty(\u0026amp;quot;rome.pluginmanager.useloadclass\u0026amp;quot;, \u0026amp;quot;false\u0026amp;quot;)).booleanValue(); for (int i = 0; i \u0026amp;lt;_propertyValues.length; i++) { // Naturalnie tutaj leciał ClassNotFoundException Class mClass = (useLoadClass ? classLoader.loadClass(_propertyValues[i]) : Class.forName(_propertyValues[i], true, classLoader)); classes.add(mClass); } Class[] array = new Class[classes.size()]; classes.toArray(array); return array; } [/code]\nPo przeróbce metoda wygląda następująco. [code language=\u0026ldquo;java\u0026rdquo;] private Class[] getClasses() throws ClassNotFoundException { ClassLoader classLoader = Thread.currentThread().getContextClassLoader(); List classes = new ArrayList(); boolean useLoadClass = Boolean.valueOf(System.getProperty(\u0026amp;quot;rome.pluginmanager.useloadclass\u0026amp;quot;, \u0026amp;quot;false\u0026amp;quot;)).booleanValue(); for (int i = 0; i \u0026amp;lt;_propertyValues.length; i++) { Class mClass = null; try { if (useLoadClass) { mClass = classLoader.loadClass(_propertyValues[i]); } else { mClass = Class.forName(_propertyValues[i], true, classLoader); } } catch (ClassNotFoundException e) { // Jeśli zewnętrzny class loader zgłosi wyjątek usiłujemy załadować klasę // z bieżącej paczki mClass = getClass().getClassLoader().loadClass(_propertyValues[i]); } classes.add(mClass); } Class[] array = new Class[classes.size()]; classes.toArray(array); return array; } [/code]\nNaturalnie, można się zastanawiać po co bibliotece do obsługi RSS zabiegi z ClassLoaderami. Otóż ROME wykorzystuje plik .properties do konfiguracji \u0026ldquo;pluginów\u0026rdquo; (poors man DI). W określonych miejscach możemy dodać własne klasy które obsłużą jakiś niestandardowy format. Problem w tym, że \u0026ldquo;patent\u0026rdquo; z plikiem properties świetnie sprawdza się przy płaskim classloaderze, niestety zawodzi w OSGi. Należy pamiętać o tym, że w OSGi nasz class loader ma dostęp do tego, do czego mu pozwalają importy i nie wszystko to, co widzi nasze oko w archiwum musi być dostępne dla naszego programu.\nPrzy okazji stworzenia bundle z ROME 1.0 postanowiłem również stworzyć bundle dla Apache POI 3.6. Bibliotekę tą wykorzystywałem wspólnie z Tomkiem Nurkiewiczem podczas prezentacji \u0026ldquo;Mule ESB vs ServiceMix\u0026rdquo;.\nOSGi-fikacja Bardzo swobodna definicja:\nOSGi-fikacja to proces mający na celu stworzenie działającego bundle z istniejącej już biblioteki. Wiele z projektów nie dostarczają poprawnych z punktu widzenia frameworku OSGi manifestów, czasami są budowane poprzez Ant bądź w ogóle są dostępne tylko ich binarne wersje co utrudnia analizę. Proces ten w większości przypadków sprowadza się do analizy zależności klas (importów) oraz zadeklarowanych klas (eksportów). W skrajnych wypadkach konieczna jest dodanie kodu bądź podmiana jego fragmentów tak by nie powodowały problemów po uruchomieniu. W celu zachowania porządku w publicznych repozytoriach Mavena stworzone w ten sposób archiwa są zapisywane z innym ArtifactId bądź GroupId, natomiast z zachowaniem oryginalnej wersji.\nNie aspiruję do miana człowieka który tworzy nowe pojęcia. Ten nieco przydługi wywód ma na celu jedynie przybliżenie działań które czasami są konieczne do uzyskania biblioteki działającej w OSGi.\nNa potrzeby przykładu OSGi-fikacji wybrałem log4j, jako popularną bibliotekę z małym zbiorem zależności. Wersja 1.2.12 nie zawiera poprawnego manifestu OSGi przez co nie można jej użyć pod Equinoxem czy Felixem. Zależności które ma log4j takie jak:\njavax.mail javax.swing javax.naming javax.activation javax.management com.sun.jdmk.comm Są opcjonalne, co znaczy że biblioteka bez problemów uruchomi się jeśli nie uda się zaimportować wyżej wymienionych paczek. Określimy zatem ich resolution na optional.\nPozostałe zależności wymienione poniżej są wymagane by móc odczytać plik log4j.xml:\norg.w3c.dom javax.xml.parsers org.xml.sax Poniżej znajduje się pom.xml który przygotowałem po to by zaprezentować użycie wcześniej wspomnianych pluginów. Pom ten ma skutkować stworzeniem artefaktu OSGi gotowego do uruchomienia pod Kara-fem. [code language=\u0026ldquo;xml\u0026rdquo;] 4.0.0\norg.apache.servicemix.bundles bundles-pom 4 org.apache.servicemix.bundles org.apache.servicemix.bundles.log4j 1.2.12-SNAPSHOT bundle Apache ServiceMix Bundles: ${pkgArtifactId}-${pkgVersion} This bundle simply wraps ${pkgArtifactId}-${pkgVersion}.jar. \u003c!\\-\\- Zmienne dla maven-bundle-plugin --\u003e org.apache.log4j\\*;version=${pkgVersion} \u003c!\\-\\- Zależności opcjonalne --\u003e com.sun.jdmk.comm;resolution:=optional, javax.jms;resolution:=optional, javax.mail\\*;resolution:=optional, javax.management;resolution:=optional, javax.naming;resolution:=optional, javax.swing\\*;resolution:=optional, \\\\* \u003c!\\-\\- Wszystkie inne zależności jakie doda analizator --\u003e \u003c!\\-\\- Zmienne artefaktu --\u003e log4j log4j 1.2.12 \u003c!\\-\\- zależność do log4j --\u003e ${pkgGroupId} ${pkgArtifactId} ${pkgVersion} true \u003c!\\-\\- Kopiowanie plików .class --\u003e org.apache.maven.plugins maven-shade-plugin package shade ${pkgGroupId}:${pkgArtifactId} ${pkgGroupId}:${pkgArtifactId} \\*\\* true true \\[/code\\] Teraz pora na instalację bundla na szynie:\n____ _ __ __ _ / ___| ___ _ ____ _(_) ___ ___| / (_)_ __ ___ / _ \u0026#39;__ / / |/ __/ _ |/| | / / ___) | __/ | V /| | (_| __/ | | | |\u0026gt; \u0026lt; |____/ ___|_| _/ |_|______|_| |_|_/_/_ Apache ServiceMix (4.2.0-fuse-01-00) Hit \u0026#39;\u0026lt;tab\u0026gt;\u0026#39; for a list of available commands and \u0026#39;[cmd] --help\u0026#39; for help on a specific command. karaf@root\u0026gt; karaf@root\u0026gt; install mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.log4j/1.2.12-SNAPSHOT Bundle ID: 186 karaf@root\u0026gt; list|grep log4j [ 186] [Resolved ] [ ] [ ] [ 60] Apache ServiceMix Bundles: log4j-1.2.12 (1.2.12.SNAPSHOT) karaf@root\u0026gt; start 186 karaf@root\u0026gt; list|grep log4j [ 186] [Active ] [ ] [ ] [ 60] Apache ServiceMix Bundles: log4j-1.2.12 (1.2.12.SNAPSHOT) karaf@root\u0026gt; packages:imports 186 OSGi System Bundle (0): javax.management; version=\u0026#34;0.0.0\u0026#34; OSGi System Bundle (0): javax.naming; version=\u0026#34;0.0.0\u0026#34; OSGi System Bundle (0): javax.swing; version=\u0026#34;0.0.0\u0026#34; OSGi System Bundle (0): javax.swing.border; version=\u0026#34;0.0.0\u0026#34; OSGi System Bundle (0): javax.swing.event; version=\u0026#34;0.0.0\u0026#34; OSGi System Bundle (0): javax.swing.table; version=\u0026#34;0.0.0\u0026#34; OSGi System Bundle (0): javax.swing.text; version=\u0026#34;0.0.0\u0026#34; OSGi System Bundle (0): javax.swing.tree; version=\u0026#34;0.0.0\u0026#34; OSGi System Bundle (0): javax.xml.parsers; version=\u0026#34;0.0.0\u0026#34; OSGi System Bundle (0): org.w3c.dom; version=\u0026#34;0.0.0\u0026#34; OSGi System Bundle (0): org.xml.sax; version=\u0026#34;0.0.0\u0026#34; OSGi System Bundle (0): org.xml.sax.helpers; version=\u0026#34;0.0.0\u0026#34; OPS4J Pax Logging - API (3): org.apache.log4j.spi; version=\u0026#34;1.2.15\u0026#34; OPS4J Pax Logging - API (3): org.apache.log4j.xml; version=\u0026#34;1.2.15\u0026#34; OPS4J Pax Logging - API (3): org.apache.log4j; version=\u0026#34;1.2.15\u0026#34; geronimo-jms_1.1_spec (64): javax.jms; version=\u0026#34;1.1.0\u0026#34; Apache ServiceMix Bundles: mail-1.4.1 (86): javax.mail.internet; version=\u0026#34;1.4.1\u0026#34; Apache ServiceMix Bundles: mail-1.4.1 (86): javax.mail; version=\u0026#34;1.4.1\u0026#34; karaf@root\u0026gt; packages:exports 186 Apache ServiceMix Bundles: log4j-1.2.12 (186): org.apache.log4j.lf5.util; version=\u0026#34;1.2.12\u0026#34; Apache ServiceMix Bundles: log4j-1.2.12 (186): org.apache.log4j.net; version=\u0026#34;1.2.12\u0026#34; Apache ServiceMix Bundles: log4j-1.2.12 (186): org.apache.log4j.lf5.viewer; version=\u0026#34;1.2.12\u0026#34; Apache ServiceMix Bundles: log4j-1.2.12 (186): org.apache.log4j.jmx; version=\u0026#34;1.2.12\u0026#34; Apache ServiceMix Bundles: log4j-1.2.12 (186): org.apache.log4j.jdbc; version=\u0026#34;1.2.12\u0026#34; Apache ServiceMix Bundles: log4j-1.2.12 (186): org.apache.log4j.config; version=\u0026#34;1.2.12\u0026#34; Apache ServiceMix Bundles: log4j-1.2.12 (186): org.apache.log4j.helpers; version=\u0026#34;1.2.12\u0026#34; Apache ServiceMix Bundles: log4j-1.2.12 (186): org.apache.log4j.lf5.config; version=\u0026#34;1.2.12\u0026#34; Apache ServiceMix Bundles: log4j-1.2.12 (186): org.apache.log4j.or.jms; version=\u0026#34;1.2.12\u0026#34; Apache ServiceMix Bundles: log4j-1.2.12 (186): org.apache.log4j.nt; version=\u0026#34;1.2.12\u0026#34; Apache ServiceMix Bundles: log4j-1.2.12 (186): org.apache.log4j.or.sax; version=\u0026#34;1.2.12\u0026#34; Apache ServiceMix Bundles: log4j-1.2.12 (186): org.apache.log4j.lf5.viewer.categoryexplorer; version=\u0026#34;1.2.12\u0026#34; Apache ServiceMix Bundles: log4j-1.2.12 (186): org.apache.log4j.lf5; version=\u0026#34;1.2.12\u0026#34; Apache ServiceMix Bundles: log4j-1.2.12 (186): org.apache.log4j.or; version=\u0026#34;1.2.12\u0026#34; Apache ServiceMix Bundles: log4j-1.2.12 (186): org.apache.log4j.chainsaw; version=\u0026#34;1.2.12\u0026#34; Apache ServiceMix Bundles: log4j-1.2.12 (186): org.apache.log4j.varia; version=\u0026#34;1.2.12\u0026#34; Apache ServiceMix Bundles: log4j-1.2.12 (186): org.apache.log4j.lf5.viewer.configure; version=\u0026#34;1.2.12\u0026#34; Apache ServiceMix Bundles: log4j-1.2.12 (186): org.apache.log4j.lf5.viewer.images; version=\u0026#34;1.2.12\u0026#34; karaf@root\u0026gt; Jak widać wszystko działa i ma się dobrze. :) Jedyna uwaga jaką mam to by nie używać tak spreparowanego log4j. W specyfikacji OSGi R4 V4.2 Enterprise jest opisana usługa logująca. Oprócz niej jest jeszcze Pax Logging (używany przez Karafa) wspierająca kilka różnych bibliotek od log4j poprzez slf4j po wspomnianą usługę OSGi.\nPozdrawiam i życzę miłej OSGi-fikacji! :)\n","permalink":"https://dywicki.pl/2010/03/osgi-new-bundles-servicemix-repository/","summary":"W tym wpisie zostanie omówiony proces OSGi-fikacji artefaktów, który przechodziłem gdy uruchamiałem prostą usługę na ServiceMix, która miała śledzić zewnętrzny RSS i pobierać z niego wpisy. Postanowiłem skorzystać z camel-rss. Przykłady które były do niego załączone są wystarczające by stworzyć odpowiedniego konsumenta\u0026hellip;","title":"OSGi-fikacja oraz nowe bundle w repozytorium ServiceMix"},{"content":"Do opublikowania tego postu zachęcił mnie Jacek Laskowski swym postem pod tytułem W piątek 4Developers ze mną z Enterprise OSGi i in.\nBardzo się cieszę że na 4Developers (na którym niestety mnie nie będzie) temat Enterprise OSGi będzie poruszony, ponieważ jak się zdaje jest to nieuchronny kierunek rozwoju Javy. Pod wpływem słów Jacka zacząłem się zastanawiać nad długofalowymi efektami jakie OSGi ma wnieść do developmentu.\nHałas który obecnie jest wokół OSGi w przybiera konkretne kształty w postaci projektów takich jak Aries czy Gemini. Obydwa projekty skupiają się nad ostatnimi draftami OSGi R4 V4.2 i mają na celu udostępnienie technologii takich jak JNDI, JPA i JMX wewnątrz kontenerów OSGi. Zacznijmy jednak od początku..\nOSGi a jarhell JAR-hell occurs when software is deployed into a runtime environment which is unsuitable, but nothing other than full integration testing would detect this. Having multiple software packages dependent upon the same piece of software, with unpredictable incompatibilities, is pure hell. Ensuring the compatibility of a variety of dependent packages is duanting, doing it amongst the variety supported by a hierarchy of complex class loaders, is inhuman.\nŹródło Apache Depot\nCzyli w skrócie - piekło zaczyna się robić gdy pojawiają się niekompatybilności między bibliotekami w poprawnym środowisku. Co więcej owe niekompatybilności można wykryć dopiero po dogłębnych testach we wszystkich środowiskach w których ma działać aplikacja.\nW przypadku OSGi wszystkie zależności są przewidywalne, co więcej nie uda się nam uruchomić paczki bez jej zależności - stąd teoretycznie nigdy nie powinniśmy widzieć ClassNotFoundException. Nie uda nam się również uruchomić naszego bundle jeśli powstanie konflikt w używanych zależnościach. Przykład z życia wzięty - mamy bundle zależące od camel-activemq oraz activemq-core. Pierwszy z nich pozwala na import spring-jms w wersji \u0026lt; 4.0, natomiast drugi w wersji \u0026lt; 2.6. Jeśli do tego mamy dwie wersje bundle spring-jms: 2.5.6 oraz 3.0.0 to mamy klapę. Naszej paczki nie da się wystartować ponieważ otrzymamy \u0026ldquo;Packages usage conflict\u0026rdquo;. Zostaliśmy ochronieni przed JAR Hellem kosztem zablokowania kodu nawet jeśli zależność była opcjonalna.\nRozwiązanie zagadki packages usage. Problem wydaje się trywialny - teoretycznie to są dwie różne wersje Springa, nie da się zaprzeczyć że 2.5.6 != 3.0.0. W praktyce jednak zmiany w spring-jms były tak niewielkie że można bez problemu uruchomić activemq-core z nową wersją. W takim wypadku jesteśmy zmuszeni do czekania na nową wersję ActiveMQ, która będzie pozwalała na korzystanie ze Springa 3.0 bądź samodzielnie zmodyfikować manifesty. Obydwa rozwiązania są równie złe - jedno to czekanie, drugie to tworzenie nowej dystrybucji ActiveMQ.\nCo w takim wypadku możemy zrobić? Możemy użyć serwisów OSGi, które pozwalają na oddzielenie implementacji od interfejsu, dzięki czemu możemy połączyć dwie wersje bibliotek za fasadą w postaci ServiceReference. Tutaj jednak może pojawić się inny problem - mianowicie część bibliotek które lubią dostęp do ClassLoaderów może skutecznie protestować - na przykład Hibernate czy Open JPA. Dla przykładu diagram obrazujący kolejny z życia wzięty przypadek:\nW tym przypadku usiłowałem stworzyć działającą usługę która zapisywała by przychodzące komunikaty w bazie danych. Może parę słów o tym, który bundle co robi:\ndatasource otwiera połączenie do bazy danych, tworzy ConnectionFactory dla JMS a także EntityManagera. binding bramka do przyjmowania komunikatów - w tym przypadku był to web service. engine definicja routingu z użyciem Camela. persistence użycie EntityManagera do zapisywania komunikatów domain POJO, klasy domenowe Całość komunikacji odbywała się z JMS w trybie request-reply dzięki Apache Camel. Po bardzo długich \u0026ldquo;walkach\u0026rdquo; poniższą strukturę udało się uruchomić pod OSGi. Ostatecznie całość działa z Hibernate w układzie takim jak poniżej.\nZ diagramu wyrzuciłem paczki które nie są istotne takie jak driver JDBC czy Commons ConnectionPool. Jedyny mankament na jaki trafiłem wiąże się z DAO Service, mianowicie bundle który go eksportuje poprzez Spring-DM musi zadeklarować widoczność wszystkich swoich klas dla paczki która będzie korzystać z usługi co jak by nie patrzeć jest drobnym wypaczeniem fasady jaką ma być ServiceReference. Niestety po 2 tygodniach poświęconych na uruchomienie JPA w OSGi nie siliłem się na elegancję.\n[code language=\u0026ldquo;xml\u0026rdquo;]\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u0026lt;osgi:reference id=\u0026ldquo;dataSource\u0026rdquo; interface=\u0026ldquo;javax.sql.DataSource\u0026rdquo; /\u0026gt;\n\u0026lt;osgi:service id=\u0026ldquo;exchangeDAOExporter\u0026rdquo; ref=\u0026ldquo;exchangeDAO\u0026rdquo; context-class-loader=\u0026ldquo;service-provider\u0026rdquo; interface=\u0026ldquo;org.code_house.dataaccess.ExchangeDAO\u0026rdquo; /\u0026gt; [/code]\nKolejne metadane Problem jaki powstaje z OSGi to metadane. Do tej pory - jeśli zarządzałem zależnościami do bibliotek robiłem to przez dependencyManagement Mavena, dzięki któremu rozwiązywałem wszystkie konflikty. OSGi jednak nie wiąże się z Mavenem ponieważ są to dwa różne obszary o zgoła innym funkcjonowaniu - OSGi to runtime, Maven to build time. Dopóki nie zapanuje harmonia pomiędzy tymi dwoma uruchamianie czegokolwiek w OSGi będzie katorgą. Należy do tego dodać jeszcze jeden element związany z OSGi - mianowicie O SGi B undle R epository (OBR), a OBR nijak się ma do repozytoriów Mavena przez co rozbieżności tylko się nasilają. Aby temu zapobiegać najpopularniejsze istniejące repozytoria SpringSource Enterprise Bundle Repository oraz ServiceMix 4 Bundles repository - publikują artefakty w repozytoriach Mavena. Problem w tym, że część artefaktów jest powielona. Tak jak kiedyś były 3 wersje Java Persistence API tak teraz dochodzą kolejne dwie - z manifestami OSGi. Czy ktoś wspominał o piekle? Należy również dodać że nie każdy JAR który ląduje w OSGi jest traktowany tak samo - oprócz standardowych bundli są również fragmenty, które są świetnym rozwiązaniem, jednakże początkowo potrafią przysporzyć wielu problemów. Cały trik sprowadza się do tego, że fragmenty mają wspólny class loader z paczką do której są przypięte.\nKorzyści z Enterprise OSGi Po całych tych wywodach na temat problemów z OSGi pora na to co ma ono nam dać - przenośność. Podobnie jak Java EE 6 z profilami tak OSGi ma zapewnić większą przenośność klocków pomiędzy środowiskami. Wyobrażacie sobie, że można przenieść aplikację z kontenera servletów na serwer aplikacyjny bez modyfikacji? Albo usługę z szyny integracyjnej na kontener servletów? Niedorzeczne, ale z OSGi możliwe do wykonania. Wystarczy zainstalować wszystkie wymagane bundle i całość będzie działać.\nOczywiście naiwna była by wiara w to, że tak będzie. Każda specyfikacja która powstaje dla Javy ma standaryzować i ujednolicać środowiska. W praktyce jednak każda z nich staje się punktem wyjściowym do rozwoju nowych produktów. Każdy dostawca oferuje zgodność ze specyfikacją plus coś. Nie twierdzę, że to złe, ponieważ polaryzacja rynku oprogramowania jest tak samo potrzebna jak wolny rynek, należy się jednak wystrzegać monopolistów a w przypadku oprogramowania również vendor locków.\nPatrzę na OSGi z nadzieją ponieważ w moim mniemaniu jest to przedłużenie idei jaką niosła specyfikacja J ava B usiness I ntegration. Propozycja JBI 2.0 spotkała się z krytyką ze strony IBM oraz BEA Systems zasłaniających się tym, że istnieje SCA. Problem w tym, że obydwie specyfikacje traktują o innych warstwach integracji - SCA gwarantuje przenośność usług, podczas gdy JBI miało zapewnić przenośność komponentów i komunikacji między nimi. SCA i JBI mogą a nawet powinny iść w parze. Teoria JBI mówiła o możliwości uruchomienia servicemix-cxf-bc (zgodnego z JBI) na Open ESB, w praktyce okazywało się to jednak bardzo trudne. Dzięki OSGi/Enterprise OSGi stanie się to łatwiejsze.\nW ciągu kilku najbliższych lat trend OSGi dzięki zainteresowaniu wielkich korporacji będzie rósł w siłę. Trzymam kciuki by Enterprise OSGI przyniosło więcej korzyści niż tylko zbawienie od JAR Hella.\n","permalink":"https://dywicki.pl/2010/03/enterprise-osgi/","summary":"\u003cp\u003eDo opublikowania tego postu zachęcił mnie \u003ca href=\"http://jlaskowski.blogspot.com/\"\u003eJacek Laskowski\u003c/a\u003e swym postem pod tytułem \u003ca href=\"http://jlaskowski.blogspot.com/2010/03/w-piatek-4developers-ze-mna-z.html\"\u003eW piątek 4Developers ze mną z Enterprise OSGi i in\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eBardzo się cieszę że na 4Developers (na którym niestety mnie nie będzie) temat Enterprise OSGi będzie poruszony, ponieważ jak się zdaje jest to nieuchronny kierunek rozwoju Javy. Pod wpływem słów Jacka zacząłem się zastanawiać nad długofalowymi efektami jakie OSGi ma wnieść do developmentu.\u003c/p\u003e\n\u003cp\u003eHałas który obecnie jest wokół OSGi w przybiera konkretne kształty w postaci projektów takich jak \u003ca href=\"http://incubator.apache.org/aries/\"\u003eAries\u003c/a\u003e czy \u003ca href=\"http://www.eclipse.org/gemini/\"\u003eGemini\u003c/a\u003e. Obydwa projekty skupiają się nad ostatnimi draftami OSGi R4 V4.2 i mają na celu udostępnienie technologii takich jak JNDI, JPA i JMX wewnątrz kontenerów OSGi. Zacznijmy jednak od początku..\u003c/p\u003e","title":"Enterprise OSGi"},{"content":"Dnia 23 lutego w ramach Warszawa JUG miałem przyjemność wraz z Tomkiem Nurkiewiczem prezentować narzędzia integracyjne z otwartym kodem źródłowym. Tomek przedstawił Mule ESB, podczas gdy ja zająłem się Apache ServiceMix i Apache Camel. Ze względu na objętość przykładu ten wpis będzie jedynie wprowadzeniem do konsoli.\nNiestety podczas prezentacji nie udało mi się uruchomić przykładu na \u0026ldquo;szynie\u0026rdquo; - ponieważ uniemożliwiły to zależności do bibliotek których nie miałem zapisanych lokalnie. Drugim mym przeciwnikiem był czas - nie było wielu chętnych by słuchać po 2h tłumaczeń dlaczego się nie udało :-) Na problem z zależnościami stworzyłem rozwiązanie i zgłosiłem je do Karaf-a ( FELIX 2141). W przyszłej wersji - 1.6 - wszyscy będą mogli skorzystać z polecenia features:info -t które wyświetli całe drzewko zależności potrzebnych do zainstalowania nowych funkcjonalności.\nPrzygotowanie środowiska Do uruchomienia przykładów potrzebować będziemy dwóch paczek - pierwsza to Apache ServiceMix, druga to Apache Maven. Ze swojej strony polecam pobranie FUSE ESB 4.2, produktu który jest oparty o ServiceMix. Zgodnie z moimi zapowiedziami od tej wersji FUSE Source wprowadza pełne wsparcie produkcyjne dla ServiceMix 4. Jeśli nie masz zainstalowanego Mavena i nigdy tego nie robiłeś zajrzyj na wiki Code-House: Wprowadzenie do Maven 2\nPo pobraniu odpowiedniej wersji należy ją rozpakować do wybranego folderu. Szynę uruchamiamy skryptem servicemix.bat. Jeśli wszystko przebiegło poprawnie naszym oczom powinien ukazać się obrazek jak poniżej:\nE:toolsprogressfuse-esb4.2.0-bbin\u0026gt;servicemix.bat ____ _ __ __ _ / ___| ___ _ ____ _(_) ___ ___| / (_)_ __ ___ / _ \u0026#39;__ / / |/ __/ _ |/| | / / ___) | __/ | V /| | (_| __/ | | | |\u0026gt; \u0026lt; |____/ ___|_| _/ |_|______|_| |_|_/_/_ Apache ServiceMix (4.2.0-fuse-01-00) Hit \u0026#39;\u0026#39; for a list of available commands and \u0026#39;[cmd] --help\u0026#39; for help on a specific command. karaf@root\u0026gt; Od tej chwili mamy do dyspozycji konsolę administracyjną. Aby zwiększyć jej użyteczność wykonujemy następujące polecenia:\nkaraf@root\u0026gt; osgi:install wrap:mvn:http://download.java.net/maven/2!net.java.dev.jna/jna/3.1.0 Bundle ID: 134 Spowoduje to pobranie z repozytorium Mavena (http://download.java.net/maven/2) biblioteki JNA w wersji 3.1.0. Nie musimy oczywiście określać za każdym razem adresu repozytoriów, ale o tym nieco później. Prefix wrap: spowoduje wygenerowanie manifestu OSGi na podstawie zawartości pobranej biblioteki. Jest on konieczny ponieważ JNA nie dostarcza potrzebnych danych do uruchomienia w środowisku OSGi.\nkaraf@root\u0026gt; osgi:install mvn:http://jansi.fusesource.org/repo/release!org.fusesource.jansi/jansi/1.2 Bundle ID: 135 Druga biblioteka wykorzystuje JNA i umożliwia kolorowanie tekstu w konsoli windowsowej zgodnie z ANSI, czyli tak jak w standardowych terminalach Unix-a. Po zainstalowaniu tych dwóch rzeczy pora zaprząc je do pracy. Poniżej filtrujemy listę zainstalowanych rzeczy po słowie Console.\nkaraf@root\u0026gt; list|grep Console [ 10] [Active ] [Created ] [ ] [ 30] Apache Felix Karaf :: Shell Console (1.4.0.fuse-01-00) karaf@root\u0026gt; refresh 10 Numer 10 oznacza id paczki OSGi, tekst Active to stan paczki a tekst Created informuje o stanie kontekstu blueprint, czwarty bloczek to stan kontekstu Springa, numer 30 to start level paczki a w ostatnim nawiasie mamy wersję. Sporo informacji jak na jedną linijkę, nieprawdaż? Blueprint to standaryzowanie tego czym jest Spring i Spring DM, także można korzystać zamiennie bądź z jednego bądź z drugiego rozwiązania. Ciekawe porównanie funkcjonalności obu rozwiązań - Spring DM oraz Blueprint opublikował kilka dni temu Guillaume Nodet na swoim blogu we wpisie Spring-DM, Aries Blueprint and custom namespaces. Apache Camel od wersji 2.3 będzie wspierał Blueprint ( CAMEL-2022).\n____ _ __ __ _ / ___| ___ _ ____ _(_) ___ ___| / (_)_ __ ___ / _ \u0026#39;__ / / |/ __/ _ |/| | / / ___) | __/ | V /| | (_| __/ | | | |\u0026gt; \u0026lt; |____/ ___|_| _/ |_|______|_| |_|_/_/_ Apache ServiceMix (4.2.0-fuse-01-00) Hit \u0026#39;\u0026lt;tab\u0026gt;\u0026#39; for a list of available commands and \u0026#39;[cmd] --help\u0026#39; for help on a specific command. karaf@root\u0026gt; Teraz wykonanie poleceń z filtrem grep spowoduje podświetlenie szukanej frazy:\nkaraf@root\u0026gt; list |grep Console [ 10] [Active ] [Created ] [ ] [ 30] Apache Felix Karaf :: Shell Console (1.4.0.fuse-01-00) Takie małe udogodnienie przy przeglądaniu dłuższych rezultatów jest nieocenione.\nPodstawowe polecenia konsoli Polecenia w Apache Karaf są podzielone na kilka grup, które ułatwiają zarządzanie. Pierwszą grupą jest OSGi.\nPolecenia OSGi PoleceniePrzeznaczenielistWyświetlenie listy zainstalowanych paczekls [bundle id]Wyświetlenie usług eksportowanych przez paczkę.ls -u [bundle id]Wyświetlenie usług używanych przez paczkę.headers [bundle id]Wyświetlenie manifestu paczki.start [bundle id]Uruchomienie paczki o danym ID.stop [bundle id]Zatrzymanie paczki o danym ID.restart [bundle id]Zatrzymanie i wystartowanie paczki o danym ID.update [bundle id]Aktualizacja paczki.refresh [bundle id]Odświeżenie importów paczki a także przeładowanie kontekstu Spring-DM.install [url]Zainstalowanie nowej paczki.uninstall [bundle id]Odinstalowanie paczki.shutdownZatrzymanie kontenera.bundle-level [bundle id] [startLevel]Ustawienie start levelu dla paczki.start-level\nKiedy znamy już listę poleceń nie pozostaje nic innego jak je wypróbować. :-) Checkout przykładowego kodu z SVN pozwoli wykonać nam kilka ćwiczeń. Po wykonaniu polecenia mvn clean install w repozytorium Mavena są JARy które, naturalnie, chcemy zainstalować. Rezultat jaki powinniśmy zobaczyć w konsoli to:\n[INFO] ------------------------------------------------------------------------ [INFO] Reactor Summary: [INFO] ------------------------------------------------------------------------ [INFO] Money transfer ServiceMix example ..................... SUCCESS [2.028s] [INFO] Money transfer :: API ................................. SUCCESS [3.182s] [INFO] Money transfer :: POI bundle .......................... SUCCESS [7.504s] [INFO] Money transfer :: Internal ............................ SUCCESS [0.093s] [INFO] Money transfer :: Internal :: CSV ..................... SUCCESS [4.399s] [INFO] Money transfer :: Internal :: XLS ..................... SUCCESS [2.823s] [INFO] Money transfer :: Internal :: Mail .................... SUCCESS [2.309s] [INFO] Money transfer :: Internal :: Splitter ................ SUCCESS [0.936s] [INFO] Money transfer :: Internal :: Routes .................. SUCCESS [1:21.241s] [INFO] Money transfer :: External ............................ SUCCESS [0.016s] [INFO] Money transfer :: External :: Customer ................ SUCCESS [3.182s] [INFO] Money transfer :: External :: Bank .................... SUCCESS [2.059s] [INFO] Money transfer :: External :: Validator ............... SUCCESS [2.918s] [INFO] Money transfer :: Features ............................ SUCCESS [0.171s] [INFO] ------------------------------------------------------------------------ [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESSFUL [INFO] ------------------------------------------------------------------------ [INFO] Total time: 1 minute 54 seconds [INFO] Finished at: Mon Mar 22 10:47:27 CET 2010 [INFO] Final Memory: 127M/341M [INFO] ------------------------------------------------------------------------ Może teraz kilka słów o modułach które są widoczne w Mavenie. ModułŚcieżka\nMaven IDPrzeznaczenieMoney transfer :: APIapi\norg.code-house.samples/apiAPI systemów zewnętrznych oraz POJO wykorzystywane do komunikacji.Money transfer :: POI bundlepoi\norg.code-house.samples/poiBundle zawierający bibliotekę Apache POI oraz wszystkie jej zależności z Manifestem OSGi pozwalającym na jej uruchomienie na szynie.Money transfer :: Internal :: CSVinternal/csv\norg.code-house.samples.internal/csvPaczka zawierająca implementację procesora przetwarzającego pliki CSV na POJO MoneyTransfer.Money transfer :: Internal :: XLSinternal/xls\norg.code-house.samples.internal/xlsPaczka zawierająca implementację procesora przetwarzającego pliki XLS na POJO MoneyTransfer.Money transfer :: Internal :: Mailinternal/mail\norg.code-house.samples.internal/mailPaczka z kodem dzielącym przychodzący mail z załącznikami na pojedyncze wiadomości które można przetworzyć jako XLS bądź CSV.Money transfer :: Internal :: Splitterinternal/splitter\norg.code-house.samples.internal/splitterPaczka z kodem odpowiedzialnym za rozdzielanie listy obiektów MoneyTransfer na listę wiadomości.Money transfer :: Internal :: Routesinternal/routes\norg.code-house.samples.internal/routesGłówna paczka z definicjami routinguMoney transfer :: External :: Customerexternal/customer\norg.code-house.samples.external/customerImplementacja WebService odpowiedzialnego za pobieranie danych klienta na podstawie numeru rachunku bankowego.Money transfer :: External :: Bankexternal/bank\norg.code-house.samples.external/bankImplementacja usługi zwracającej informację o nazwie banku na podstawie numeru rachunku.Money transfer :: External :: Validatorexternal/validator\norg.code-house.samples.external/validatorUsługa weryfikująca czy MoneyTransfer jest poprawny.Money transfer :: Featuresfeatures\norg.code-house.samples/featuresModuł zawierający opcjonalny deskryptor do uruchomienia modułów.\nJak widać dwa główne obszary projektu są skupione w katalogach internal oraz external. Ten pierwszy zawiera implementacje ściśle związaną z usługami natomiast drugi to \u0026ldquo;zatyczki\u0026rdquo; emulujące działanie systemów zewnętrznych. Drobna nota - kolumna Maven ID nie zawiera informacji o wersji - w każdym module jest to 1.0.0.SNAPSHOT.\nAby zainstalować któryś bundle przechodzimy do konsoli ServiceMix\u0026rsquo;a i wykonujemy takie polecenia:\nkaraf@root\u0026gt; install -s mvn:org.code-house.samples/api/1.0.0.SNAPSHOT Bundle ID: 210 karaf@root\u0026gt; features:install camel-jetty karaf@root\u0026gt; install -s mvn:org.code-house.samples.external/customer/1.0.0.SNAPSHOT Bundle ID: 211 Po wykonaniu tych poleceń powinien być uruchomiony Web Service którego WSDL znajduje się pod adresem http://localhost:9001/CustomerWs?wsdl. Przełącznik -s w przypadku polecenia install powoduje że po zainstalowaniu bundle zostanie wystartowany. Polecenie features-install camel-jetty jest potrzebne nie ze względu na to że kolejna paczka korzysta z Camela, dzięki jego wykonaniu zostanie zainstalowane Jetty, z którego korzysta CXF.\nKolejne paczki instalujemy analogicznie:\nkaraf@root\u0026gt; features:install camel-activemq karaf@root\u0026gt; install -s mvn:org.code-house.samples.external/validator/1.0.0.SNAPSHOT Bundle ID: 215 karaf@root\u0026gt; install -s mvn:org.code-house.samples.external/bank/1.0.0.SNAPSHOT Bundle ID: 217 Rozszerzenie camel-activemq jest rozszerzeniem modułu camel-jms które pozwala na nieco wydajniejszą pracę z ActiveMQ.\nIntegracja ServiceMix z repozytoriami Mavena Maven jako narzędzie do budowania korzysta z określonego schematu składowania bibliotek które następnie są automatycznie pobierane. Karaf, który jak wspomniałem podczas prezentacji, wyłonił się z projektu ServiceMix Kernel korzysta z biblioteki Pax URL. Dzięki temu z marszu mamy dostęp do standardowych repozytoriów Mavena, co jednak gdy mamy swoje repozytorium, które zawiera tylko nasze artefakty? Otwieramy plik etc/org.ops4j.pax.url.mvn.cfg i dodajemy w nim co trzeba. Moja standardowa konfiguracja wygląda następująco: [code] org.ops4j.pax.url.mvn.settings=E:/tools/maven-2.2.1/conf/settings.xml org.ops4j.pax.url.mvn.localRepository=E:/repository org.ops4j.pax.url.mvn.defaultRepositories=file:${karaf.home}/${karaf.default.repository}@snapshots org.ops4j.pax.url.mvn.repositories= http://repo1.maven.org/maven2, http://repo.fusesource.com/maven2, http://repo.fusesource.com/maven2-snapshot@snapshots@noreleases, http://repository.apache.org/content/groups/snapshots-group@snapshots@noreleases, http://repository.ops4j.org/maven2, http://svn.apache.org/repos/asf/servicemix/m2-repo, http://repository.springsource.com/maven/bundles/release, http://repository.springsource.com/maven/bundles/external, http://repository.code-house.org/content/groups/release, http://repository.code-house.org/content/groups/snapshot@snapshots@noreleases, http://jansi.fusesource.org/repo/release [/code] Dzięki temu PAX w pierwszej kolejności będzie skanował katalog E:/repository zamiast standardowego ~/.m2/repository. Jeśli któreś z repozytoriów wymaga autoryzacji adres powinien wyglądać następująco: [code]http://user:pass@jansi.fusesource.org/repo/release[/code]\nPodsumowanie Mam nadzieję że wpis ten przybliży chociaż w niewielkim stopniu ServiceMix 4 oraz Karafa. W przyszłym wpisie, którego daty publikacji nie sposób przewidzieć zostaną dokładniej omówione polecenia z grupy features. Póki co życzę miłej zabawy z konsolą. :-) W razie pytań, niejasności i problemów - proszę o komentarze.\n","permalink":"https://dywicki.pl/2010/03/introduction-to-apache-servicemix4-part-1/","summary":"\u003cp\u003eDnia 23 lutego w ramach Warszawa JUG miałem przyjemność wraz z \u003ca href=\"http://nurkiewicz.blogspot.com/\"\u003eTomkiem Nurkiewiczem\u003c/a\u003e prezentować narzędzia integracyjne z otwartym kodem źródłowym. Tomek przedstawił Mule ESB, podczas gdy ja zająłem się Apache ServiceMix i Apache Camel. Ze względu na objętość przykładu ten wpis będzie jedynie wprowadzeniem do konsoli.\u003c/p\u003e\n\u003cp\u003eNiestety podczas prezentacji nie udało mi się uruchomić przykładu na \u0026ldquo;szynie\u0026rdquo; - ponieważ uniemożliwiły to zależności do bibliotek których nie miałem zapisanych lokalnie. Drugim mym przeciwnikiem był czas - nie było wielu chętnych by słuchać po 2h tłumaczeń dlaczego się nie udało :-) Na problem z zależnościami stworzyłem rozwiązanie i zgłosiłem je do Karaf-a ( \u003ca href=\"https://issues.apache.org/jira/browse/FELIX-2141\"\u003eFELIX 2141\u003c/a\u003e). W przyszłej wersji - 1.6 - wszyscy będą mogli skorzystać z polecenia \u003ccode\u003efeatures:info -t\u003c/code\u003e które wyświetli całe drzewko zależności potrzebnych do zainstalowania nowych funkcjonalności.\u003c/p\u003e","title":"Wprowadzenie do Apache ServiceMix 4 cz. 1"},{"content":"W nawiązaniu do poprzedniej noty o CXFie, którą napisałem jakiś czas temu, gonię aby uzupełnić brak konfiguracji klienta. Sam proces jest bardzo zbliżony do tworzenia klienta w oparciu o XFire. Nie jest wymagana duża ilość kodu Javy, a w zasadzie tylko dwa pliki XML (client.xml, myservice.xml).\nPierwszy z nich odpowiada za wczytanie wymaganych rozszerzeń CXFa oraz definicję bazowej konfiguracji fabryki z interceptorami. W interceptorach możemy skonfigurować logowanie, obsługę załączników czy standardów WS-Security etc. Wszystkie te ustawienia będą dziedziczone, a fabryki docelowych usług będą dodawać tylko adres, do odpytywania. Na koniec bean klienta będzie miał określony autowire by nie przekazywać mu wszystkich własności.\nOto najważniejsze wstawki kodu oraz ich opis: [sourcecode language=\u0026ldquo;xml\u0026rdquo;]org.code-house.cxfparent1.0-SNAPSHOT4.0.0org.code-house.cxfclient1.0-SNAPSHOTCode House.Org - CXF - Clientlog4jlog4j1.2.12org.code-house.cxfcontract1.0-SNAPSHOTorg.apache.cxfcxf-rt-frontend-jaxwsorg.apache.cxfcxf-rt-transports-httporg.apache.cxfcxf-rt-transports-http-jetty${code-house.cxf.version}org.springframeworkspring-core${code-house.spring.version}org.springframeworkspring-test${code-house.spring.version}testorg.springframeworkspring-beans${code-house.spring.version}org.springframeworkspring-core${code-house.spring.version}org.springframeworkspring-context${code-house.spring.version}[/sourcecode]\nDeskryptor nie jest zbyt złożony, istotny jest tylko kawałek z kontraktem, który jak wskazuje nazwa jest definicją używanych typów: [sourcecode language=\u0026ldquo;xml\u0026rdquo;]org.code-house.cxfcontract1.0-SNAPSHOT[/sourcecode]\nTeraz kolej na jedyną wstawkę Javy, która się pojawia w projekcie. Jest to zwykły bean, który będzie miał później wstrzykiwane obiekty pośredniczące w wywoływaniu usług.\n[sourcecode language=\u0026ldquo;java\u0026rdquo;]package org.code_house.cxf.client;\nimport org.code_house.services.maven.MavenArtifactType;\n/\\\\ \\* Klient usług Code-House. \\* \\* @author Łukasz Dywicki email \\* \\* $Id$ */ public class Client {\n/\\\\ \\* Usługa do obsługi wyszukiwania artefaktów Mavena. */ private MavenArtifactType maven;\n/\\\\ \\* Pobranie wartości pola maven. \\* \\* @return Wartość maven. */ public MavenArtifactType getMaven() { return maven; }\n/\\\\ \\* Ustawienie wartości pola maven. \\* \\* @param maven Nowa wartość pola maven. */ public void setMaven(MavenArtifactType maven) { this.maven = maven; } }[/sourcecode]\nResztę magii załatwia Spring: [sourcecode language=\u0026ldquo;xml\u0026rdquo;][/sourcecode]\nZgodnie ze wstawką w linii 20 konieczna jest jeszcze konfiguracja usługi. Sztuczka polega na użyciu części konfiguracji zdefiniowanej wcześniej - baseClientFactory. [sourcecode language=\u0026ldquo;xml\u0026rdquo;][/sourcecode]\nUstawienia, które mogą ulec zmianie, to znaczy użytkownik, hasło oraz adres usługi są wyodrębnione do pliku client.properties: [sourcecode language=\u0026ldquo;properties\u0026rdquo;] # Placeholdery dla kontekstow springa # Adresy uslug server.port = 8080 host = localhost org.code_house.cxf.service.maven http://${host}:${server.port}/webapp/services/maven\n# Autoryzacja org.code_house.cxf.user org.code_house.cxf.password [/sourcecode]\nNo i na koniec opcjonalny test, który odpytuje usługę: [sourcecode lang=\u0026ldquo;java\u0026rdquo;] package org.code_house.cxf.client; import org.code_house.services.maven.definition.ArtifactInfo; import org.code_house.services.maven.types.FindArtifactRequest; import org.code_house.services.maven.types.FindArtifactRespose; import org.springframework.test.AbstractDependencyInjectionSpringContextTests;\n/\\\\ \\* Proste wywołanie klasy klienta. \\* \\* @author Łukasz Dywicki email \\* \\* $Id$ */ public class MainTest extends AbstractDependencyInjectionSpringContextTests {\n/\\\\ \\* Wstrzyknięty klient. */ private Client client;\n@Override protected String[] getConfigLocations() { return new String[] {\u0026ldquo;classpath:client.xml\u0026rdquo;}; }\npublic void testOne() { FindArtifactRequest request = new FindArtifactRequest(); ArtifactInfo artifact = new ArtifactInfo(); artifact.setGroupId(\u0026ldquo;org.code_house.cxf\u0026rdquo;); artifact.setArtifactId(\u0026ldquo;contract\u0026rdquo;); request.setQuery(artifact);\nFindArtifactRespose respose = client.getMaven().findArtifact(request); System.out.println(respose.getDownloadURL()); }\n/\\\\ \\* Ustawienie wartości pola client. \\* \\* @param client Nowa wartość pola client. */ public void setClient(Client client) { this.client = client; }\n}[/sourcecode]\nTo by było na tyle. Cały działający kod projektu jest już zamieszczony przy poprzedniej nocie, paczka ze wszystkimi listingami gotowa do pobrania.\nTeraz chyba pora zacząć opisywać mechanizmy Springa. :)\n","permalink":"https://dywicki.pl/2008/09/budowanie-klienta-uslugi-sieciowej-w-oparciu-o-apache-cxf/","summary":"\u003cp\u003eW nawiązaniu do \u003ca href=\"http://blog.dywicki.pl/2008/07/23/budowanie-uslugi-sieciowej-w-oparciu-o-apache-cxf/\"\u003epoprzedniej noty o CXFie\u003c/a\u003e, którą napisałem jakiś czas temu, gonię aby uzupełnić brak konfiguracji klienta. Sam proces jest bardzo zbliżony do tworzenia klienta w oparciu o XFire. Nie jest wymagana duża ilość kodu Javy, a w zasadzie tylko dwa pliki XML (client.xml, \u003cem\u003emyservice.xml\u003c/em\u003e).\u003c/p\u003e\n\u003cp\u003ePierwszy z nich odpowiada za wczytanie wymaganych rozszerzeń CXFa oraz definicję bazowej konfiguracji fabryki z interceptorami. W interceptorach możemy skonfigurować logowanie, obsługę załączników czy standardów WS-Security etc. Wszystkie te ustawienia będą dziedziczone, a fabryki docelowych usług będą dodawać tylko adres, do odpytywania. Na koniec bean klienta będzie miał określony \u003cstrong\u003eautowire\u003c/strong\u003e by nie przekazywać mu wszystkich własności.\u003c/p\u003e\n","title":"Budowanie klienta usługi sieciowej w oparciu o Apache CXF"},{"content":"Od jakiegoś czasu w pracy do tworzenia usług sieciowych korzystam z Apache CXF. Jako że biblioteka jest stosunkowo nowa i nie najlepiej udokumentowana postanowiłem przedstawić na blogu jak wygląda proces tworzenia.\nCXF jest połączeniem kilku bibliotek - YOKO, Celtixa oraz XFire. Każda z nich wcześniej realizowała pewien fragment obecnej funkcjonalności CXF - YOKO obsługuje Corbę a XFire usługi sieciowe. Obecne CXF jest gotowy do używania \u0026ldquo;produkcyjnego\u0026rdquo;, ponieważ niedawno wyszedł z fazy inkubacji. :)\nArchitektura CXF ma dosyć elastyczną budowę. Zgodnie z dokumentacją można wyróżnić najważniejsze składowe:\nBus, jest trzonem architektury CXF w którym definiuje i konfiguruje się rozszerzenia. Messaging \u0026amp; Interceptors, zapewniają niskopoziomowy dostęp do komunikatów oraz warstwę na której jest oparta większość funkcjonalności. Front ends, frontendy są interfejsami programistycznymi do tworzenia usług (np. JAX-WS). Services, usługi zapewniają model wraz z opisem Bidings, element ten jest odpowiedzialny za obsługę konkretnego protokołu (SOAP, REST, Corba etc). Transports, warstwa abstrakcji ułatwiająca zmianę sposobu transportu do/z usług. Markieting :) CXF oferuje infrastrukturę konieczną do budowania usług, z najważniejszy zalet można wymienić:\nWsparcie dla różnych protokołów. Obsługa standardów WS-*, tj. WS-Addressing, WS-Security, WS-ReliableMessaging, oraz WS-Policy. Obsługa wielu transportów. Dołączane data-bindingi (np JAXB, Aegis). Jasny podział front endów takich jak JAX-WS od najważniejszego kodu. Wysoka wydajność. Możliwość osadzania w różnych środowiskach. Z dodatkowych zalet, mogę dodać - bardzo łatwą integrację ze Springiem.\nPierwsza usługa Do budowania projektów będziemy używać Mavena. Implementowana usługa będzie oparta o frontend JAX-WS z ręcznie pisanym deskryptorem usługi (WSDL first). Jakkolwiek w bardzo prosty sposób można odwrócić kolejność i przy pomocy pluginu CXF do Mavena wygenerować deskryptor.\nStruktura projektów będzie następująca:\nparent\nRodzic projektu ze zdefiniowanymi wersjami bibliotek i raportami.\ncontract\nDefinicje używane zarówno przez klienta jak i serwer - WSDL oraz konfiguracja pluginu CXF.\nclient\nProsta biblioteka kliencka oparta o mechanizmy CXFa (JaxWSProxyFactoryBean).\nserver\nPrzykładowa implementacja usługi z bardzo prostym wykorzystaniem Springa.\nwebapp\nKonfiguracja transporty CXF - w tym konkretnym przypadku servletu CXF.\nParent Poniżej znajduje się deskryptor projektu, który jest używany do budowania całości. [sourcecode lang=\u0026ldquo;xml\u0026rdquo;] 4.0.0org.code-house.cxfparentCode House.Org - CXF1.0-SNAPSHOTpomRodzic projektu, zawiera wszystkie moduly.cxf-clientcxf-contractcxf-servercxf-webapp2.1.12.1.32.5.4maven-compiler-plugin1.5 1.5org.apache.cxfcxf-rt-frontend-jaxws${code-house.cxf.version}org.apache.cxfcxf-rt-transports-http${code-house.cxf.version}org.apache.cxfcxf-rt-transports-http-jetty${code-house.cxf.version}com.sun.xml.bindjaxb-impl${code-house.jaxb.version}javax.xml.bindjaxb-api2.1 [/sourcecode]\nContract Zgodnie z tym, co napisałem wcześniej - przyjąłem podejście, że WSDL jest pisany ręcznie, głównie dlatego że dla większych projektów można w prosty sposób narzucić jakąś organizację i podział plików, z których są następnie generowane źródła.\nNajistotniejsza wstawka, która powinna znaleźć się w pomie: [sourcecode lang=\u0026ldquo;xml\u0026rdquo;] org.apache.cxfcxf-codegen-plugingenerate-sources${basedir}/target/jaxws ${basedir}/src/main/resources/maven.wsdl -quietwsdl2java [/sourcecode]\nPo dodaniu tej wstawki do sekcji build/plugins możemy przejść do tworzenia deskryptora usługi. W moim przypadku przyjąłem następujący podział:\nmaven.wsdl - definicje metod oraz komunikatów w rozumieniu WSDL (messages). Można z powodzeniem wyłączyć z tego pliku same koperty i pozostawić metody a to co potrzebne włączyć dyrektywą wsdl:import. types.xsd - typy używane do komunikacji - zazwyczaj pary request+response używane bezpośrednio w definiowaniu elementów wsdl:part. definition.xsd - definicje typów złożonych, niezależnych od usług, tj. opis domain-modelu z którym usługa pracuje. Każdy z tych plików ma inną przestrzeń nazw.\nmaven.wsdl [sourcecode lang=\u0026ldquo;xml\u0026rdquo;]\n[/sourcecode]\ntypes.xsd [sourcecode lang=\u0026ldquo;xml\u0026rdquo;]\n[/sourcecode]\ndefinition.xsd [sourcecode lang=\u0026ldquo;xml\u0026rdquo;]\n[/sourcecode]\nPo odpaleniu polecenia mvn:install powinniśmy otrzymać w konsoli fragment podobny do tego: [sourcecode] [INFO] [cxf-codegen:wsdl2java {execution: default}] [/sourcecode] Jest to informacja, że plugin CXF został poprawnie skonfigurowany i uruchomiony.\nSerwer Sercem naszej usługi jest oczywiście jej implementacja dlatego też nie możemy obejść się bez niej. :) Projekt ten ma tylko dwie zależności - contract oraz artefakt cxf-rt-frontend-jaxws.\nKonfiguracja serwera odbywa się w oparciu o springa: [sourcecode lang=\u0026ldquo;xml\u0026rdquo;]\n[/sourcecode]\nSerwer zawiera w zasadzie niewiele kodu, oto i on: [sourcecode lang=\u0026ldquo;java\u0026rdquo;] package org.code_house.services.maven;\nimport java.net.URISyntaxException;\nimport javax.jws.WebService;\nimport org.code_house.services.maven.definition.ArtifactInfo; import org.code_house.services.maven.types.FindArtifactRequest; import org.code_house.services.maven.types.FindArtifactRespose;\n/\\\\ \\* Implementacja usługi. */ @WebService(serviceName = \u0026ldquo;MavenService\u0026rdquo;, endpointInterface = \u0026ldquo;org.code_house.services.maven.MavenArtifactType\u0026rdquo;, targetNamespace = \u0026ldquo;http://code-house.org/services/maven\u0026quot; ) public class MavenArtifactTypeImpl implements MavenArtifactType {\n/\\\\ \\* Bean zawierający implementację logiki biznesowej. */ private MavenSearchService service;\npublic FindArtifactRespose findArtifact(FindArtifactRequest request) { ArtifactInfo info = request.getQuery(); // pobranie struktury przekazanej od klienta\n// sformuowanie odpowiedzi FindArtifactRespose response = new FindArtifactRespose(); try { response.setDownloadURL(service.find(info).toURI().toString()); } catch (URISyntaxException e) { throw new RuntimeException(e); } return response; }\n/\\\\ \\* Ustawienie wartości pola service. \\* \\* @param service Nowa wartość pola service. */ public void setService(MavenSearchService service) { this.service = service; } } [/sourcecode]\nDodatkowy kod, który nie jest konieczny do implementacji usługi to definicja interfejsu MavenSearchService. Dzięki zastosowaniu takiego rozwiązania można w prostszy sposób testować działanie usługi poprzez przekazywanie jej mocka stworzonego np. przy pomocy easy mocka. Kod takiego testu pominąłem ze względu na to, że notka i tak już jest za długa w tym momencie a jesteśmy ledwo w połowie drogi. :)\nPrzykładowa implementacja wcześniej wspomnianego intefejsu nie zawiera żadnej logiki i zawsze zwraca tą samą wartość. [sourcecode lang=\u0026ldquo;java\u0026rdquo;] package org.code_house.services.maven;\nimport java.net.MalformedURLException; import java.net.URL;\nimport org.code_house.services.maven.definition.ArtifactInfo;\n// Najprostsza implementacja tylko po to żeby sprawdzić działanie usługi public class DummyMavenSearchServiceImpl implements MavenSearchService {\npublic URL find(ArtifactInfo info) { try { return new URL(\u0026ldquo;http://repo1.maven.org/maven2/org/apache/apache/4/apache-4.pom\u0026quot;); } catch (MalformedURLException e) { throw new RuntimeException(e); } }\n} [/sourcecode]\nZanim odpalimy serwer konieczna będzie jeszcze jedna rzecz - konfiguracja transporu, w naszym przypadku servletu CXF.\nWebapp Webapp posiada bezpośrednie zależności do 2 artefaktów CXFa: cxf-rt-transports-http oraz cxf-rt-bindings-http. Druga jest opcjonalna, bez niej CXF nie będzie wyświetlał dostępnych usług w postaci tabelki html.\nPrzydatna może być wtyczka jetty (sekcja build/plugins), która pozwala na uruchomienie aplikacji bez konieczności instalowania kontenera servletów: [sourcecode lang=\u0026ldquo;xml\u0026rdquo;] org.mortbay.jettymaven-jetty-plugin6.1.1110808060000 [/sourcecode]\nOstatnia zależność odnosi się do implementacji usługi - czyli naszego artefaktu server. Najistotniejsze elementy konfiguracji webappa to web.xml oraz cxf-beans.xml.\nweb.xml Warto zauważyć w poniższym listingu fajną możliwość, którą daje nam Spring w postaci wildcardów określających położenie konfiguracji - w tym przypadku classpath:/module/*-context.xml. Oznacza to, że Spring przeskanuje wszystkie biblioteki od których zależy projekt i dołączy ich konfigurację do głownej, dzięki czemu będziemy mogli uzyskać bardziej modułową i elastyczną budowę aplikacji. Może Dynamic Modules to nie jest ale na codzień w zupełności wystarcza :).\n[sourcecode lang=\u0026ldquo;xml\u0026rdquo;] Maven Search Services :: WebappFrontend indeksera.contextConfigLocation /WEB-INF/security.xml\n/WEB-INF/cxf-beans.xml\n/WEB-INF/applicationContext.xml\nclasspath:/module/*-context.xml CXFServletorg.apache.cxf.transport.servlet.CXFServlet1CXFServlet/services/*org.springframework.web.context.ContextLoaderListener [/sourcecode]\ncxf-beans.xml Plik ten zawiera definicje konfiguracyjne CXFa. W poniższej formie włączana jest tylko część modułów CXF w celu zmniejszenia użycia zasobów. [sourcecode lang=\u0026ldquo;xml\u0026rdquo;]\n[/sourcecode]\nPozostałe pliki konfiguracyjne są praktycznie puste, ponieważ tyczą się obszarów niezwiązanych z tematem tego wpisu. Po tym wszystkim możemy uruchomić naszą aplikację poleceniem mvn jetty:run. Po wejściu na adres http://localhost:8080/webapp/services naszym oczom powinna ukazać się lista podobna do poniższej: Nie wiem niestety dlaczego CXF generuje zły adres usługi (pomijający mapowanie servletu) oraz ma problemy z wystawieniem WSDLa. Jakkolwiek i bez tego wszystko działa poprawnie - znaczy wywoływanie usług rzecz jasna. :)\nW tym momencie możemy uruchomić Soap UI i wykonać jedyną dostępną metodę - findArtifact.\nClient Ostatnim z elementów, który pozostał do omówienia jest klient. Niestety z racji na to, że ta nota już się wystarczająco rozrosła zrobię to w kolejnym wpisie. Mam nadzieję, że nota ta nieco ułatwi przyszłym użytkownikom CXFa jego poznanie. :) Dla tych, którym nie chce się wpisywać tego wszystkiego w edytorze zamieszczam źródła razem z działającym klientem. Projekty nie są nadzwyczajnie dopieszczone, ale w zupełności wystarczą do startu. Znajduje się też tam w pełni działający klient stworzonej usługi. Pozdrawiam i życzę miłej zabawy!\n","permalink":"https://dywicki.pl/2008/07/budowanie-uslugi-sieciowej-w-oparciu-o-apache-cxf/","summary":"\u003cp\u003eOd jakiegoś czasu w pracy do tworzenia usług sieciowych korzystam z \u003ca href=\"http://cxf.apache.org\"\u003eApache CXF\u003c/a\u003e. Jako że biblioteka jest stosunkowo nowa i nie najlepiej udokumentowana postanowiłem przedstawić na blogu jak wygląda proces tworzenia.\u003c/p\u003e\n\u003cp\u003eCXF jest połączeniem kilku bibliotek - \u003ca href=\"http://cwiki.apache.org/YOKO/\"\u003eYOKO\u003c/a\u003e, \u003ca href=\"http://celtix.objectweb.org/\"\u003eCeltixa\u003c/a\u003e oraz \u003ca href=\"http://xfire.codehaus.org/\"\u003eXFire\u003c/a\u003e. Każda z nich wcześniej realizowała pewien fragment obecnej funkcjonalności CXF - YOKO obsługuje Corbę a XFire usługi sieciowe. Obecne CXF jest gotowy do używania \u0026ldquo;produkcyjnego\u0026rdquo;, ponieważ niedawno wyszedł z fazy inkubacji. :)\u003c/p\u003e\n","title":"Budowanie usługi sieciowej w oparciu o Apache CXF"},{"content":"Jakiś czas temu, jeszcze podczas pracy w poprzedniej firmie przypadło mi zadanie podpięcia się pod magistralę usług opartą o Apache Service Mix (SMX). Był to wówczas dla mnie temat zupełnie nowy, ba nawet nie wiedziałem z czym to się je. :) Koniec końców jednak podpięcie pod ESB (Enterprises Service Bus) nie było w ogóle trudne. Po jakimś czasie i drobnych przetasowaniach na płaszczyźnie zawodowej zająłem się SMX-em nie jako klient magistrali a osoba implementująca usługi na szynie a ten wpis jest drobną przeróbką prezentacji, którą przygotowałem w pracy.\nCzym jest ESB Jednoznaczne określenie terminu ESB nie jest łatwe, ponieważ wokół tego tematu rozpętana została burza marketingowa. Jedni uważają je za oś SOA (Service Oriented Architecture) inni jako zło konieczne w dużych instytucjach.\nDlatego pomijając teorię przejdźmy do najistotniejszych cech, jakie oferuje ESB, niezależnie od producenta oraz osoby definiującej pojęcie. Sam nie chciałbym wdawać się w dyskusję na temat postrzegania i ESB i SOA.\nŹródło - CodePlex\nNa załączonym wyżej obrazku widać typową strukturę logiczną w oparciu o ESB. Z lewej strony mamy komercyjne rozwiązanie – MQ Series firmy IBM, dalej idąc dołem, widzimy bazę danych, serwer pocztowy a na końcu mainframe. U góry natomiast pojawiają się klienci.\nCiąg dalszy Na bazie tego obrazku można powiedzieć co nieco o tym, czym owa niebieska rurka symbolizująca ESB jest:\nJest to z pewnością centralny rejestr usług firmy, dzięki któremu nie jest konieczne wiązanie aplikacji między sobą. Wiąże się je tylko i wyłącznie z jednym elementem – z magistralą. Kolejny ważny punkt, to most pomiędzy protokołami. W dobie, gdy wszyscy żyją już Web Services nie można zapomnieć o innych sposobach komunikacji – poczynając od poczciwej Corby po kolejki JMS czy też odczyt zasobów z FTP. Transformacja komunikatów to opcjonalna czynność, której nie widać na wyżej wymienionym obrazku. Jest ona wykonywana pod maską, wewnątrz magistrali, w zależności od potrzeb. W chwili gdy mamy komunikaty z systemu A do systemu B możemy wszystko sprowadzić do jednego uniwersalnego API danych. Inteligentny router informacji. Większość magistral ma coś wspólnego z pojęciem EIP, czyli Enterprise Integration Patterns. Jednym z tych wzorców jest Content Based Router, to znaczy w zależności od kształtu, zawartości komunikatu, nagłówka, fragmentu, czegokolwiek możemy odbijać komunikat w różnych kierunkach. Dalej z wzorców można wymienić Message Filter, Recipient List, Routing Slip, Wire Tap, Splitter, Resequencer itd. Integrator procesów biznesowych (bardziej marketingowo) – po pierwsze odwzorowanie usług magistrali do czynności biznesowych w organizacji a po drugie wsparcie dla procesorów reguł biznesowych ( WS-BPEL). Service Mix jako ESB Mając zestaw wyżej wymienionych cech możemy przejść do omówienia projektu Apache Service Mix. Może na początku kilka słów o tym, czym jest JBI, ponieważ pojawia się sam skrót, ale nie ma jego omówienia. Otóż, JBI w rozwinięciu oznacza Java Business Integration. Jest to standard przyjęty w ramach Java Community Process w celu określenia norm budowania rozwiązań SOA (znów buzzword). Pomijając politykę wielkich korporacji oraz marketing przejdźmy do elementów, które standard ten określa: Źródło Open ESB Starter Kit\nTypy komponentów: Service Engine (SE) – backend do obsługi zapytań. Binding Components (BC) – frontend, do nasłuchiwania w danym standardzie. Shared Libraries (SL) – kod współdzielony przez w/w komponenty. Service Assembly (SA) – zbiór usług rozumianych jako jedność przez magistralę (zwykle para BC+SE). Normalized Message Router – jest to serce rozwiązania opartego o JBI, ponieważ w nim są transportowane komunikaty. To on zapewnia przepływ informacji z komponentów bindujących do silników. Message Exchange Patterns – w oparciu o definicję dla SOAP JBI przewiduje następujące typy komunikatów: In-Only – tylko wejście, usługa nie zwraca żadnej odpowiedzi Rebust In-Only – zwrócony zostanie status po obsłudze zapytania bądź wyjątek. In-Out – standardowa obsługa wejście-wyjście. In Optional-Out – wejście z niewiążącą (nieobowiązkową) odpowiedzią. Dostępnych jest kilka implementacji JBI:\nOpen ESB Apache Service Mix FUSE ESB (na bazie Service Mix) Bostech Chain Builder ESB Mule JBoss ESB Fusion Middleware ActiveMatrix Service Bus Service Mix od środka Wewnątrz Service Mix jest spięciem kilku potężnych projektów, rozwijanych od dłuższego czasu, które zdobyły już renomę i popularność. Między innymi można wyróżnić:\nPierwszy z tych projektów to Spring Framework, rozwijany od bodajże 2000 roku, z powodzeniem rywalizujący z architekturami opartymi o EJB. Spring jest nie tylko mechanizmem konfiguracyjnym ale również zbiorem bardzo dobrych komponentów umożliwiających szereg operacji (bazy danych, JMS, przetwarzanie wsadowe, Web Services etc). Drugi, bardzo ważny projekt to Active MQ. Największa i najpopularniejsza otwarta implementacja standardu JMS. Jest on używany wewnątrz Service Mix-a jako transporter komunikatów w Normalized Message Router jak i do obsługi końcówek JMS. - Wymieniony nieco niżej pod-projekt Active MQ to Camel. Jest to szkielet przeznaczony do tworzenia reguł routingu. Wspiera różnorakie transporty (HTTP, JMS, JBI, MS MQ itp.). XBean jest fragmentem projektu Apache Geronimo (serwer aplikacyjny ze stajni Apache) przeznaczonym do tworzenia konfiguracji i zarządzania komponentami. Jest zbudowany w oparciu o Springa. Apache CXF jest stosunkowo nowym projektem, który jest używany poprzez Service Mix w celu obsługi zapytań SOAP (chociaż możliwe jest użycie innego komponentu). Apache ODE jest silnikiem reguł biznesowych w oparciu o WS-BPEL. JBoss Drools jest kolejnym procesorem reguł biznesowych. Może być użyty do routingu. Jest dostępny plugin pozwalający na łatwą pracę z tą technologią. Możliwości Service Mix Wyżej wymienione projekty są używane w Service Mix w celu uzyskania typowych funkcjonalności ESB:\nJMS, czyli obsługa kolejek WS-BPEL, obsługa reguł biznesowych Web Services przy pomocy CXF jak i modułu JSR-181 Transformacje XSLT oraz XQuery (w oparciu o Saxona) File Drop to odczyt i zapis do plików dostępnych lokalnie jak i zdalnie ( FTP) Obsługa protokołu XMPP pozwala na łatwą integrację z komunikatorami zbudowanymi w oparciu o Jabbera. Dostęp do poczty przy pomocy modułu servicemix-mail Komponenty programowe: Dają możliwość dopisania własnych \u0026ldquo;endpointów\u0026rdquo;, czyli implementacji docelowych usług bądź pośredników. Dodatkowe funkcjonalności (cache, rss, walidacja) Języki skryptowe (min Groovy) ESB – dlaczego Open Source Wybór Service Mix-a nie był podyktowany przypadkiem. Jest to bowiem najpopularniejsze tego typu rozwiązanie z otwartym kodem źródłowym. Co więcej, nie jest to projekt rozwijany przez osoby bez doświadczenia, pozostawiony bez wsparcia. Otwarty kod ułatwia przede wszystkim adaptację tego rozwiązania do potrzeb organizacji a nie odwrotnie – organizacji do potrzeb magistrali. W razie potrzeb jesteśmy w stanie dopisać własne komponenty, obsługę mniej standardowych protokołów na bazie dostarczonych interfejsów czy to w Service Mix czy to w Camelu. Przyjęte standardy gwarantują ciągłość rozwoju oraz ułatwiają integrację ( JMX umożliwia łatwe podpięcie konsoli administracyjnej), podczas gdy J2EE Connector Architecture definiuje kontrakty (zarządzanie połączeniami, transakcjami, bezpieczeństwem). Nie bez znaczenia jest również koszt, jaki organizacja ponosi w przypadku zdecydowania się na otwarte rozwiązanie. Rozpoczęcie prac z Service Mix-em kosztuje 0 PLN. Każdy, bez rejestracji, podawania jakichkolwiek danych może pobrać źródła albo gotowe dystrybucje i uruchomić je na swoim komputerze. W chwili gdy istnieje takie zapotrzebowanie, organizacja posiada kompetencje i skromny budżet to taka konfiguracja początkowo jest optymalna. Z biegiem czasu gdy zaistnieje konieczność wsparcia czy szkoleń to są one oferowane przez firmę IONA, która jest zaangażowana w rozwój Service Mix-a.\nService Mix a bezpieczeństwo Większość, jeśli nie wszystkie rozwiązania w Javie, które wiążą się z kryptografią są oparte na JCA – Java Cryptography Architecture. Jest to zestaw interfejsów oraz ich implementacji zawierający implementację najpopularniejszych algorytmów kryptograficznych jak i API umożliwiające tworzenie własnych rozszerzeń ( JCE). Standard autoryzacji i uwierzytelniania w Javie to JAAS ( Java Authentication and Authorization Service). W oparciu o niego jest budowanych większość rozwiązań związanych z bezpieczeństwem. Nawet największe alternatywy takie jak Acegi Security (obecnie Spring Security posiadają adaptery integrujące je ze standardem). Przy użyciu dostępnych interfejsów możliwe jest dostarczenie własnej implementacji usługi obsługującej autoryzację bądź uwierzytelnianie użytkowników/systemów. Bezpieczeństwo usług sieciowych jest zależne od wybranego komponentu Service Mix. Pełne wsparcie dla WS-Security oferują komponenty zbudowane w oparciu o Apache CXF (szyfrowanie, podpisywanie komunikatów). Z innych standardów CXF wspiera także WS-Policy, WS-Addressing. Szyfrowane połączenia są łatwe do uzyskania przy pomocy komponentu servicemi-http. Dostępne zabezpieczenia na poziomie wirtualnej maszyny Javy to certyfikowanie (podpisywanie) kodu modułów oraz konfiguracja Security Managera (umożliwia wyłączenie dostępu do pakietów/klas/metod).\n","permalink":"https://dywicki.pl/2008/06/apache-servicemix-open-source-es/","summary":"Jakiś czas temu, jeszcze podczas pracy w poprzedniej firmie przypadło mi zadanie podpięcia się pod magistralę usług opartą o \u003ca href=\"http://servicemix.apache.org\"\u003eApache Service Mix\u003c/a\u003e (SMX). Był to wówczas dla mnie temat zupełnie nowy, ba nawet nie wiedziałem z czym to się je. :) Koniec końców jednak podpięcie pod ESB (Enterprises Service Bus) nie było w ogóle trudne. Po jakimś czasie i drobnych przetasowaniach na płaszczyźnie zawodowej zająłem się SMX-em nie jako klient magistrali a osoba implementująca usługi na szynie a ten wpis jest drobną przeróbką prezentacji, którą przygotowałem w pracy.","title":"Apache ServiceMix, Open Source ESB"}]